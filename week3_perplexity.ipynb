{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 â€” Perplexity and Basic Benchmarking\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand what perplexity means and how to compute it\n",
        "2. Implement pseudo-perplexity calculation using an ONNX model\n",
        "3. Visualize perplexity distributions across sentences\n",
        "4. Understand the limitations of perplexity as an evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ§  What is Perplexity? (Feynman Explanation)\n",
        "\n",
        "Imagine you're playing a word-guessing game. Before each word is revealed, you make a guess:\n",
        "\n",
        "**Sentence:** \"The cat sat on the ___\"\n",
        "\n",
        "A good guesser might say \"mat\" (common ending). If they're right, they're not surprised.\n",
        "If the actual word is \"elephant,\" they'd be very surprised!\n",
        "\n",
        "**Perplexity measures how \"surprised\" a language model is by actual text.**\n",
        "\n",
        "- **Low perplexity** = Model predicted well, wasn't surprised\n",
        "- **High perplexity** = Model was often wrong, very surprised\n",
        "\n",
        "### Technical Definition\n",
        "\n",
        "Perplexity is the **exponentiated average negative log-likelihood** of the tokens:\n",
        "\n",
        "1. For each token, compute probability the model assigned\n",
        "2. Take negative log of each probability\n",
        "3. Average across all tokens\n",
        "4. Exponentiate the result\n",
        "\n",
        "Lower is better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ› ï¸ Step 1: Setup & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install onnxruntime transformers datasets pandas numpy matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"âœ… ONNX Runtime version: {ort.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“¦ Step 2: Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the ONNX model (upload your model to Colab)\n",
        "model_path = \"/tmp/tinygpt.onnx\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create inference session\n",
        "session = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "print(\"âœ… Model and tokenizer loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š Step 3: Download a Small Dataset\n",
        "\n",
        "We'll use a small subset of WikiText for our perplexity experiments.\n",
        "WikiText is a collection of high-quality Wikipedia articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a small subset of wikitext\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "\n",
        "# Filter out empty lines and very short texts\n",
        "texts = [text for text in dataset[\"text\"] if len(text.strip()) > 50]\n",
        "\n",
        "# Take first 100 samples for efficiency\n",
        "sample_texts = texts[:100]\n",
        "\n",
        "print(f\"âœ… Loaded {len(sample_texts)} sample texts\")\n",
        "print(f\"\\nðŸ“ Example text:\")\n",
        "print(sample_texts[0][:200] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¢ Step 4: Implement Softmax Function\n",
        "\n",
        "Since ONNX models output raw logits, we need to convert them to probabilities.\n",
        "Softmax converts a vector of numbers into a probability distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(logits: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert logits to probabilities using the softmax function.\n",
        "    \n",
        "    Args:\n",
        "        logits: Raw model outputs (can be any real numbers)\n",
        "    \n",
        "    Returns:\n",
        "        Probabilities that sum to 1\n",
        "    \"\"\"\n",
        "    # Subtract max for numerical stability (prevents overflow)\n",
        "    exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))\n",
        "    return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)\n",
        "\n",
        "# Quick test\n",
        "test_logits = np.array([1.0, 2.0, 3.0])\n",
        "test_probs = softmax(test_logits)\n",
        "print(f\"Test logits: {test_logits}\")\n",
        "print(f\"Test probabilities: {test_probs}\")\n",
        "print(f\"Sum of probabilities: {test_probs.sum():.4f} (should be 1.0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Step 5: Implement Pseudo-Perplexity Function\n",
        "\n",
        "### How it works:\n",
        "\n",
        "1. Tokenize the input text\n",
        "2. For each position i, use tokens 0...i as context\n",
        "3. Predict the probability distribution for the next token\n",
        "4. Record the probability assigned to the actual next token\n",
        "5. Compute average negative log probability and exponentiate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_pseudo_perplexity(\n",
        "    text: str, \n",
        "    session: ort.InferenceSession, \n",
        "    tokenizer,\n",
        "    max_context_length: int = 50\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute approximate perplexity for a text sequence.\n",
        "    \n",
        "    Uses a sliding window approach:\n",
        "    - For each position, predict the next token\n",
        "    - Sum negative log probabilities\n",
        "    - Exponentiate the average\n",
        "    \n",
        "    Args:\n",
        "        text: Input text to evaluate\n",
        "        session: ONNX Runtime inference session\n",
        "        tokenizer: Hugging Face tokenizer\n",
        "        max_context_length: Maximum context window size\n",
        "    \n",
        "    Returns:\n",
        "        Pseudo-perplexity score (lower is better)\n",
        "    \"\"\"\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer(text, return_tensors=\"np\")\n",
        "    input_ids = inputs[\"input_ids\"][0]\n",
        "    \n",
        "    # Need at least 2 tokens to compute perplexity\n",
        "    if len(input_ids) < 2:\n",
        "        return float('inf')\n",
        "    \n",
        "    total_nll = 0.0  # Total negative log-likelihood\n",
        "    count = 0\n",
        "    \n",
        "    # For each position, predict next token\n",
        "    for i in range(len(input_ids) - 1):\n",
        "        # Use sliding window for context\n",
        "        start_idx = max(0, i + 1 - max_context_length)\n",
        "        context = input_ids[start_idx:i+1].reshape(1, -1)\n",
        "        \n",
        "        # Get model output (logits)\n",
        "        outputs = session.run(None, {\"input_ids\": context})\n",
        "        logits = outputs[0][0, -1, :]  # Last position's logits\n",
        "        \n",
        "        # Convert to probabilities\n",
        "        probs = softmax(logits)\n",
        "        \n",
        "        # Get probability of actual next token\n",
        "        next_token = input_ids[i + 1]\n",
        "        prob = probs[next_token]\n",
        "        \n",
        "        # Compute negative log probability\n",
        "        # Add small epsilon to avoid log(0)\n",
        "        nll = -np.log(prob + 1e-10)\n",
        "        total_nll += nll\n",
        "        count += 1\n",
        "    \n",
        "    # Compute perplexity: exp(average NLL)\n",
        "    if count == 0:\n",
        "        return float('inf')\n",
        "    \n",
        "    avg_nll = total_nll / count\n",
        "    perplexity = np.exp(avg_nll)\n",
        "    \n",
        "    return float(perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's test on a few example sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test sentences with expected perplexity patterns\n",
        "test_sentences = [\n",
        "    \"The cat sat on the mat.\",                          # Common, should be low\n",
        "    \"Machine learning is transforming technology.\",     # Technical but coherent\n",
        "    \"Colorless green ideas sleep furiously.\",           # Grammatical but nonsensical\n",
        "    \"asdf qwerty zxcv bnm poiu\",                        # Random, should be high\n",
        "]\n",
        "\n",
        "print(\"ðŸ“Š Perplexity Test Results:\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    ppl = compute_pseudo_perplexity(sentence, session, tokenizer)\n",
        "    print(f\"PPL: {ppl:>10.2f} | {sentence}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸ’¡ What to observe:\n",
        "\n",
        "- Common sentences should have **lower** perplexity\n",
        "- Random text should have **higher** perplexity\n",
        "- The Chomsky sentence (\"Colorless green ideas...\") is interestingâ€”it's grammatically correct but semantically odd!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ˆ Step 6: Compute Perplexity on Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_perplexities_batch(\n",
        "    texts: list, \n",
        "    session: ort.InferenceSession, \n",
        "    tokenizer,\n",
        "    max_samples: int = 50\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Compute perplexity for a list of texts.\n",
        "    \n",
        "    Args:\n",
        "        texts: List of text strings\n",
        "        session: ONNX inference session\n",
        "        tokenizer: Hugging Face tokenizer\n",
        "        max_samples: Maximum number of samples to process\n",
        "    \n",
        "    Returns:\n",
        "        List of (text, perplexity) tuples\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    texts_to_process = texts[:max_samples]\n",
        "    \n",
        "    for i, text in enumerate(texts_to_process):\n",
        "        # Skip very short texts\n",
        "        if len(text.strip()) < 20:\n",
        "            continue\n",
        "            \n",
        "        # Truncate very long texts for efficiency\n",
        "        truncated = text[:500] if len(text) > 500 else text\n",
        "        \n",
        "        ppl = compute_pseudo_perplexity(truncated, session, tokenizer)\n",
        "        \n",
        "        # Filter out infinite/very large values\n",
        "        if ppl < 10000:\n",
        "            results.append({\n",
        "                \"text\": truncated[:100] + \"...\" if len(truncated) > 100 else truncated,\n",
        "                \"perplexity\": ppl\n",
        "            })\n",
        "        \n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {i + 1}/{len(texts_to_process)} texts...\")\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute perplexities on our sample texts\n",
        "print(\"ðŸ”„ Computing perplexities (this may take a few minutes)...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "perplexity_results = compute_perplexities_batch(\n",
        "    sample_texts, \n",
        "    session, \n",
        "    tokenizer, \n",
        "    max_samples=50\n",
        ")\n",
        "\n",
        "# Create DataFrame\n",
        "ppl_df = pd.DataFrame(perplexity_results)\n",
        "\n",
        "print(f\"\\nâœ… Computed perplexity for {len(ppl_df)} texts\")\n",
        "print(f\"\\nðŸ“Š Summary Statistics:\")\n",
        "print(ppl_df[\"perplexity\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š Step 7: Visualize Perplexity Distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create histogram of perplexities\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Histogram\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(ppl_df[\"perplexity\"], bins=30, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel(\"Perplexity\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Sentence-Level Perplexities\")\n",
        "plt.axvline(ppl_df[\"perplexity\"].mean(), color='red', linestyle='--', label=f'Mean: {ppl_df[\"perplexity\"].mean():.2f}')\n",
        "plt.axvline(ppl_df[\"perplexity\"].median(), color='green', linestyle='--', label=f'Median: {ppl_df[\"perplexity\"].median():.2f}')\n",
        "plt.legend()\n",
        "\n",
        "# Box plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(ppl_df[\"perplexity\"], vert=True)\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.title(\"Perplexity Box Plot\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Interpretation:\")\n",
        "print(\"- Lower perplexity = model predicted the text well\")\n",
        "print(\"- Higher perplexity = model was 'surprised' by the text\")\n",
        "print(\"- Outliers (high perplexity) may indicate unusual or rare content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“‹ Step 8: Show Lowest and Highest Perplexity Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort by perplexity\n",
        "sorted_df = ppl_df.sort_values(\"perplexity\")\n",
        "\n",
        "print(\"\\nðŸ† TOP 5 LOWEST PERPLEXITY (model predicted well):\")\n",
        "print(\"=\" * 70)\n",
        "for _, row in sorted_df.head(5).iterrows():\n",
        "    print(f\"PPL: {row['perplexity']:>8.2f} | {row['text'][:60]}...\")\n",
        "\n",
        "print(\"\\n\\nâš ï¸ TOP 5 HIGHEST PERPLEXITY (model was surprised):\")\n",
        "print(\"=\" * 70)\n",
        "for _, row in sorted_df.tail(5).iterrows():\n",
        "    print(f\"PPL: {row['perplexity']:>8.2f} | {row['text'][:60]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸš« Step 9: Understanding Perplexity Limitations\n",
        "\n",
        "### What Low vs High Perplexity Means\n",
        "\n",
        "| Perplexity | Interpretation |\n",
        "|------------|----------------|\n",
        "| Very Low (<50) | Model predicts almost perfectlyâ€”may indicate memorization or very common text |\n",
        "| Low (50-200) | Model handles this text wellâ€”likely similar to training distribution |\n",
        "| Medium (200-500) | Some uncertaintyâ€”text may have unusual patterns |\n",
        "| High (>500) | Model strugglesâ€”text is very different from training data |\n",
        "\n",
        "### Key Limitations\n",
        "\n",
        "1. **Perplexity â‰  Quality**: A model can have low perplexity on harmful content\n",
        "2. **Domain Specific**: Low perplexity on Wikipedia doesn't mean good at conversation\n",
        "3. **Memorization Risk**: Very low perplexity might indicate the model memorized training data\n",
        "4. **Not Task-Specific**: Doesn't measure if the model can answer questions, follow instructions, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration: Same perplexity, different quality\n",
        "demo_sentences = [\n",
        "    # Grammatically correct, factually wrong\n",
        "    \"The capital of France is Berlin.\",\n",
        "    # Grammatically correct, factually right\n",
        "    \"The capital of France is Paris.\",\n",
        "    # Both might have similar perplexity!\n",
        "]\n",
        "\n",
        "print(\"\\nðŸ”¬ Limitation Demo: Perplexity vs Factual Accuracy\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for sentence in demo_sentences:\n",
        "    ppl = compute_pseudo_perplexity(sentence, session, tokenizer)\n",
        "    print(f\"PPL: {ppl:>8.2f} | {sentence}\")\n",
        "\n",
        "print(\"\\nâš ï¸ Notice: Perplexity doesn't distinguish factual accuracy!\")\n",
        "print(\"   Both sentences may have similar perplexity because they're\")\n",
        "print(\"   grammatically similar, but only one is true.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Mini-Project: Compare Two Text Domains\n",
        "\n",
        "Your task: Compare perplexity on formal vs casual text.\n",
        "\n",
        "### Instructions:\n",
        "1. Define 5-10 sentences for each domain\n",
        "2. Compute perplexity for each\n",
        "3. Compare average perplexities\n",
        "4. Analyze what this tells us about the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample data for the mini-project\n",
        "formal_texts = [\n",
        "    \"The Federal Reserve announced a quarter-point interest rate increase.\",\n",
        "    \"Scientists discovered a new species of deep-sea fish in the Pacific Ocean.\",\n",
        "    \"The United Nations Security Council convened for an emergency session.\",\n",
        "    \"Economic indicators suggest moderate growth in the coming quarter.\",\n",
        "    \"The research team published their findings in a peer-reviewed journal.\",\n",
        "]\n",
        "\n",
        "casual_texts = [\n",
        "    \"hey whats up, u free tonight?\",\n",
        "    \"lol that was so funny i cant even\",\n",
        "    \"gonna grab some food, want anything?\",\n",
        "    \"ngl this new song is fire\",\n",
        "    \"omg did u see what happened yesterday??\",\n",
        "]\n",
        "\n",
        "# YOUR CODE: Compute and compare perplexities\n",
        "print(\"ðŸ“Š Formal Text Perplexities:\")\n",
        "formal_ppls = []\n",
        "for text in formal_texts:\n",
        "    ppl = compute_pseudo_perplexity(text, session, tokenizer)\n",
        "    formal_ppls.append(ppl)\n",
        "    print(f\"  PPL: {ppl:>8.2f} | {text[:50]}...\")\n",
        "\n",
        "print(f\"\\n  Average: {np.mean(formal_ppls):.2f}\")\n",
        "\n",
        "print(\"\\nðŸ“Š Casual Text Perplexities:\")\n",
        "casual_ppls = []\n",
        "for text in casual_texts:\n",
        "    ppl = compute_pseudo_perplexity(text, session, tokenizer)\n",
        "    casual_ppls.append(ppl)\n",
        "    print(f\"  PPL: {ppl:>8.2f} | {text[:50]}...\")\n",
        "\n",
        "print(f\"\\n  Average: {np.mean(casual_ppls):.2f}\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Analysis:\")\n",
        "if np.mean(formal_ppls) < np.mean(casual_ppls):\n",
        "    print(\"   Formal text has lower perplexity - model was likely trained on more formal text\")\n",
        "else:\n",
        "    print(\"   Casual text has lower perplexity - model may have seen more casual content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 4, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I can explain perplexity using the \"surprise\" analogy\n",
        "- [ ] I understand perplexity as exponentiated average negative log-likelihood\n",
        "- [ ] I can implement pseudo-perplexity calculation\n",
        "- [ ] I can interpret perplexity histograms\n",
        "- [ ] I understand why perplexity alone isn't enough for evaluation\n",
        "- [ ] I completed the mini-project comparing text domains\n",
        "\n",
        "---\n",
        "\n",
        "**Week 3 Complete!** ðŸŽ‰\n",
        "\n",
        "**Next:** *Week 4 â€” Industry Benchmark Suites (MMLU, HellaSwag, BBH, TruthfulQA, ToxiGen)*"
      ]
    }
  ]
}
