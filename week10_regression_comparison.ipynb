{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 10 ‚Äî Regression & Version Comparison\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand regression testing concepts and why they matter\n",
        "2. Use the `compare_runs` function to compare benchmark results\n",
        "3. Use the `summarize_regressions` function to identify performance drops\n",
        "4. Analyze regression severity and prioritize fixes\n",
        "5. Generate comprehensive regression reports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† Why Regression Testing Matters\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "When updating LLM models, improvements in some areas can cause degradation in others:\n",
        "\n",
        "| Scenario | Improvement | Potential Regression |\n",
        "|----------|-------------|---------------------|\n",
        "| **Model fine-tuning** | Better task performance | Worse general knowledge |\n",
        "| **Quantization** | Faster inference | Lower accuracy |\n",
        "| **Architecture change** | More efficient | Different failure modes |\n",
        "\n",
        "### Why Compare Runs?\n",
        "\n",
        "- Catch degradation before deployment\n",
        "- Make data-driven upgrade decisions\n",
        "- Prioritize which regressions to fix\n",
        "- Track quality over time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Install dependencies if needed\n",
        "# !pip install pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Step 2: Import the Reporting Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the reporting functions\n",
        "from src.benchmark_engine.reporting import (\n",
        "    compare_runs,\n",
        "    summarize_regressions,\n",
        "    generate_regression_report,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Reporting module imported successfully!\")\n",
        "print(\"\\nüìã Available functions:\")\n",
        "print(\"   - compare_runs: Compare metrics between two benchmark runs\")\n",
        "print(\"   - summarize_regressions: Identify cases where new model is worse\")\n",
        "print(\"   - generate_regression_report: Create comprehensive regression analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 3: Create Synthetic Benchmark Data\n",
        "\n",
        "We'll create two synthetic DataFrames representing benchmark results from:\n",
        "- **Run A (Baseline):** Current production model\n",
        "- **Run B (New Model):** Updated model we're evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic benchmark data for baseline model (Run A)\n",
        "run_a = pd.DataFrame({\n",
        "    \"prompt\": [\n",
        "        \"What is the capital of France?\",\n",
        "        \"Explain machine learning in simple terms.\",\n",
        "        \"What is 2+2?\",\n",
        "        \"Define artificial intelligence.\",\n",
        "        \"What is the speed of light?\",\n",
        "        \"Summarize the key principles of software engineering.\",\n",
        "        \"What is the largest planet in our solar system?\",\n",
        "        \"Explain quantum computing basics.\",\n",
        "    ],\n",
        "    \"score\": [0.95, 0.88, 1.00, 0.92, 0.85, 0.78, 0.90, 0.82],\n",
        "    \"latency_ms\": [45, 62, 38, 55, 70, 85, 48, 95],\n",
        "    \"tokens_per_second\": [120, 95, 140, 105, 85, 72, 115, 65],\n",
        "})\n",
        "\n",
        "# Create synthetic benchmark data for new model (Run B)\n",
        "# Note: Some metrics improve, some regress\n",
        "run_b = pd.DataFrame({\n",
        "    \"prompt\": [\n",
        "        \"What is the capital of France?\",\n",
        "        \"Explain machine learning in simple terms.\",\n",
        "        \"What is 2+2?\",\n",
        "        \"Define artificial intelligence.\",\n",
        "        \"What is the speed of light?\",\n",
        "        \"Summarize the key principles of software engineering.\",\n",
        "        \"What is the largest planet in our solar system?\",\n",
        "        \"Explain quantum computing basics.\",\n",
        "    ],\n",
        "    \"score\": [0.92, 0.91, 1.00, 0.85, 0.88, 0.82, 0.88, 0.79],\n",
        "    \"latency_ms\": [42, 58, 40, 65, 68, 78, 52, 88],\n",
        "    \"tokens_per_second\": [125, 100, 135, 90, 88, 80, 110, 70],\n",
        "})\n",
        "\n",
        "print(\"üìä Run A (Baseline Model):\")\n",
        "print(\"=\" * 80)\n",
        "display(run_a)\n",
        "\n",
        "print(\"\\nüìä Run B (New Model):\")\n",
        "print(\"=\" * 80)\n",
        "display(run_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 4: Compare Benchmark Runs\n",
        "\n",
        "The `compare_runs` function merges two DataFrames and calculates differences for each metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare the two runs\n",
        "print(\"üìà Comparing Runs...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "diff_df = compare_runs(run_a, run_b, on=\"prompt\")\n",
        "\n",
        "print(\"\\nüìä Comparison DataFrame Columns:\")\n",
        "print(diff_df.columns.tolist())\n",
        "\n",
        "print(\"\\nüìä Merged Data with Differences:\")\n",
        "display(diff_df[['prompt', 'score_a', 'score_b', 'score_diff', 'latency_ms_diff']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the differences\n",
        "print(\"üìä Difference Summary Statistics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüéØ Score Differences (new - baseline):\")\n",
        "print(f\"   Mean:  {diff_df['score_diff'].mean():.4f}\")\n",
        "print(f\"   Std:   {diff_df['score_diff'].std():.4f}\")\n",
        "print(f\"   Min:   {diff_df['score_diff'].min():.4f}\")\n",
        "print(f\"   Max:   {diff_df['score_diff'].max():.4f}\")\n",
        "\n",
        "print(\"\\n‚è±Ô∏è Latency Differences (new - baseline):\")\n",
        "print(f\"   Mean:  {diff_df['latency_ms_diff'].mean():.2f} ms\")\n",
        "print(f\"   Std:   {diff_df['latency_ms_diff'].std():.2f} ms\")\n",
        "print(f\"   Min:   {diff_df['latency_ms_diff'].min():.2f} ms\")\n",
        "print(f\"   Max:   {diff_df['latency_ms_diff'].max():.2f} ms\")\n",
        "\n",
        "# Interpretation\n",
        "avg_score_diff = diff_df['score_diff'].mean()\n",
        "avg_latency_diff = diff_df['latency_ms_diff'].mean()\n",
        "\n",
        "print(\"\\nüìù Interpretation:\")\n",
        "if avg_score_diff > 0:\n",
        "    print(f\"   ‚úÖ Average score improved by {avg_score_diff:.4f}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Average score decreased by {abs(avg_score_diff):.4f}\")\n",
        "\n",
        "if avg_latency_diff < 0:\n",
        "    print(f\"   ‚úÖ Average latency improved by {abs(avg_latency_diff):.2f} ms\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è Average latency increased by {avg_latency_diff:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 5: Identify Score Regressions\n",
        "\n",
        "For score metrics where **higher is better**, a regression occurs when the new model scores lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find score regressions (higher is better)\n",
        "print(\"üîç Finding Score Regressions (higher is better)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "score_regressions = summarize_regressions(\n",
        "    diff_df, \n",
        "    metric=\"score\", \n",
        "    higher_is_better=True\n",
        ")\n",
        "\n",
        "if not score_regressions.empty:\n",
        "    print(f\"\\n‚ö†Ô∏è Found {len(score_regressions)} score regressions:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Display regressions sorted by severity\n",
        "    display(score_regressions[['prompt', 'score_a', 'score_b', 'score_diff', 'regression_severity']])\n",
        "    \n",
        "    print(\"\\nüìã Regression Details:\")\n",
        "    for i, (_, row) in enumerate(score_regressions.iterrows(), 1):\n",
        "        print(f\"\\n   [{i}] {row['prompt'][:50]}...\")\n",
        "        print(f\"       Baseline: {row['score_a']:.2f} ‚Üí New: {row['score_b']:.2f}\")\n",
        "        print(f\"       Change: {row['score_diff']:.4f} (Severity: {row['regression_severity']:.4f})\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No score regressions found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 6: Identify Latency Regressions\n",
        "\n",
        "For latency metrics where **lower is better**, a regression occurs when the new model is slower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find latency regressions (lower is better)\n",
        "print(\"üîç Finding Latency Regressions (lower is better)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "latency_regressions = summarize_regressions(\n",
        "    diff_df, \n",
        "    metric=\"latency_ms\", \n",
        "    higher_is_better=False\n",
        ")\n",
        "\n",
        "if not latency_regressions.empty:\n",
        "    print(f\"\\n‚ö†Ô∏è Found {len(latency_regressions)} latency regressions:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    # Display regressions sorted by severity\n",
        "    display(latency_regressions[['prompt', 'latency_ms_a', 'latency_ms_b', 'latency_ms_diff', 'regression_severity']])\n",
        "    \n",
        "    print(\"\\nüìã Regression Details:\")\n",
        "    for i, (_, row) in enumerate(latency_regressions.iterrows(), 1):\n",
        "        print(f\"\\n   [{i}] {row['prompt'][:50]}...\")\n",
        "        print(f\"       Baseline: {row['latency_ms_a']:.0f}ms ‚Üí New: {row['latency_ms_b']:.0f}ms\")\n",
        "        print(f\"       Slowdown: +{row['latency_ms_diff']:.0f}ms\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No latency regressions found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 7: Identify Throughput Regressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find throughput regressions (higher is better)\n",
        "print(\"üîç Finding Throughput Regressions (higher is better)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "throughput_regressions = summarize_regressions(\n",
        "    diff_df, \n",
        "    metric=\"tokens_per_second\", \n",
        "    higher_is_better=True\n",
        ")\n",
        "\n",
        "if not throughput_regressions.empty:\n",
        "    print(f\"\\n‚ö†Ô∏è Found {len(throughput_regressions)} throughput regressions:\")\n",
        "    print(\"-\" * 80)\n",
        "    \n",
        "    display(throughput_regressions[['prompt', 'tokens_per_second_a', 'tokens_per_second_b', 'tokens_per_second_diff', 'regression_severity']])\n",
        "else:\n",
        "    print(\"\\n‚úÖ No throughput regressions found!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 8: Generate Comprehensive Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive regression report\n",
        "print(\"üìã Generating Full Regression Report...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "report = generate_regression_report(\n",
        "    run_a,\n",
        "    run_b,\n",
        "    on=\"prompt\",\n",
        "    metrics=[\"score\", \"latency_ms\", \"tokens_per_second\"],\n",
        "    metric_directions={\n",
        "        \"score\": True,             # Higher is better\n",
        "        \"latency_ms\": False,       # Lower is better\n",
        "        \"tokens_per_second\": True, # Higher is better\n",
        "    },\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Report Summary:\")\n",
        "print(f\"   Total test cases: {report['total_cases']}\")\n",
        "print(f\"   Metrics analyzed: {report['metrics_analyzed']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display per-metric summary\n",
        "print(\"üìà Per-Metric Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for metric, stats in report['summary'].items():\n",
        "    direction_symbol = \"‚Üë\" if stats['higher_is_better'] else \"‚Üì\"\n",
        "    \n",
        "    print(f\"\\nüîπ {metric} ({direction_symbol} = better):\")\n",
        "    print(f\"   Mean difference: {stats['mean_diff']:.4f}\")\n",
        "    print(f\"   Std difference:  {stats['std_diff']:.4f}\")\n",
        "    print(f\"   Min difference:  {stats['min_diff']:.4f}\")\n",
        "    print(f\"   Max difference:  {stats['max_diff']:.4f}\")\n",
        "    print(f\"   Regressions:     {stats['total_regressions']} ({stats['regression_rate']:.1%})\")\n",
        "    \n",
        "    if stats['total_regressions'] > 0:\n",
        "        print(f\"   Max severity:    {stats['max_regression_severity']:.4f}\")\n",
        "        print(f\"   Mean severity:   {stats['mean_regression_severity']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 9: Create Regression Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a summary table for all regressions\n",
        "print(\"üìä Regression Summary Table\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "summary_data = []\n",
        "for metric, stats in report['summary'].items():\n",
        "    direction = \"‚Üë better\" if stats['higher_is_better'] else \"‚Üì better\"\n",
        "    summary_data.append({\n",
        "        \"Metric\": metric,\n",
        "        \"Direction\": direction,\n",
        "        \"Mean Œî\": f\"{stats['mean_diff']:.4f}\",\n",
        "        \"Regressions\": stats['total_regressions'],\n",
        "        \"Rate\": f\"{stats['regression_rate']:.1%}\",\n",
        "        \"Max Severity\": f\"{stats.get('max_regression_severity', 0):.4f}\" if stats['total_regressions'] > 0 else \"N/A\",\n",
        "    })\n",
        "\n",
        "summary_table = pd.DataFrame(summary_data)\n",
        "display(summary_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 10: Using Thresholds to Filter Noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use thresholds to filter out small fluctuations\n",
        "print(\"üîß Filtering Regressions with Threshold\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Only count regressions > 5% drop as significant\n",
        "threshold = 0.05\n",
        "\n",
        "print(f\"\\nüìã Using threshold: {threshold} (5% minimum regression)\")\n",
        "\n",
        "significant_regressions = summarize_regressions(\n",
        "    diff_df,\n",
        "    metric=\"score\",\n",
        "    threshold=threshold,\n",
        "    higher_is_better=True,\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Without threshold: {len(score_regressions)} regressions\")\n",
        "print(f\"üìä With 5% threshold: {len(significant_regressions)} significant regressions\")\n",
        "\n",
        "if not significant_regressions.empty:\n",
        "    print(\"\\n‚ö†Ô∏è Significant regressions (> 5%):\")\n",
        "    display(significant_regressions[['prompt', 'score_a', 'score_b', 'score_diff', 'regression_severity']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 11: Identify Improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find cases where the new model improved\n",
        "print(\"‚úÖ Identifying Improvements\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Score improvements (positive diff for higher-is-better)\n",
        "score_improvements = diff_df[diff_df['score_diff'] > 0].copy()\n",
        "\n",
        "print(f\"\\nüéØ Score Improvements: {len(score_improvements)} cases\")\n",
        "if not score_improvements.empty:\n",
        "    for _, row in score_improvements.iterrows():\n",
        "        print(f\"   ‚úì '{row['prompt'][:40]}...': {row['score_a']:.2f} ‚Üí {row['score_b']:.2f} (+{row['score_diff']:.2f})\")\n",
        "\n",
        "# Latency improvements (negative diff for lower-is-better)\n",
        "latency_improvements = diff_df[diff_df['latency_ms_diff'] < 0].copy()\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è Latency Improvements: {len(latency_improvements)} cases\")\n",
        "if not latency_improvements.empty:\n",
        "    for _, row in latency_improvements.iterrows():\n",
        "        print(f\"   ‚úì '{row['prompt'][:40]}...': {row['latency_ms_a']:.0f}ms ‚Üí {row['latency_ms_b']:.0f}ms ({row['latency_ms_diff']:.0f}ms)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Mini-Project: Regression Audit\n",
        "\n",
        "### Task\n",
        "\n",
        "Create a comprehensive regression analysis for a model update.\n",
        "\n",
        "### Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your regression audit code here\n",
        "\n",
        "# Step 1: Create or load your benchmark data\n",
        "# my_baseline = pd.DataFrame({...})\n",
        "# my_new_model = pd.DataFrame({...})\n",
        "\n",
        "# Step 2: Compare runs\n",
        "# diff = compare_runs(my_baseline, my_new_model, on='prompt')\n",
        "\n",
        "# Step 3: Analyze regressions for each metric\n",
        "# - Use summarize_regressions for quality metrics\n",
        "# - Use summarize_regressions with higher_is_better=False for latency\n",
        "\n",
        "# Step 4: Generate report\n",
        "# report = generate_regression_report(...)\n",
        "\n",
        "# Step 5: Make a deployment decision\n",
        "# - How many regressions are acceptable?\n",
        "# - What severity threshold is critical?\n",
        "# - Do improvements outweigh regressions?\n",
        "\n",
        "print(\"üìù Complete the mini-project using the template above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§î Paul-Elder Critical Thinking Questions\n",
        "\n",
        "Reflect on these questions:\n",
        "\n",
        "### Question 1: EVIDENCE\n",
        "**If a model improves average score but has severe regressions on 10% of cases, should you deploy?**\n",
        "*Consider: Severity of regressions, user impact, rollback capability, monitoring.*\n",
        "\n",
        "### Question 2: ASSUMPTIONS\n",
        "**What assumptions are we making about the representativeness of our test prompts?**\n",
        "*Consider: Production distribution, edge cases, domain coverage, user behavior.*\n",
        "\n",
        "### Question 3: IMPLICATIONS\n",
        "**If we set a threshold to ignore small regressions, what might we miss over time?**\n",
        "*Consider: Accumulation of small drops, compounding effects, gradual degradation.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitations of Regression Testing\n",
        "\n",
        "### What These Tests DON'T Cover\n",
        "\n",
        "1. **Statistical Significance:** Current implementation doesn't test if differences are statistically significant\n",
        "2. **Distribution Shift:** Test prompts may not represent production distribution\n",
        "3. **Qualitative Changes:** Some regressions are subjective and hard to measure\n",
        "4. **Side Effects:** Changes may affect other metrics not being tracked\n",
        "5. **Long-term Trends:** Single comparison doesn't show degradation over time\n",
        "\n",
        "### Future Improvements (TODO)\n",
        "\n",
        "- Statistical significance testing (p-values, confidence intervals)\n",
        "- Multi-version trend analysis\n",
        "- Automatic regression alerting\n",
        "- Visualization of regression distributions\n",
        "- Integration with CI/CD pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 11, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand why regression testing is critical for model updates\n",
        "- [ ] I can use `compare_runs` to compare two benchmark DataFrames\n",
        "- [ ] I can use `summarize_regressions` to identify performance drops\n",
        "- [ ] I understand the difference between metrics where higher/lower is better\n",
        "- [ ] I can interpret regression severity and prioritize fixes\n",
        "- [ ] I know the limitations of regression testing\n",
        "\n",
        "---\n",
        "\n",
        "**Week 10 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 11 ‚Äî Banking & Finance Use Cases*"
      ]
    }
  ]
}
