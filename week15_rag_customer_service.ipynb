{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 15 ‚Äî RAG & Customer Service Evaluation\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand how Retrieval-Augmented Generation (RAG) works for customer service\n",
        "2. Create a tiny FAQ corpus and index it with a simple vector store\n",
        "3. Implement retrieval and generation components for RAG\n",
        "4. Benchmark tinyGPT as a generator given retrieved snippets\n",
        "5. Evaluate answer groundedness using LLM-as-Judge and string matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† What is RAG?\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "LLMs have knowledge cutoffs and may not know about your company's specific policies:\n",
        "\n",
        "| Challenge | Without RAG | With RAG |\n",
        "|-----------|-------------|----------|\n",
        "| Knowledge | Limited to training data | Access to external knowledge base |\n",
        "| Accuracy | May hallucinate | Grounded in retrieved documents |\n",
        "| Updates | Requires retraining | Just update knowledge base |\n",
        "| Verification | Hard to trace claims | Can cite source documents |\n",
        "\n",
        "### The RAG Pipeline\n",
        "\n",
        "```\n",
        "Question ‚Üí Embed ‚Üí Search KB ‚Üí Retrieve Docs ‚Üí Generate Answer\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import numpy as np\n",
        "import sys\n",
        "import json\n",
        "from typing import Dict, List, Any, Tuple, Optional, Callable\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# For data display\n",
        "try:\n",
        "    from IPython.display import display, HTML\n",
        "except ImportError:\n",
        "    display = print\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"   NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Step 2: Define the FAQ Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Customer service FAQ corpus\n",
        "FAQ_CORPUS = [\n",
        "    {\n",
        "        \"id\": \"faq_001\",\n",
        "        \"question\": \"What is your return policy?\",\n",
        "        \"answer\": \"You can return any item within 30 days of purchase for a full refund. Items must be unused and in original packaging. Return shipping is free for defective items.\",\n",
        "        \"category\": \"returns\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"faq_002\",\n",
        "        \"question\": \"How long does shipping take?\",\n",
        "        \"answer\": \"Standard shipping takes 5-7 business days. Express shipping takes 2-3 business days. Free shipping is available on orders over $50.\",\n",
        "        \"category\": \"shipping\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"faq_003\",\n",
        "        \"question\": \"How do I track my order?\",\n",
        "        \"answer\": \"You can track your order by logging into your account and visiting the 'Order History' section. You will also receive tracking updates via email once your order ships.\",\n",
        "        \"category\": \"orders\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"faq_004\",\n",
        "        \"question\": \"What payment methods do you accept?\",\n",
        "        \"answer\": \"We accept Visa, Mastercard, American Express, PayPal, and Apple Pay. All transactions are secured with SSL encryption.\",\n",
        "        \"category\": \"payment\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"faq_005\",\n",
        "        \"question\": \"How do I cancel my order?\",\n",
        "        \"answer\": \"You can cancel your order within 1 hour of placing it by contacting customer support. After 1 hour, orders enter processing and cannot be canceled, but you can return the item once received.\",\n",
        "        \"category\": \"orders\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"faq_006\",\n",
        "        \"question\": \"Do you offer international shipping?\",\n",
        "        \"answer\": \"Yes, we ship to over 50 countries worldwide. International shipping rates vary by destination and typically take 10-14 business days.\",\n",
        "        \"category\": \"shipping\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"faq_007\",\n",
        "        \"question\": \"What if my item arrives damaged?\",\n",
        "        \"answer\": \"If your item arrives damaged, please contact us within 48 hours with photos of the damage. We will send a replacement at no additional cost and arrange free return shipping for the damaged item.\",\n",
        "        \"category\": \"returns\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"faq_008\",\n",
        "        \"question\": \"How do I change my shipping address?\",\n",
        "        \"answer\": \"You can update your shipping address in your account settings before placing an order. For orders already placed, contact customer support within 1 hour to request an address change.\",\n",
        "        \"category\": \"shipping\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìö FAQ Corpus: {len(FAQ_CORPUS)} entries\")\n",
        "print(\"\")\n",
        "print(\"Categories:\")\n",
        "categories = {}\n",
        "for faq in FAQ_CORPUS:\n",
        "    cat = faq[\"category\"]\n",
        "    categories[cat] = categories.get(cat, 0) + 1\n",
        "\n",
        "for cat, count in sorted(categories.items()):\n",
        "    print(f\"  ‚Ä¢ {cat}: {count} FAQs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 3: Implement the Simple Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleVectorStore:\n",
        "    \"\"\"\n",
        "    A simple vector store using numpy for demonstration.\n",
        "    \n",
        "    In production, use FAISS, Pinecone, Weaviate, or similar.\n",
        "    This implementation uses cosine similarity for retrieval.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the vector store.\"\"\"\n",
        "        self.documents: List[Dict[str, Any]] = []\n",
        "        self.embeddings: Optional[np.ndarray] = None\n",
        "        self.embed_fn = None\n",
        "    \n",
        "    def set_embedding_function(self, embed_fn):\n",
        "        \"\"\"\n",
        "        Set the embedding function.\n",
        "        \n",
        "        Args:\n",
        "            embed_fn: Function that takes text and returns embedding vector\n",
        "        \"\"\"\n",
        "        self.embed_fn = embed_fn\n",
        "    \n",
        "    def add_documents(self, documents: List[Dict[str, Any]]) -> None:\n",
        "        \"\"\"\n",
        "        Add documents to the vector store.\n",
        "        \n",
        "        Args:\n",
        "            documents: List of documents with 'answer' or 'text' field\n",
        "        \"\"\"\n",
        "        self.documents = documents\n",
        "        \n",
        "        if self.embed_fn is None:\n",
        "            raise ValueError(\"Embedding function not set. Call set_embedding_function first.\")\n",
        "        \n",
        "        # Generate embeddings for all documents\n",
        "        texts = [doc.get(\"answer\", doc.get(\"text\", \"\")) for doc in documents]\n",
        "        embeddings_list = [self.embed_fn(text) for text in texts]\n",
        "        self.embeddings = np.array(embeddings_list)\n",
        "    \n",
        "    def search(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 3,\n",
        "    ) -> List[Tuple[Dict[str, Any], float]]:\n",
        "        \"\"\"\n",
        "        Search for similar documents.\n",
        "        \n",
        "        Args:\n",
        "            query: Search query\n",
        "            top_k: Number of results to return\n",
        "            \n",
        "        Returns:\n",
        "            List of (document, similarity_score) tuples\n",
        "        \"\"\"\n",
        "        if self.embeddings is None or len(self.documents) == 0:\n",
        "            return []\n",
        "        \n",
        "        # Embed the query\n",
        "        query_embedding = np.array(self.embed_fn(query))\n",
        "        \n",
        "        # Compute cosine similarity\n",
        "        similarities = self._cosine_similarity(query_embedding, self.embeddings)\n",
        "        \n",
        "        # Get top-k indices\n",
        "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
        "        \n",
        "        # Return documents with scores\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append((self.documents[idx], float(similarities[idx])))\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _cosine_similarity(\n",
        "        self,\n",
        "        query: np.ndarray,\n",
        "        documents: np.ndarray,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute cosine similarity between query and documents.\n",
        "        \n",
        "        Args:\n",
        "            query: Query embedding (1D array)\n",
        "            documents: Document embeddings (2D array)\n",
        "            \n",
        "        Returns:\n",
        "            Array of similarity scores\n",
        "        \"\"\"\n",
        "        # Normalize vectors\n",
        "        query_norm = query / (np.linalg.norm(query) + 1e-8)\n",
        "        doc_norms = documents / (np.linalg.norm(documents, axis=1, keepdims=True) + 1e-8)\n",
        "        \n",
        "        # Compute dot product\n",
        "        similarities = np.dot(doc_norms, query_norm)\n",
        "        \n",
        "        return similarities\n",
        "\n",
        "\n",
        "print(\"‚úÖ SimpleVectorStore class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üßÆ Step 4: Implement Simple Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vocabulary for bag-of-words embedding\n",
        "VOCAB = [\n",
        "    \"return\", \"refund\", \"shipping\", \"ship\", \"order\", \"payment\",\n",
        "    \"track\", \"cancel\", \"day\", \"days\", \"hour\", \"free\",\n",
        "    \"international\", \"damaged\", \"broken\", \"address\", \"account\",\n",
        "    \"paypal\", \"visa\", \"credit\", \"policy\", \"replace\", \"replacement\",\n",
        "    \"express\", \"standard\", \"worldwide\", \"countries\", \"email\",\n",
        "    \"photos\", \"48\", \"30\", \"1\", \"package\", \"item\", \"items\"\n",
        "]\n",
        "\n",
        "\n",
        "def simple_bow_embedding(text: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Create a simple bag-of-words embedding.\n",
        "    \n",
        "    For production, use sentence-transformers or similar.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to embed\n",
        "        \n",
        "    Returns:\n",
        "        Embedding vector as numpy array\n",
        "    \"\"\"\n",
        "    words = text.lower().split()\n",
        "    embedding = np.array([words.count(w) for w in VOCAB], dtype=np.float32)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "# Test the embedding function\n",
        "test_text = \"How do I return an item for a refund?\"\n",
        "test_embedding = simple_bow_embedding(test_text)\n",
        "\n",
        "print(\"‚úÖ Embedding function defined!\")\n",
        "print(f\"\")\n",
        "print(f\"Test text: '{test_text}'\")\n",
        "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
        "print(f\"Non-zero features: {np.sum(test_embedding > 0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèóÔ∏è Step 5: Initialize Vector Store with FAQ Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and populate vector store\n",
        "vector_store = SimpleVectorStore()\n",
        "vector_store.set_embedding_function(simple_bow_embedding)\n",
        "vector_store.add_documents(FAQ_CORPUS)\n",
        "\n",
        "print(\"‚úÖ Vector store initialized!\")\n",
        "print(f\"   Documents indexed: {len(FAQ_CORPUS)}\")\n",
        "print(f\"   Embedding shape: {vector_store.embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 6: Test Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test retrieval with sample questions\n",
        "test_questions = [\n",
        "    \"Can I get a refund?\",\n",
        "    \"How fast is delivery?\",\n",
        "    \"Can I pay with PayPal?\",\n",
        "    \"My package is broken\",\n",
        "]\n",
        "\n",
        "print(\"üîç Testing Retrieval...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\nQuestion: {question}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    results = vector_store.search(question, top_k=3)\n",
        "    \n",
        "    for i, (doc, score) in enumerate(results, 1):\n",
        "        print(f\"  {i}. [{doc['id']}] Score: {score:.3f}\")\n",
        "        print(f\"     Q: {doc['question'][:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§ñ Step 7: Implement Mock Generator (tinyGPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockTinyGPT:\n",
        "    \"\"\"\n",
        "    Mock generator that simulates tinyGPT behavior.\n",
        "    \n",
        "    For demonstration, uses template-based generation\n",
        "    based on retrieved context.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the mock generator.\"\"\"\n",
        "        self.templates = [\n",
        "            \"Based on our FAQ: {context}\",\n",
        "            \"According to our policies: {context}\",\n",
        "            \"Here's what you need to know: {context}\",\n",
        "        ]\n",
        "    \n",
        "    def generate(\n",
        "        self,\n",
        "        question: str,\n",
        "        context_list: List[str],\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer given question and context.\n",
        "        \n",
        "        Args:\n",
        "            question: Customer question\n",
        "            context_list: List of retrieved context snippets\n",
        "            \n",
        "        Returns:\n",
        "            Generated answer\n",
        "        \"\"\"\n",
        "        if not context_list:\n",
        "            return \"I don't have information about that topic. Please contact customer support.\"\n",
        "        \n",
        "        # Use the first (most relevant) context\n",
        "        main_context = context_list[0]\n",
        "        \n",
        "        # Select template based on question hash\n",
        "        template_idx = len(question) % len(self.templates)\n",
        "        template = self.templates[template_idx]\n",
        "        \n",
        "        return template.format(context=main_context)\n",
        "\n",
        "\n",
        "# Create generator\n",
        "mock_generator = MockTinyGPT()\n",
        "\n",
        "\n",
        "def generator_fn(question: str, context_list: List[str]) -> str:\n",
        "    \"\"\"Wrapper function for the generator.\"\"\"\n",
        "    return mock_generator.generate(question, context_list)\n",
        "\n",
        "\n",
        "print(\"‚úÖ Mock tinyGPT generator created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 8: Implement RAG Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAGEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluator for Retrieval-Augmented Generation systems.\n",
        "    \n",
        "    Evaluates:\n",
        "    1. Retrieval quality (precision, recall)\n",
        "    2. Answer groundedness (string match and LLM judge)\n",
        "    3. Overall answer quality\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_store: SimpleVectorStore,\n",
        "        generator_fn: Callable[[str, List[str]], str],\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the RAGEvaluator.\n",
        "        \n",
        "        Args:\n",
        "            vector_store: Vector store for retrieval\n",
        "            generator_fn: Function that takes (question, context_list) and returns answer\n",
        "        \"\"\"\n",
        "        self.vector_store = vector_store\n",
        "        self.generator_fn = generator_fn\n",
        "    \n",
        "    def retrieve(\n",
        "        self,\n",
        "        question: str,\n",
        "        top_k: int = 3,\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Retrieve relevant documents for a question.\n",
        "        \n",
        "        Args:\n",
        "            question: Customer question\n",
        "            top_k: Number of documents to retrieve\n",
        "            \n",
        "        Returns:\n",
        "            List of retrieved documents with scores\n",
        "        \"\"\"\n",
        "        results = self.vector_store.search(question, top_k=top_k)\n",
        "        return [{\"document\": doc, \"score\": score} for doc, score in results]\n",
        "    \n",
        "    def generate_answer(\n",
        "        self,\n",
        "        question: str,\n",
        "        retrieved_docs: List[Dict[str, Any]],\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate an answer using retrieved context.\n",
        "        \n",
        "        Args:\n",
        "            question: Customer question\n",
        "            retrieved_docs: Retrieved documents\n",
        "            \n",
        "        Returns:\n",
        "            Generated answer\n",
        "        \"\"\"\n",
        "        context_list = [\n",
        "            doc[\"document\"].get(\"answer\", doc[\"document\"].get(\"text\", \"\"))\n",
        "            for doc in retrieved_docs\n",
        "        ]\n",
        "        return self.generator_fn(question, context_list)\n",
        "    \n",
        "    def evaluate_retrieval(\n",
        "        self,\n",
        "        retrieved_ids: List[str],\n",
        "        expected_ids: List[str],\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluate retrieval quality.\n",
        "        \n",
        "        Args:\n",
        "            retrieved_ids: IDs of retrieved documents\n",
        "            expected_ids: IDs of expected relevant documents\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with precision, recall, and hit_rate\n",
        "        \"\"\"\n",
        "        retrieved_set = set(retrieved_ids)\n",
        "        expected_set = set(expected_ids)\n",
        "        \n",
        "        if len(retrieved_ids) == 0:\n",
        "            return {\"precision\": 0.0, \"recall\": 0.0, \"hit_rate\": 0.0}\n",
        "        \n",
        "        hits = len(retrieved_set & expected_set)\n",
        "        \n",
        "        precision = hits / len(retrieved_ids) if retrieved_ids else 0.0\n",
        "        recall = hits / len(expected_ids) if expected_ids else 0.0\n",
        "        hit_rate = 1.0 if hits > 0 else 0.0\n",
        "        \n",
        "        return {\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"hit_rate\": hit_rate,\n",
        "        }\n",
        "    \n",
        "    def evaluate_groundedness_string_match(\n",
        "        self,\n",
        "        answer: str,\n",
        "        expected_phrases: List[str],\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate answer groundedness using string matching.\n",
        "        \n",
        "        Args:\n",
        "            answer: Generated answer\n",
        "            expected_phrases: Phrases that should appear in the answer\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with groundedness score and matched phrases\n",
        "        \"\"\"\n",
        "        answer_lower = answer.lower()\n",
        "        matched = []\n",
        "        unmatched = []\n",
        "        \n",
        "        for phrase in expected_phrases:\n",
        "            if phrase.lower() in answer_lower:\n",
        "                matched.append(phrase)\n",
        "            else:\n",
        "                unmatched.append(phrase)\n",
        "        \n",
        "        score = len(matched) / len(expected_phrases) if expected_phrases else 0.0\n",
        "        \n",
        "        return {\n",
        "            \"groundedness_score\": score,\n",
        "            \"matched_phrases\": matched,\n",
        "            \"unmatched_phrases\": unmatched,\n",
        "            \"total_expected\": len(expected_phrases),\n",
        "        }\n",
        "    \n",
        "    def evaluate_groundedness_llm_judge(\n",
        "        self,\n",
        "        question: str,\n",
        "        answer: str,\n",
        "        context: List[str],\n",
        "        judge_fn: Callable[[str], str] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate answer groundedness using LLM-as-Judge.\n",
        "        \n",
        "        Args:\n",
        "            question: Original question\n",
        "            answer: Generated answer\n",
        "            context: Retrieved context snippets\n",
        "            judge_fn: Function that takes a prompt and returns judgment\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with groundedness assessment\n",
        "        \"\"\"\n",
        "        if judge_fn is None:\n",
        "            return {\n",
        "                \"grounded\": None,\n",
        "                \"explanation\": \"No judge function provided\",\n",
        "                \"hallucination_detected\": None,\n",
        "            }\n",
        "        \n",
        "        context_text = \"\\n\".join([f\"- {c}\" for c in context])\n",
        "        prompt = f\"\"\"Evaluate if this answer is grounded in the provided context.\n",
        "\n",
        "Context:\n",
        "{context_text}\n",
        "\n",
        "Question: {question}\n",
        "Answer: {answer}\n",
        "\n",
        "Is the answer grounded in the context? (yes/no)\n",
        "\"\"\"\n",
        "        \n",
        "        judgment = judge_fn(prompt)\n",
        "        grounded = \"yes\" in judgment.lower()\n",
        "        hallucination = \"no\" in judgment.lower() or \"not grounded\" in judgment.lower()\n",
        "        \n",
        "        return {\n",
        "            \"grounded\": grounded,\n",
        "            \"hallucination_detected\": hallucination,\n",
        "            \"raw_judgment\": judgment,\n",
        "        }\n",
        "    \n",
        "    def run_evaluation(\n",
        "        self,\n",
        "        question: str,\n",
        "        expected_faq_ids: List[str],\n",
        "        expected_answer_contains: List[str],\n",
        "        top_k: int = 3,\n",
        "        judge_fn: Callable[[str], str] = None,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Run a complete RAG evaluation on a single question.\n",
        "        \n",
        "        Args:\n",
        "            question: Customer question\n",
        "            expected_faq_ids: IDs of FAQs that should be retrieved\n",
        "            expected_answer_contains: Phrases that should appear in answer\n",
        "            top_k: Number of documents to retrieve\n",
        "            judge_fn: Optional LLM judge function\n",
        "            \n",
        "        Returns:\n",
        "            Complete evaluation results\n",
        "        \"\"\"\n",
        "        # Step 1: Retrieve\n",
        "        retrieved = self.retrieve(question, top_k=top_k)\n",
        "        retrieved_ids = [r[\"document\"].get(\"id\", \"\") for r in retrieved]\n",
        "        \n",
        "        # Step 2: Generate answer\n",
        "        answer = self.generate_answer(question, retrieved)\n",
        "        \n",
        "        # Step 3: Evaluate retrieval\n",
        "        retrieval_metrics = self.evaluate_retrieval(retrieved_ids, expected_faq_ids)\n",
        "        \n",
        "        # Step 4: Evaluate groundedness (string match)\n",
        "        groundedness_string = self.evaluate_groundedness_string_match(\n",
        "            answer, expected_answer_contains\n",
        "        )\n",
        "        \n",
        "        # Step 5: Evaluate groundedness (LLM judge) if provided\n",
        "        context = [r[\"document\"].get(\"answer\", \"\") for r in retrieved]\n",
        "        groundedness_llm = self.evaluate_groundedness_llm_judge(\n",
        "            question, answer, context, judge_fn\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"retrieved_docs\": retrieved,\n",
        "            \"retrieved_ids\": retrieved_ids,\n",
        "            \"answer\": answer,\n",
        "            \"retrieval_metrics\": retrieval_metrics,\n",
        "            \"groundedness_string\": groundedness_string,\n",
        "            \"groundedness_llm\": groundedness_llm,\n",
        "        }\n",
        "    \n",
        "    def compute_aggregate_metrics(\n",
        "        self,\n",
        "        results: List[Dict[str, Any]],\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute aggregate metrics across multiple evaluations.\n",
        "        \n",
        "        Args:\n",
        "            results: List of evaluation results\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with aggregate metrics\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return {}\n",
        "        \n",
        "        avg_precision = np.mean([r[\"retrieval_metrics\"][\"precision\"] for r in results])\n",
        "        avg_recall = np.mean([r[\"retrieval_metrics\"][\"recall\"] for r in results])\n",
        "        avg_hit_rate = np.mean([r[\"retrieval_metrics\"][\"hit_rate\"] for r in results])\n",
        "        avg_groundedness = np.mean([\n",
        "            r[\"groundedness_string\"][\"groundedness_score\"] for r in results\n",
        "        ])\n",
        "        \n",
        "        return {\n",
        "            \"avg_precision\": avg_precision,\n",
        "            \"avg_recall\": avg_recall,\n",
        "            \"avg_hit_rate\": avg_hit_rate,\n",
        "            \"avg_groundedness_score\": avg_groundedness,\n",
        "            \"total_evaluated\": len(results),\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úÖ RAGEvaluator class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 9: Define Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test cases for RAG evaluation\n",
        "TEST_CASES = [\n",
        "    {\n",
        "        \"question\": \"Can I get a refund if I don't like the product?\",\n",
        "        \"expected_faq_ids\": [\"faq_001\"],\n",
        "        \"expected_answer_contains\": [\"30 days\", \"refund\"],\n",
        "        \"category\": \"returns\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How fast is delivery?\",\n",
        "        \"expected_faq_ids\": [\"faq_002\"],\n",
        "        \"expected_answer_contains\": [\"5-7 business days\", \"express\"],\n",
        "        \"category\": \"shipping\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Where can I see my order status?\",\n",
        "        \"expected_faq_ids\": [\"faq_003\"],\n",
        "        \"expected_answer_contains\": [\"Order History\", \"account\"],\n",
        "        \"category\": \"orders\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Can I pay with PayPal?\",\n",
        "        \"expected_faq_ids\": [\"faq_004\"],\n",
        "        \"expected_answer_contains\": [\"PayPal\"],\n",
        "        \"category\": \"payment\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"My package arrived broken, what do I do?\",\n",
        "        \"expected_faq_ids\": [\"faq_007\"],\n",
        "        \"expected_answer_contains\": [\"48 hours\", \"replacement\", \"photos\"],\n",
        "        \"category\": \"returns\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Do you ship to Canada?\",\n",
        "        \"expected_faq_ids\": [\"faq_006\"],\n",
        "        \"expected_answer_contains\": [\"international\", \"50 countries\"],\n",
        "        \"category\": \"shipping\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"I want to cancel my order\",\n",
        "        \"expected_faq_ids\": [\"faq_005\"],\n",
        "        \"expected_answer_contains\": [\"1 hour\", \"cancel\"],\n",
        "        \"category\": \"orders\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìã Defined {len(TEST_CASES)} test cases:\")\n",
        "print(\"\")\n",
        "for tc in TEST_CASES:\n",
        "    print(f\"  [{tc['category']}] {tc['question'][:40]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèÉ Step 10: Run Full RAG Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluator\n",
        "rag_evaluator = RAGEvaluator(\n",
        "    vector_store=vector_store,\n",
        "    generator_fn=generator_fn,\n",
        ")\n",
        "\n",
        "# Run evaluation\n",
        "print(\"üîÑ Running Full RAG Evaluation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_results = []\n",
        "for tc in TEST_CASES:\n",
        "    result = rag_evaluator.run_evaluation(\n",
        "        question=tc[\"question\"],\n",
        "        expected_faq_ids=tc[\"expected_faq_ids\"],\n",
        "        expected_answer_contains=tc[\"expected_answer_contains\"],\n",
        "    )\n",
        "    all_results.append(result)\n",
        "    \n",
        "    # Display results\n",
        "    hit_status = \"‚úÖ\" if result[\"retrieval_metrics\"][\"hit_rate\"] > 0 else \"‚ùå\"\n",
        "    ground_status = \"‚úÖ\" if result[\"groundedness_string\"][\"groundedness_score\"] > 0.5 else \"‚ùå\"\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Question: {tc['question']}\")\n",
        "    print(f\"Category: {tc['category']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Answer: {result['answer'][:80]}...\")\n",
        "    print(f\"\")\n",
        "    print(f\"Retrieval: {hit_status}\")\n",
        "    print(f\"   Retrieved: {result['retrieved_ids'][:3]}\")\n",
        "    print(f\"   Expected: {tc['expected_faq_ids']}\")\n",
        "    print(f\"   Hit Rate: {result['retrieval_metrics']['hit_rate']:.0%}\")\n",
        "    print(f\"\")\n",
        "    print(f\"Groundedness: {ground_status}\")\n",
        "    print(f\"   Score: {result['groundedness_string']['groundedness_score']:.0%}\")\n",
        "    print(f\"   Matched: {result['groundedness_string']['matched_phrases']}\")\n",
        "    print(f\"   Unmatched: {result['groundedness_string']['unmatched_phrases']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 11: Compute and Display Aggregate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute aggregate metrics\n",
        "metrics = rag_evaluator.compute_aggregate_metrics(all_results)\n",
        "\n",
        "print(\"üìä Aggregate RAG Evaluation Metrics\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\")\n",
        "print(f\"Total Test Cases: {metrics['total_evaluated']}\")\n",
        "print(f\"\")\n",
        "print(f\"Retrieval Metrics:\")\n",
        "print(f\"   Average Precision: {metrics['avg_precision']:.0%}\")\n",
        "print(f\"   Average Recall: {metrics['avg_recall']:.0%}\")\n",
        "print(f\"   Average Hit Rate: {metrics['avg_hit_rate']:.0%}\")\n",
        "print(f\"\")\n",
        "print(f\"Groundedness Metrics:\")\n",
        "print(f\"   Average Groundedness Score: {metrics['avg_groundedness_score']:.0%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 12: Generate Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Evaluation Summary Table\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'#':<3} {'Question':<35} {'Category':<10} {'Retrieval':<12} {'Groundedness':<12}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, (tc, r) in enumerate(zip(TEST_CASES, all_results), 1):\n",
        "    retrieval_status = \"‚úÖ Hit\" if r[\"retrieval_metrics\"][\"hit_rate\"] > 0 else \"‚ùå Miss\"\n",
        "    groundedness_pct = r[\"groundedness_string\"][\"groundedness_score\"]\n",
        "    groundedness_status = f\"{groundedness_pct:.0%}\"\n",
        "    \n",
        "    question_short = tc[\"question\"][:33] + \"..\" if len(tc[\"question\"]) > 35 else tc[\"question\"]\n",
        "    \n",
        "    print(f\"{i:<3} {question_short:<35} {tc['category']:<10} {retrieval_status:<12} {groundedness_status:<12}\")\n",
        "\n",
        "print(\"-\" * 100)\n",
        "print(f\"\")\n",
        "print(f\"Summary: {metrics['avg_hit_rate']:.0%} hit rate, {metrics['avg_groundedness_score']:.0%} groundedness\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 13: Analyze Results by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze results by category\n",
        "print(\"ÔøΩÔøΩ Results by Category\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "category_results = {}\n",
        "for tc, r in zip(TEST_CASES, all_results):\n",
        "    cat = tc[\"category\"]\n",
        "    if cat not in category_results:\n",
        "        category_results[cat] = []\n",
        "    category_results[cat].append(r)\n",
        "\n",
        "for cat, results in sorted(category_results.items()):\n",
        "    hit_rate = np.mean([r[\"retrieval_metrics\"][\"hit_rate\"] for r in results])\n",
        "    groundedness = np.mean([r[\"groundedness_string\"][\"groundedness_score\"] for r in results])\n",
        "    \n",
        "    print(f\"\\n{cat.upper()} ({len(results)} questions):\")\n",
        "    print(f\"   Hit Rate: {hit_rate:.0%}\")\n",
        "    print(f\"   Groundedness: {groundedness:.0%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 14: Optional LLM-as-Judge Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mock LLM judge function (for demonstration)\n",
        "def mock_llm_judge(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Mock LLM judge that checks if answer contains context words.\n",
        "    \n",
        "    In production, use an actual LLM (GPT-4, Claude, etc.)\n",
        "    \"\"\"\n",
        "    # Simple heuristic: check if answer seems to contain policy info\n",
        "    if \"policies\" in prompt.lower() or \"faq\" in prompt.lower():\n",
        "        return \"yes, the answer appears to be grounded in the provided context\"\n",
        "    return \"no, the answer may contain unsupported claims\"\n",
        "\n",
        "\n",
        "print(\"üß™ LLM-as-Judge Evaluation (Mock)\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\")\n",
        "print(\"Note: Using mock judge for demonstration.\")\n",
        "print(\"      In production, use GPT-4, Claude, or similar.\")\n",
        "print(\"\")\n",
        "\n",
        "# Run evaluation with judge on a subset\n",
        "for tc in TEST_CASES[:3]:\n",
        "    result = rag_evaluator.run_evaluation(\n",
        "        question=tc[\"question\"],\n",
        "        expected_faq_ids=tc[\"expected_faq_ids\"],\n",
        "        expected_answer_contains=tc[\"expected_answer_contains\"],\n",
        "        judge_fn=mock_llm_judge,\n",
        "    )\n",
        "    \n",
        "    print(f\"Question: {tc['question'][:40]}...\")\n",
        "    print(f\"   String Match Score: {result['groundedness_string']['groundedness_score']:.0%}\")\n",
        "    print(f\"   LLM Judge Grounded: {result['groundedness_llm']['grounded']}\")\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 15: Failure Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç Failure Analysis\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Identify retrieval failures\n",
        "retrieval_failures = [\n",
        "    (tc, r) for tc, r in zip(TEST_CASES, all_results)\n",
        "    if r[\"retrieval_metrics\"][\"hit_rate\"] == 0\n",
        "]\n",
        "\n",
        "# Identify groundedness failures\n",
        "groundedness_failures = [\n",
        "    (tc, r) for tc, r in zip(TEST_CASES, all_results)\n",
        "    if r[\"groundedness_string\"][\"groundedness_score\"] < 0.5\n",
        "]\n",
        "\n",
        "print(f\"\")\n",
        "print(f\"Retrieval Failures: {len(retrieval_failures)} / {len(TEST_CASES)}\")\n",
        "if retrieval_failures:\n",
        "    for tc, r in retrieval_failures:\n",
        "        print(f\"   ‚ùå {tc['question'][:40]}...\")\n",
        "        print(f\"      Expected: {tc['expected_faq_ids']}\")\n",
        "        print(f\"      Got: {r['retrieved_ids'][:3]}\")\n",
        "\n",
        "print(f\"\")\n",
        "print(f\"Groundedness Failures: {len(groundedness_failures)} / {len(TEST_CASES)}\")\n",
        "if groundedness_failures:\n",
        "    for tc, r in groundedness_failures:\n",
        "        print(f\"   ‚ö†Ô∏è {tc['question'][:40]}...\")\n",
        "        print(f\"      Missing phrases: {r['groundedness_string']['unmatched_phrases']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 16: Test Custom Queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with custom queries\n",
        "print(\"üß™ Test Custom Queries\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "custom_queries = [\n",
        "    \"What's your refund policy?\",\n",
        "    \"How do I get my money back?\",\n",
        "    \"Is shipping free?\",\n",
        "    \"What credit cards do you take?\",\n",
        "]\n",
        "\n",
        "for query in custom_queries:\n",
        "    # Retrieve\n",
        "    results = vector_store.search(query, top_k=1)\n",
        "    top_doc, score = results[0] if results else (None, 0)\n",
        "    \n",
        "    # Generate\n",
        "    context = [top_doc[\"answer\"]] if top_doc else []\n",
        "    answer = generator_fn(query, context)\n",
        "    \n",
        "    print(f\"\\nQuery: {query}\")\n",
        "    print(f\"   Top Match: {top_doc['id'] if top_doc else 'None'} (score: {score:.3f})\")\n",
        "    print(f\"   Answer: {answer[:70]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Summary\n",
        "\n",
        "In this notebook, you learned how to:\n",
        "\n",
        "1. **Create a FAQ corpus** for customer service use cases\n",
        "2. **Implement a simple vector store** for semantic retrieval\n",
        "3. **Build a mock generator** (simulating tinyGPT)\n",
        "4. **Evaluate retrieval quality** using precision, recall, and hit rate\n",
        "5. **Evaluate groundedness** using string matching and LLM-as-Judge\n",
        "6. **Analyze failures** to identify areas for improvement\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. RAG combines retrieval and generation for grounded answers\n",
        "2. Retrieval quality directly impacts answer quality\n",
        "3. Groundedness can be evaluated with string matching or LLM judges\n",
        "4. Simple bag-of-words embeddings work for demonstrations but production systems need better embeddings\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Use FAISS** for efficient similarity search at scale\n",
        "2. **Use sentence-transformers** for better semantic embeddings\n",
        "3. **Integrate a real LLM** (tinyGPT ONNX) for generation\n",
        "4. **Add citation tracking** to show which FAQ the answer is based on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úî Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 16, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand what RAG is and why it's useful for customer service\n",
        "- [ ] I can create a FAQ corpus and explain its structure\n",
        "- [ ] I can implement a simple vector store for semantic retrieval\n",
        "- [ ] I understand how to evaluate retrieval quality (precision, recall, hit rate)\n",
        "- [ ] I can evaluate answer groundedness using string matching\n",
        "- [ ] I understand how to use an LLM as a judge for groundedness\n",
        "- [ ] I can identify common RAG failure modes\n",
        "- [ ] I understand the trade-off between groundedness and helpfulness\n",
        "\n",
        "---\n",
        "\n",
        "**Week 15 Complete!**\n",
        "\n",
        "*Next: Week 16 ‚Äî Marketing & Content Use Cases*"
      ]
    }
  ]
}
