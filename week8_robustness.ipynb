{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 8 ‚Äî Robustness Tests\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand robustness testing concepts and why they matter\n",
        "2. Use the `perturb_prompt` function to generate input variations\n",
        "3. Use the `robustness_sweep` function to measure model stability\n",
        "4. Analyze robustness evaluation results\n",
        "5. Identify model weaknesses through perturbation testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† Why Robustness Testing Matters\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "LLMs can produce inconsistent outputs when inputs are slightly modified:\n",
        "\n",
        "| Perturbation Type | Description | Example |\n",
        "|-------------------|-------------|----------|\n",
        "| **Typos** | Character-level errors from keyboard mistakes | \"Waht\" instead of \"What\" |\n",
        "| **Synonyms** | Different words with same meaning | \"capital\" vs \"main city\" |\n",
        "| **Reordering** | Words in different order | \"is what\" vs \"what is\" |\n",
        "\n",
        "### Why Test?\n",
        "\n",
        "- Real-world inputs are messy and varied\n",
        "- Users make typos and use diverse vocabulary\n",
        "- Inconsistent outputs undermine user trust\n",
        "- Production systems need reliable behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Step 2: Import the Robustness Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the robustness testing functions\n",
        "from src.benchmark_engine.robustness import perturb_prompt, robustness_sweep\n",
        "\n",
        "print(\"‚úÖ Robustness module imported successfully!\")\n",
        "print(\"\\nüìã Available functions:\")\n",
        "print(\"   - perturb_prompt: Generate perturbed input variants\")\n",
        "print(\"   - robustness_sweep: Measure model stability across perturbations\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 3: Understanding Perturbation Modes\n",
        "\n",
        "The `perturb_prompt` function supports three perturbation modes:\n",
        "- **typo**: Inject character-level errors based on keyboard proximity\n",
        "- **synonym**: Replace words with their synonyms\n",
        "- **reorder**: Swap adjacent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate perturbation modes\n",
        "original_prompt = \"What is the capital of France?\"\n",
        "\n",
        "print(\"üìù Perturbation Modes Demonstration\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nOriginal: {original_prompt}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for mode in [\"typo\", \"synonym\", \"reorder\"]:\n",
        "    perturbed = perturb_prompt(original_prompt, mode=mode, seed=42)\n",
        "    print(f\"[{mode:8}] {perturbed}\")\n",
        "\n",
        "print(\"\\n‚úÖ All perturbation modes demonstrated!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§ñ Step 4: Create Mock Models\n",
        "\n",
        "We'll create mock models for demonstration. In practice, you would use:\n",
        "- An ONNX model\n",
        "- A Hugging Face transformer\n",
        "- An API-based model (OpenAI, Anthropic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def robust_model(prompt: str) -> str:\n",
        "    \"\"\"A mock model that handles perturbations well.\"\"\"\n",
        "    prompt_lower = prompt.lower()\n",
        "    \n",
        "    # Handle capital of France with various perturbations\n",
        "    if (\"capital\" in prompt_lower or \"main city\" in prompt_lower or \n",
        "        \"chief city\" in prompt_lower) and \"france\" in prompt_lower:\n",
        "        return \"Paris\"\n",
        "    elif \"2+2\" in prompt_lower or \"2 + 2\" in prompt_lower:\n",
        "        return \"4\"\n",
        "    elif \"color\" in prompt_lower and \"sky\" in prompt_lower:\n",
        "        return \"The sky is blue.\"\n",
        "    elif \"largest\" in prompt_lower and \"planet\" in prompt_lower:\n",
        "        return \"Jupiter\"\n",
        "    elif (\"big\" in prompt_lower or \"huge\" in prompt_lower) and \"planet\" in prompt_lower:\n",
        "        return \"Jupiter\"\n",
        "    else:\n",
        "        return \"I'm not sure about that.\"\n",
        "\n",
        "\n",
        "def fragile_model(prompt: str) -> str:\n",
        "    \"\"\"A mock model that is sensitive to perturbations.\"\"\"\n",
        "    # Only exact matches work\n",
        "    exact_matches = {\n",
        "        \"What is the capital of France?\": \"Paris\",\n",
        "        \"What is 2+2?\": \"4\",\n",
        "        \"What color is the sky?\": \"Blue\",\n",
        "        \"What is the largest planet?\": \"Jupiter\",\n",
        "    }\n",
        "    return exact_matches.get(prompt, \"I don't understand the question.\")\n",
        "\n",
        "\n",
        "print(\"‚úÖ Mock models created!\")\n",
        "print(\"   - robust_model: Handles various input perturbations\")\n",
        "print(\"   - fragile_model: Only works with exact prompts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 5: Run Robustness Sweep on Robust Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run robustness sweep on the robust model\n",
        "print(\"üìä Robustness Sweep: Robust Model\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "robust_results = robustness_sweep(\n",
        "    model_fn=robust_model,\n",
        "    prompt=\"What is the capital of France?\",\n",
        "    n=15,  # Generate 15 variants\n",
        "    seed=42  # For reproducibility\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà Results:\")\n",
        "print(f\"   Original prompt: {robust_results['original_prompt']}\")\n",
        "print(f\"   Original output: {robust_results['original_output']}\")\n",
        "print(f\"   Total variants: {robust_results['total_variants']}\")\n",
        "print(f\"   Matching outputs: {robust_results['matching_outputs']}\")\n",
        "print(f\"   Stability score: {robust_results['stability_score']:.2%}\")\n",
        "print(f\"   Time: {robust_results['total_time_seconds']:.4f} seconds\")\n",
        "\n",
        "print(f\"\\nüìã Perturbation Breakdown:\")\n",
        "for mode, count in robust_results['perturbation_breakdown'].items():\n",
        "    print(f\"   {mode}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 6: Run Robustness Sweep on Fragile Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run robustness sweep on the fragile model\n",
        "print(\"üìä Robustness Sweep: Fragile Model\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fragile_results = robustness_sweep(\n",
        "    model_fn=fragile_model,\n",
        "    prompt=\"What is the capital of France?\",\n",
        "    n=15,  # Generate 15 variants\n",
        "    seed=42  # For reproducibility\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà Results:\")\n",
        "print(f\"   Original prompt: {fragile_results['original_prompt']}\")\n",
        "print(f\"   Original output: {fragile_results['original_output']}\")\n",
        "print(f\"   Total variants: {fragile_results['total_variants']}\")\n",
        "print(f\"   Matching outputs: {fragile_results['matching_outputs']}\")\n",
        "print(f\"   Stability score: {fragile_results['stability_score']:.2%}\")\n",
        "print(f\"   Time: {fragile_results['total_time_seconds']:.4f} seconds\")\n",
        "\n",
        "print(f\"\\nüìã Perturbation Breakdown:\")\n",
        "for mode, count in fragile_results['perturbation_breakdown'].items():\n",
        "    print(f\"   {mode}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 7: Compare Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Model Comparison\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(f\"| Model         | Stability Score | Matching Outputs |\")\n",
        "print(f\"|---------------|-----------------|------------------|\")\n",
        "print(f\"| Robust Model  | {robust_results['stability_score']:.1%}           | {robust_results['matching_outputs']}/{robust_results['total_variants']}              |\")\n",
        "print(f\"| Fragile Model | {fragile_results['stability_score']:.1%}            | {fragile_results['matching_outputs']}/{fragile_results['total_variants']}               |\")\n",
        "print()\n",
        "\n",
        "# Interpretation\n",
        "if robust_results['stability_score'] > fragile_results['stability_score']:\n",
        "    print(\"‚úÖ The robust model shows significantly better stability across perturbations.\")\n",
        "    print(\"   This is expected as it was designed to handle input variations.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Unexpected result: The fragile model appears more stable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 8: Analyze Failure Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Fragile Model: Failure Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Find and display failure cases\n",
        "failures = [r for r in fragile_results['results'] if not r['is_semantically_similar']]\n",
        "\n",
        "print(f\"\\nTotal failures: {len(failures)}/{fragile_results['total_variants']}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for i, result in enumerate(failures[:5], 1):  # Show first 5 failures\n",
        "    print(f\"\\n[Failure {i}] Mode: {result['perturbation_mode']}\")\n",
        "    print(f\"   Perturbed prompt: {result['perturbed_prompt']}\")\n",
        "    print(f\"   Original output:  {result['original_output']}\")\n",
        "    print(f\"   Perturbed output: {result['perturbed_output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 9: Analyze Success Cases for Robust Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Robust Model: Success Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Find and display success cases\n",
        "successes = [r for r in robust_results['results'] if r['is_semantically_similar']]\n",
        "\n",
        "print(f\"\\nTotal successes: {len(successes)}/{robust_results['total_variants']}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Group by perturbation mode\n",
        "for mode in [\"typo\", \"synonym\", \"reorder\"]:\n",
        "    mode_results = [r for r in successes if r['perturbation_mode'] == mode]\n",
        "    print(f\"\\n[{mode.upper()}] {len(mode_results)} successes\")\n",
        "    for r in mode_results[:2]:  # Show first 2 of each mode\n",
        "        print(f\"   Perturbed: {r['perturbed_prompt']}\")\n",
        "        print(f\"   Output:    {r['perturbed_output']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 10: Testing Multiple Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test robustness across multiple prompts\n",
        "test_prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"What is the largest planet?\",\n",
        "    \"What color is the sky?\",\n",
        "]\n",
        "\n",
        "print(\"üìä Multi-Prompt Robustness Evaluation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for prompt in test_prompts:\n",
        "    results = robustness_sweep(\n",
        "        model_fn=robust_model,\n",
        "        prompt=prompt,\n",
        "        n=10,\n",
        "        seed=42\n",
        "    )\n",
        "    all_results.append({\n",
        "        \"prompt\": prompt,\n",
        "        \"stability\": results['stability_score'],\n",
        "        \"matching\": results['matching_outputs'],\n",
        "        \"total\": results['total_variants'],\n",
        "    })\n",
        "\n",
        "print(\"\\nüìà Results Summary:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"| Prompt                           | Stability | Matching |\")\n",
        "print(f\"|----------------------------------|-----------|----------|\")\n",
        "\n",
        "for r in all_results:\n",
        "    prompt_display = r['prompt'][:32] + \"...\" if len(r['prompt']) > 32 else r['prompt']\n",
        "    print(f\"| {prompt_display:<32} | {r['stability']:.1%}     | {r['matching']}/{r['total']}      |\")\n",
        "\n",
        "# Calculate average stability\n",
        "avg_stability = sum(r['stability'] for r in all_results) / len(all_results)\n",
        "print(f\"\\nüìä Average Stability Score: {avg_stability:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Mini-Project: Robustness Audit\n",
        "\n",
        "### Task\n",
        "\n",
        "Create a comprehensive robustness audit of a model.\n",
        "\n",
        "### Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your custom model function\n",
        "def my_model(prompt: str) -> str:\n",
        "    \"\"\"Your model implementation here.\"\"\"\n",
        "    # Option 1: Use robust_model for testing\n",
        "    # Option 2: Connect to an API-based model\n",
        "    # Option 3: Load a local model\n",
        "    pass\n",
        "\n",
        "# Define test prompts for your use case\n",
        "# my_test_prompts = [\n",
        "#     \"Your prompt 1\",\n",
        "#     \"Your prompt 2\",\n",
        "#     ...\n",
        "# ]\n",
        "\n",
        "# Run robustness evaluation\n",
        "# for prompt in my_test_prompts:\n",
        "#     results = robustness_sweep(my_model, prompt, n=20, seed=42)\n",
        "#     print(f\"Prompt: {prompt}\")\n",
        "#     print(f\"Stability: {results['stability_score']:.2%}\")\n",
        "\n",
        "# Analyze results and create your audit report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§î Paul-Elder Critical Thinking Questions\n",
        "\n",
        "Reflect on these questions:\n",
        "\n",
        "### Question 1: EVIDENCE\n",
        "**A model achieves 90% stability. Is this sufficient for production use?**\n",
        "*Consider: Use case criticality, failure impact, user expectations, industry standards.*\n",
        "\n",
        "### Question 2: ASSUMPTIONS\n",
        "**What assumptions are we making about how users will input queries?**\n",
        "*Consider: Typo frequency, vocabulary diversity, grammar variations, language proficiency.*\n",
        "\n",
        "### Question 3: IMPLICATIONS\n",
        "**If we only test with synthetic perturbations, what might we miss?**\n",
        "*Consider: Real-world variations, domain-specific language, adversarial inputs, multi-lingual users.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitations of Robustness Testing\n",
        "\n",
        "### What These Tests DON'T Cover\n",
        "\n",
        "1. **Semantic Similarity Gap:** Current implementation uses word overlap, not embeddings\n",
        "2. **Limited Perturbation Types:** Only typos, synonyms, and reordering\n",
        "3. **Language-Specific:** Focused on English\n",
        "4. **Adversarial Attacks:** Not designed for security testing\n",
        "5. **Multi-Turn Conversations:** Only tests single prompts\n",
        "\n",
        "### Future Improvements (TODO)\n",
        "\n",
        "- Embedding-based semantic similarity\n",
        "- Character deletion and insertion\n",
        "- Phonetic spelling variations\n",
        "- Cross-lingual robustness\n",
        "- LLM-as-judge for similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 9, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand why robustness testing is critical for LLM deployment\n",
        "- [ ] I can use `perturb_prompt` to generate input variations\n",
        "- [ ] I can use `robustness_sweep` to measure model stability\n",
        "- [ ] I can interpret and analyze stability scores\n",
        "- [ ] I understand the limitations of current robustness testing\n",
        "- [ ] I know how to identify and analyze failure cases\n",
        "\n",
        "---\n",
        "\n",
        "**Week 8 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 9 ‚Äî Performance Benchmarking*"
      ]
    }
  ]
}
