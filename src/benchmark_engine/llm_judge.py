"""
LLM-as-Judge evaluation module for BenchRight.

This module provides a class for using a language model to evaluate
the quality of model-generated answers. The LLMJudge class is designed
to be reusable across different evaluation scenarios (Weeks 6, 11-16).

Example usage:
    >>> from openai import OpenAI
    >>> client = OpenAI()
    >>> judge = LLMJudge(client)
    >>> result = judge.score_answer(
    ...     question="What is the capital of France?",
    ...     answer="Paris is the capital of France.",
    ...     reference="Paris"
    ... )
    >>> print(result)
    {'score': 1.0, 'rationale': 'The answer correctly identifies Paris...'}
"""

import json
from typing import Any, Protocol, runtime_checkable


# System prompt for LLM-as-Judge evaluation
# Designed to be general and reusable across different evaluation tasks
JUDGE_SYSTEM_PROMPT = """You are an expert evaluator assessing the quality of AI-generated responses.

Your task is to evaluate an answer based on three criteria:
1. **Correctness**: Does the answer accurately address the question? Is the information factually correct when compared to the reference answer?
2. **Coherence**: Is the answer well-structured, logical, and easy to understand?
3. **Helpfulness**: Does the answer provide useful information that addresses the user's needs?

You will be given:
- A QUESTION that was asked
- An ANSWER that was generated by an AI model
- A REFERENCE answer that represents the expected/correct response

Evaluate the ANSWER and provide:
1. A score from 0.0 to 1.0, where:
   - 0.0 = Completely incorrect, incoherent, or unhelpful
   - 0.5 = Partially correct or somewhat helpful but with significant issues
   - 1.0 = Fully correct, coherent, and helpful

2. A brief rationale explaining your score (2-3 sentences maximum)

IMPORTANT:
- Be objective and consistent in your evaluations
- Consider partial credit for answers that are close but not exact
- Focus on the substance of the answer, not stylistic preferences
- If the answer conveys the same meaning as the reference but with different wording, it should still receive a high score

Respond ONLY with a valid JSON object in this exact format:
{"score": <float between 0.0 and 1.0>, "rationale": "<brief explanation>"}

Do not include any other text before or after the JSON object."""


@runtime_checkable
class LLMClient(Protocol):
    """Protocol defining the interface for an LLM client.

    This protocol allows LLMJudge to work with any client that implements
    a chat completions interface similar to OpenAI's API.

    The client must have a `chat.completions.create` method that accepts:
        - model: str - The model identifier
        - messages: list - A list of message dictionaries with 'role' and 'content'
        - temperature: float - Sampling temperature

    And returns an object with:
        - choices[0].message.content: str - The generated response text
    """

    @property
    def chat(self) -> Any:
        """Access the chat interface."""
        ...


class LLMJudge:
    """A judge that uses an LLM to evaluate the quality of model answers.

    This class implements the LLM-as-Judge pattern, where a language model
    is used to evaluate the quality of answers generated by another model.
    It is designed to be flexible and work with various LLM APIs.

    Attributes:
        client: An LLM client object (e.g., OpenAI client) that provides
               access to chat completion functionality.
        model: The model identifier to use for evaluation (default: "gpt-4o-mini").
        system_prompt: The system prompt used to instruct the judge.

    Example:
        >>> from openai import OpenAI
        >>> client = OpenAI()
        >>> judge = LLMJudge(client)
        >>> result = judge.score_answer(
        ...     question="What is 2+2?",
        ...     answer="The answer is 4.",
        ...     reference="4"
        ... )
        >>> print(f"Score: {result['score']}, Rationale: {result['rationale']}")
    """

    def __init__(
        self,
        client: Any,
        model: str = "gpt-4o-mini",
        system_prompt: str = JUDGE_SYSTEM_PROMPT,
    ) -> None:
        """Initialize the LLMJudge with an LLM client.

        Args:
            client: An LLM client object that provides chat completion
                   functionality. Must have a `chat.completions.create` method
                   compatible with the OpenAI API format.
            model: The model identifier to use for evaluation.
                  Defaults to "gpt-4o-mini" for cost-effectiveness.
            system_prompt: The system prompt to use for the judge.
                          Defaults to JUDGE_SYSTEM_PROMPT which evaluates
                          correctness, coherence, and helpfulness.
        """
        self.client = client
        self.model = model
        self.system_prompt = system_prompt

    def score_answer(
        self,
        question: str,
        answer: str,
        reference: str,
    ) -> dict:
        """Evaluate an answer using the LLM judge.

        This method sends the question, answer, and reference to the LLM
        for evaluation and returns a structured result with a score and
        rationale.

        Args:
            question: The original question or prompt that was asked.
            answer: The answer generated by the model being evaluated.
            reference: The expected/correct answer to compare against.

        Returns:
            A dictionary with keys:
                - "score": float between 0.0 and 1.0
                - "rationale": str explaining the score

            In case of parsing errors, returns:
                - "score": 0.0
                - "rationale": Error message explaining the failure

        Example:
            >>> result = judge.score_answer(
            ...     question="What is the capital of France?",
            ...     answer="Paris",
            ...     reference="Paris"
            ... )
            >>> print(result)
            {'score': 1.0, 'rationale': 'The answer correctly identifies...'}
        """
        # Construct the user message with the evaluation context
        user_message = f"""QUESTION: {question}

ANSWER: {answer}

REFERENCE: {reference}

Please evaluate the ANSWER and provide your assessment as a JSON object."""

        # Call the LLM for evaluation
        response = self.client.chat.completions.create(
            model=self.model,
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_message},
            ],
            temperature=0.0,  # Use deterministic output for consistency
        )

        # Extract the response content
        response_text = response.choices[0].message.content.strip()

        # Parse the JSON response
        return self._parse_response(response_text)

    def _parse_response(self, response_text: str) -> dict:
        """Parse the LLM response into a structured result.

        Args:
            response_text: The raw text response from the LLM.

        Returns:
            A dictionary with 'score' and 'rationale' keys.
        """
        try:
            # Try to parse the response as JSON
            result = json.loads(response_text)

            # Validate required keys
            if "score" not in result or "rationale" not in result:
                return {
                    "score": 0.0,
                    "rationale": f"Invalid response format: missing required keys. Response: {response_text}",
                }

            # Validate and clamp score to [0.0, 1.0]
            score = float(result["score"])
            score = max(0.0, min(1.0, score))

            return {
                "score": score,
                "rationale": str(result["rationale"]),
            }

        except json.JSONDecodeError as e:
            # Handle JSON parsing errors
            return {
                "score": 0.0,
                "rationale": f"Failed to parse LLM response as JSON: {e}. Response: {response_text}",
            }
        except (ValueError, TypeError) as e:
            # Handle type conversion errors
            return {
                "score": 0.0,
                "rationale": f"Invalid score value in response: {e}. Response: {response_text}",
            }


if __name__ == "__main__":
    # Demonstration with a mock client for testing
    print("=" * 60)
    print("BenchRight LLM-as-Judge Demo")
    print("=" * 60)

    # Create a simple mock client for demonstration
    class MockChatCompletions:
        """Mock chat completions for demonstration."""

        def create(self, model: str, messages: list, temperature: float = 0.0):
            """Return a mock response."""

            class MockChoice:
                class MockMessage:
                    content = '{"score": 0.9, "rationale": "The answer correctly identifies Paris as the capital of France, matching the reference answer. The response is clear and coherent."}'

                message = MockMessage()

            class MockResponse:
                choices = [MockChoice()]

            return MockResponse()

    class MockChat:
        completions = MockChatCompletions()

    class MockClient:
        chat = MockChat()

    # Create judge with mock client
    mock_client = MockClient()
    judge = LLMJudge(mock_client)

    # Test evaluation
    result = judge.score_answer(
        question="What is the capital of France?",
        answer="Paris is the capital of France.",
        reference="Paris",
    )

    print("\nüìù Test Evaluation:")
    print(f"   Question: What is the capital of France?")
    print(f"   Answer: Paris is the capital of France.")
    print(f"   Reference: Paris")
    print(f"\nüìä Result:")
    print(f"   Score: {result['score']}")
    print(f"   Rationale: {result['rationale']}")

    print("\n" + "=" * 60)
    print("Demo complete! Use with a real OpenAI client for actual evaluation.")
    print("=" * 60)
