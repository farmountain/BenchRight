{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 16 ‚Äî Creative & Marketing Content Evaluation\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand creative writing and marketing copy evaluation\n",
        "2. Define criteria: brand voice alignment, clarity, call-to-action strength\n",
        "3. Design and apply a simple rubric (1-5) for each dimension\n",
        "4. Use the LLM-as-Judge pattern to assign multi-dimensional scores\n",
        "5. Analyze patterns in high-scoring vs. low-scoring content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† What is Creative Content Evaluation?\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "Unlike factual QA, marketing content has no single \"correct\" answer:\n",
        "\n",
        "| Aspect | Factual QA | Creative Content |\n",
        "|--------|------------|------------------|\n",
        "| Correct answer | Yes, one | No, many possible |\n",
        "| Evaluation | Objective | Subjective |\n",
        "| Metrics | Accuracy | Voice, clarity, persuasion |\n",
        "| Success | Match reference | Achieve desired effect |\n",
        "\n",
        "### Our Approach: Multi-Dimensional Scoring\n",
        "\n",
        "We evaluate creative content on three key dimensions:\n",
        "1. **Brand Voice Alignment** - Does it sound like our brand?\n",
        "2. **Clarity** - Is it easy to understand?\n",
        "3. **Call-to-Action Strength** - Does it motivate action?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import numpy as np\n",
        "import sys\n",
        "import json\n",
        "from typing import Dict, List, Any, Optional, Callable\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# For data display\n",
        "try:\n",
        "    from IPython.display import display, HTML\n",
        "except ImportError:\n",
        "    display = print\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")\n",
        "print(f\"   NumPy version: {np.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 2: Define the Evaluation Rubric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the creative content evaluation rubric\n",
        "RUBRIC = {\n",
        "    \"brand_voice\": {\n",
        "        \"name\": \"Brand Voice Alignment\",\n",
        "        \"description\": \"Does the content match the specified brand voice?\",\n",
        "        \"scores\": {\n",
        "            5: \"Perfectly on-brand; indistinguishable from best brand examples\",\n",
        "            4: \"Mostly on-brand; minor adjustments needed\",\n",
        "            3: \"Neutral; not distinctly on or off-brand\",\n",
        "            2: \"Mostly off-brand; needs substantial revision\",\n",
        "            1: \"Completely off-brand; unusable\",\n",
        "        },\n",
        "    },\n",
        "    \"clarity\": {\n",
        "        \"name\": \"Clarity\",\n",
        "        \"description\": \"How easy is the content to understand?\",\n",
        "        \"scores\": {\n",
        "            5: \"Crystal clear; immediately understandable\",\n",
        "            4: \"Mostly clear; minor ambiguities\",\n",
        "            3: \"Understandable but with some unclear sections\",\n",
        "            2: \"Difficult to understand\",\n",
        "            1: \"Incomprehensible\",\n",
        "        },\n",
        "    },\n",
        "    \"cta_strength\": {\n",
        "        \"name\": \"Call-to-Action Strength\",\n",
        "        \"description\": \"How effectively does it motivate action?\",\n",
        "        \"scores\": {\n",
        "            5: \"Exceptional; compelling and impossible to ignore\",\n",
        "            4: \"Strong; clear and motivating\",\n",
        "            3: \"Adequate; present but not compelling\",\n",
        "            2: \"Weak; vague or easily ignored\",\n",
        "            1: \"No effective CTA\",\n",
        "        },\n",
        "    },\n",
        "}\n",
        "\n",
        "# Display the rubric\n",
        "print(\"üìã Creative Content Evaluation Rubric\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for dim_key, dim in RUBRIC.items():\n",
        "    print(f\"\\n{dim['name'].upper()}\")\n",
        "    print(f\"   {dim['description']}\")\n",
        "    print()\n",
        "    for score, description in sorted(dim['scores'].items(), reverse=True):\n",
        "        print(f\"   {score}: {description}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üè¢ Step 3: Define Brand Guidelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define comprehensive brand guidelines\n",
        "BRAND_GUIDELINES = \"\"\"\n",
        "Brand Name: TechFlow\n",
        "Industry: SaaS / Productivity Software\n",
        "Target Audience: Busy professionals and knowledge workers\n",
        "\n",
        "Voice Characteristics:\n",
        "- Tone: Professional yet approachable, confident but not arrogant\n",
        "- Style: Helpful, empowering, forward-thinking\n",
        "- Personality: Like a smart colleague who makes complex things simple\n",
        "\n",
        "Core Values:\n",
        "- Efficiency: We value people's time\n",
        "- Innovation: We embrace new solutions\n",
        "- Simplicity: We make things easy\n",
        "\n",
        "Vocabulary Guidelines:\n",
        "- USE: action verbs, specific benefits, relatable scenarios\n",
        "- AVOID: jargon, buzzwords, overly technical language\n",
        "\n",
        "Writing Style:\n",
        "- Short sentences, active voice\n",
        "- Benefit-focused, not feature-focused\n",
        "- Conversational but professional\n",
        "\n",
        "CTA Style:\n",
        "- Direct and action-oriented\n",
        "- Highlight value, not just the action\n",
        "- Create gentle urgency when appropriate\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚úÖ Brand guidelines defined!\")\n",
        "print()\n",
        "print(\"Brand: TechFlow\")\n",
        "print(\"Industry: SaaS / Productivity Software\")\n",
        "print(\"Target: Busy professionals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 4: Create Test Content Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample marketing content for evaluation\n",
        "CONTENT_SAMPLES = [\n",
        "    {\n",
        "        \"id\": \"content_001\",\n",
        "        \"type\": \"Product Description\",\n",
        "        \"product\": \"Smart Inbox Feature\",\n",
        "        \"content\": \"\"\"\n",
        "Stop drowning in emails. TechFlow's smart inbox prioritizes what matters, \n",
        "so you can focus on work that moves the needle. Join 50,000+ professionals \n",
        "who've reclaimed 2 hours every day. Start your free trial now.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"high\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"content_002\",\n",
        "        \"type\": \"Product Description\",\n",
        "        \"product\": \"Task Management\",\n",
        "        \"content\": \"\"\"\n",
        "Our cutting-edge, enterprise-grade solution leverages advanced AI/ML \n",
        "capabilities to optimize your task management paradigm through \n",
        "synergistic workflow automation. Request a demo to learn more.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"poor_voice\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"content_003\",\n",
        "        \"type\": \"Product Description\",\n",
        "        \"product\": \"Calendar Sync\",\n",
        "        \"content\": \"\"\"\n",
        "TechFlow helps you manage your calendar better. It's a good tool for \n",
        "staying organized. Many people like it. You might want to \n",
        "consider trying it someday.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"weak_cta\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"content_004\",\n",
        "        \"type\": \"Email Subject Line\",\n",
        "        \"product\": \"Weekly Digest\",\n",
        "        \"content\": \"Your productivity just leveled up‚Äîsee what's new in TechFlow\",\n",
        "        \"expected_quality\": \"high\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"content_005\",\n",
        "        \"type\": \"Landing Page Hero\",\n",
        "        \"product\": \"TechFlow Platform\",\n",
        "        \"content\": \"\"\"\n",
        "Work smarter, not harder.\n",
        "\n",
        "TechFlow brings your email, calendar, and tasks into one place. \n",
        "Less switching, more doing. See the difference in your first week.\n",
        "\n",
        "Try free for 14 days ‚Üí\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"high\",\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"content_006\",\n",
        "        \"type\": \"Product Description\",\n",
        "        \"product\": \"Analytics Dashboard\",\n",
        "        \"content\": \"\"\"\n",
        "BUY NOW!!! BEST PRODUCTIVITY APP EVER!!! \n",
        "Amazing analytics! Super powerful! Everyone needs this!\n",
        "LIMITED TIME OFFER - 90% OFF TODAY ONLY!!!\n",
        "CLICK HERE IMMEDIATELY!!! DON'T MISS OUT!!!\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"poor_all\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìù Created {len(CONTENT_SAMPLES)} content samples for evaluation\")\n",
        "print()\n",
        "for sample in CONTENT_SAMPLES:\n",
        "    print(f\"   ‚Ä¢ {sample['id']}: {sample['product']} ({sample['type']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§ñ Step 5: Define the LLM-as-Judge System Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# System prompt for creative content evaluation\n",
        "CREATIVE_JUDGE_SYSTEM_PROMPT = \"\"\"You are an expert marketing content evaluator. Your task is to evaluate AI-generated creative and marketing copy across three dimensions.\n",
        "\n",
        "## Evaluation Dimensions\n",
        "\n",
        "### 1. Brand Voice Alignment (1-5)\n",
        "Does the content match the specified brand voice?\n",
        "- 5: Perfectly on-brand, indistinguishable from best brand examples\n",
        "- 4: Mostly on-brand with minor adjustments needed\n",
        "- 3: Neutral, not distinctly on or off-brand\n",
        "- 2: Mostly off-brand, needs substantial revision\n",
        "- 1: Completely off-brand, unusable\n",
        "\n",
        "### 2. Clarity (1-5)\n",
        "How easy is the content to understand?\n",
        "- 5: Crystal clear, immediately understandable\n",
        "- 4: Mostly clear with minor ambiguities\n",
        "- 3: Understandable but with some unclear sections\n",
        "- 2: Difficult to understand\n",
        "- 1: Incomprehensible\n",
        "\n",
        "### 3. Call-to-Action Strength (1-5)\n",
        "How effectively does it motivate action?\n",
        "- 5: Exceptional, compelling and impossible to ignore\n",
        "- 4: Strong, clear and motivating\n",
        "- 3: Adequate, present but not compelling\n",
        "- 2: Weak, vague or easily ignored\n",
        "- 1: No effective CTA\n",
        "\n",
        "## Instructions\n",
        "\n",
        "1. Read the brand guidelines carefully\n",
        "2. Evaluate the content against each dimension\n",
        "3. Provide specific examples from the content to justify each score\n",
        "4. Be objective and consistent\n",
        "\n",
        "Respond ONLY with a valid JSON object in this exact format:\n",
        "{\n",
        "    \"brand_voice_score\": <int 1-5>,\n",
        "    \"brand_voice_rationale\": \"<specific justification>\",\n",
        "    \"clarity_score\": <int 1-5>,\n",
        "    \"clarity_rationale\": \"<specific justification>\",\n",
        "    \"cta_score\": <int 1-5>,\n",
        "    \"cta_rationale\": \"<specific justification>\",\n",
        "    \"overall_score\": <float, average of three scores>,\n",
        "    \"summary\": \"<2-3 sentence overall assessment>\"\n",
        "}\n",
        "\n",
        "Do not include any other text before or after the JSON object.\"\"\"\n",
        "\n",
        "print(\"‚úÖ LLM-as-Judge system prompt defined!\")\n",
        "print(f\"   Prompt length: {len(CREATIVE_JUDGE_SYSTEM_PROMPT)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 6: Implement the CreativeContentJudge Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CreativeContentJudge:\n",
        "    \"\"\"\n",
        "    A judge that evaluates creative and marketing content using an LLM.\n",
        "    \n",
        "    This class implements the LLM-as-Judge pattern for multi-dimensional\n",
        "    evaluation of marketing copy, assessing brand voice, clarity, and\n",
        "    call-to-action effectiveness.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        client: Any,\n",
        "        model: str = \"gpt-4o-mini\",\n",
        "        system_prompt: str = CREATIVE_JUDGE_SYSTEM_PROMPT,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the CreativeContentJudge.\n",
        "        \n",
        "        Args:\n",
        "            client: An LLM client with chat.completions.create method\n",
        "            model: Model identifier for evaluation\n",
        "            system_prompt: System prompt for the judge\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.system_prompt = system_prompt\n",
        "    \n",
        "    def evaluate(\n",
        "        self,\n",
        "        content: str,\n",
        "        brand_guidelines: str,\n",
        "        target_audience: str = \"\",\n",
        "        desired_action: str = \"\",\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate marketing content across all dimensions.\n",
        "        \n",
        "        Args:\n",
        "            content: The marketing copy to evaluate\n",
        "            brand_guidelines: Description of brand voice and style\n",
        "            target_audience: Who the content is for\n",
        "            desired_action: What action should readers take\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with scores and rationales for each dimension\n",
        "        \"\"\"\n",
        "        # Construct the evaluation prompt\n",
        "        user_message = f\"\"\"## Brand Guidelines\n",
        "{brand_guidelines}\n",
        "\n",
        "## Target Audience\n",
        "{target_audience if target_audience else \"General audience\"}\n",
        "\n",
        "## Desired Action\n",
        "{desired_action if desired_action else \"Not specified\"}\n",
        "\n",
        "## Content to Evaluate\n",
        "{content}\n",
        "\n",
        "Please evaluate this content according to the rubric.\"\"\"\n",
        "\n",
        "        # Call the LLM for evaluation\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_message},\n",
        "            ],\n",
        "            temperature=0.0,\n",
        "        )\n",
        "        \n",
        "        # Parse the response\n",
        "        response_text = response.choices[0].message.content.strip()\n",
        "        return self._parse_response(response_text)\n",
        "    \n",
        "    def _parse_response(self, response_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Parse the LLM response into a structured result.\"\"\"\n",
        "        try:\n",
        "            result = json.loads(response_text)\n",
        "            \n",
        "            # Validate and clamp scores\n",
        "            for key in [\"brand_voice_score\", \"clarity_score\", \"cta_score\"]:\n",
        "                if key in result:\n",
        "                    result[key] = max(1, min(5, int(result[key])))\n",
        "            \n",
        "            # Recalculate overall score\n",
        "            scores = [\n",
        "                result.get(\"brand_voice_score\", 3),\n",
        "                result.get(\"clarity_score\", 3),\n",
        "                result.get(\"cta_score\", 3),\n",
        "            ]\n",
        "            result[\"overall_score\"] = sum(scores) / len(scores)\n",
        "            \n",
        "            return result\n",
        "            \n",
        "        except json.JSONDecodeError as e:\n",
        "            return {\n",
        "                \"brand_voice_score\": 0,\n",
        "                \"brand_voice_rationale\": f\"Parse error: {e}\",\n",
        "                \"clarity_score\": 0,\n",
        "                \"clarity_rationale\": f\"Parse error: {e}\",\n",
        "                \"cta_score\": 0,\n",
        "                \"cta_rationale\": f\"Parse error: {e}\",\n",
        "                \"overall_score\": 0.0,\n",
        "                \"summary\": f\"Failed to parse response: {response_text[:100]}\",\n",
        "            }\n",
        "    \n",
        "    def evaluate_batch(\n",
        "        self,\n",
        "        contents: List[str],\n",
        "        brand_guidelines: str,\n",
        "        target_audience: str = \"\",\n",
        "        desired_action: str = \"\",\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Evaluate multiple pieces of content.\n",
        "        \n",
        "        Args:\n",
        "            contents: List of marketing copy to evaluate\n",
        "            brand_guidelines: Description of brand voice and style\n",
        "            target_audience: Who the content is for\n",
        "            desired_action: What action should readers take\n",
        "            \n",
        "        Returns:\n",
        "            List of evaluation results\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        for content in contents:\n",
        "            result = self.evaluate(\n",
        "                content=content,\n",
        "                brand_guidelines=brand_guidelines,\n",
        "                target_audience=target_audience,\n",
        "                desired_action=desired_action,\n",
        "            )\n",
        "            results.append(result)\n",
        "        return results\n",
        "    \n",
        "    def compute_aggregate_metrics(\n",
        "        self,\n",
        "        results: List[Dict[str, Any]],\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute aggregate metrics across multiple evaluations.\n",
        "        \n",
        "        Args:\n",
        "            results: List of evaluation results\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with average scores and distributions\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return {}\n",
        "        \n",
        "        brand_scores = [r.get(\"brand_voice_score\", 0) for r in results if r.get(\"brand_voice_score\", 0) > 0]\n",
        "        clarity_scores = [r.get(\"clarity_score\", 0) for r in results if r.get(\"clarity_score\", 0) > 0]\n",
        "        cta_scores = [r.get(\"cta_score\", 0) for r in results if r.get(\"cta_score\", 0) > 0]\n",
        "        overall_scores = [r.get(\"overall_score\", 0) for r in results if r.get(\"overall_score\", 0) > 0]\n",
        "        \n",
        "        return {\n",
        "            \"avg_brand_voice\": np.mean(brand_scores) if brand_scores else 0.0,\n",
        "            \"avg_clarity\": np.mean(clarity_scores) if clarity_scores else 0.0,\n",
        "            \"avg_cta_strength\": np.mean(cta_scores) if cta_scores else 0.0,\n",
        "            \"avg_overall\": np.mean(overall_scores) if overall_scores else 0.0,\n",
        "            \"total_evaluated\": len(results),\n",
        "            \"valid_evaluations\": len([r for r in results if r.get(\"overall_score\", 0) > 0]),\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"‚úÖ CreativeContentJudge class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 7: Create a Mock LLM Client for Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockCreativeJudgeClient:\n",
        "    \"\"\"\n",
        "    Mock LLM client for demonstration purposes.\n",
        "    \n",
        "    Simulates LLM responses based on content analysis heuristics.\n",
        "    In production, use an actual LLM (GPT-4, Claude, etc.)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.chat = self._MockChat()\n",
        "    \n",
        "    class _MockChat:\n",
        "        def __init__(self):\n",
        "            self.completions = self._MockCompletions()\n",
        "        \n",
        "        class _MockCompletions:\n",
        "            def create(self, model: str, messages: list, temperature: float = 0.0):\n",
        "                # Extract content from the user message\n",
        "                user_msg = next((m[\"content\"] for m in messages if m[\"role\"] == \"user\"), \"\")\n",
        "                \n",
        "                # Simple heuristics for scoring\n",
        "                content_lower = user_msg.lower()\n",
        "                \n",
        "                # Brand voice scoring\n",
        "                if any(word in content_lower for word in [\"synergistic\", \"paradigm\", \"leverage\", \"enterprise-grade\"]):\n",
        "                    brand_voice = 2\n",
        "                    brand_rationale = \"Uses jargon and buzzwords that don't match the approachable brand voice.\"\n",
        "                elif any(word in content_lower for word in [\"buy now!!!\", \"limited time\", \"don't miss out\"]):\n",
        "                    brand_voice = 1\n",
        "                    brand_rationale = \"Aggressive promotional tone contradicts the professional, approachable brand voice.\"\n",
        "                elif any(word in content_lower for word in [\"might want to\", \"consider\", \"someday\"]):\n",
        "                    brand_voice = 3\n",
        "                    brand_rationale = \"Neutral tone, lacks the confident and empowering voice expected.\"\n",
        "                else:\n",
        "                    brand_voice = 4\n",
        "                    brand_rationale = \"Good match with professional yet approachable tone. Uses benefit-focused language.\"\n",
        "                \n",
        "                # Clarity scoring\n",
        "                if \"paradigm\" in content_lower or \"synergistic\" in content_lower:\n",
        "                    clarity = 2\n",
        "                    clarity_rationale = \"Technical jargon and complex sentence structures make it difficult to understand.\"\n",
        "                elif \"!!!\" in user_msg:\n",
        "                    clarity = 3\n",
        "                    clarity_rationale = \"Excessive punctuation creates noise, but message is somewhat understandable.\"\n",
        "                else:\n",
        "                    clarity = 5\n",
        "                    clarity_rationale = \"Clear, concise language with logical flow. Easy to understand immediately.\"\n",
        "                \n",
        "                # CTA scoring\n",
        "                if \"start your free trial\" in content_lower or \"try free\" in content_lower:\n",
        "                    cta = 5\n",
        "                    cta_rationale = \"Strong, specific CTA with clear value proposition and low barrier to action.\"\n",
        "                elif \"request a demo\" in content_lower:\n",
        "                    cta = 4\n",
        "                    cta_rationale = \"Clear call-to-action, though could emphasize value more.\"\n",
        "                elif \"might want to\" in content_lower or \"consider\" in content_lower:\n",
        "                    cta = 2\n",
        "                    cta_rationale = \"Weak, vague language that fails to motivate action. No urgency.\"\n",
        "                elif \"click here immediately\" in content_lower:\n",
        "                    cta = 2\n",
        "                    cta_rationale = \"Aggressive CTA that may turn off the target professional audience.\"\n",
        "                else:\n",
        "                    cta = 4\n",
        "                    cta_rationale = \"Solid CTA that motivates action appropriately for the brand.\"\n",
        "                \n",
        "                overall = (brand_voice + clarity + cta) / 3\n",
        "                \n",
        "                # Generate summary\n",
        "                if overall >= 4:\n",
        "                    summary = \"Strong content that aligns well with brand guidelines. Ready for publication with minor adjustments.\"\n",
        "                elif overall >= 3:\n",
        "                    summary = \"Adequate content with room for improvement. Needs revision to better match brand voice or strengthen CTA.\"\n",
        "                else:\n",
        "                    summary = \"Content requires significant revision. Major issues with brand alignment, clarity, or call-to-action.\"\n",
        "                \n",
        "                response_json = json.dumps({\n",
        "                    \"brand_voice_score\": brand_voice,\n",
        "                    \"brand_voice_rationale\": brand_rationale,\n",
        "                    \"clarity_score\": clarity,\n",
        "                    \"clarity_rationale\": clarity_rationale,\n",
        "                    \"cta_score\": cta,\n",
        "                    \"cta_rationale\": cta_rationale,\n",
        "                    \"overall_score\": overall,\n",
        "                    \"summary\": summary,\n",
        "                })\n",
        "                \n",
        "                class MockChoice:\n",
        "                    class MockMessage:\n",
        "                        def __init__(self, content):\n",
        "                            self.content = content\n",
        "                    \n",
        "                    def __init__(self, content):\n",
        "                        self.message = self.MockMessage(content)\n",
        "                \n",
        "                class MockResponse:\n",
        "                    def __init__(self, content):\n",
        "                        self.choices = [MockChoice(content)]\n",
        "                \n",
        "                return MockResponse(response_json)\n",
        "\n",
        "\n",
        "# Create mock client\n",
        "mock_client = MockCreativeJudgeClient()\n",
        "\n",
        "print(\"‚úÖ Mock LLM client created!\")\n",
        "print(\"   Note: Using mock for demonstration.\")\n",
        "print(\"   In production, use GPT-4, Claude, or similar.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèÉ Step 8: Run Creative Content Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluator\n",
        "judge = CreativeContentJudge(mock_client)\n",
        "\n",
        "# Run evaluation on all samples\n",
        "print(\"üîÑ Running Creative Content Evaluation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "all_results = []\n",
        "for sample in CONTENT_SAMPLES:\n",
        "    result = judge.evaluate(\n",
        "        content=sample[\"content\"],\n",
        "        brand_guidelines=BRAND_GUIDELINES,\n",
        "        target_audience=\"Busy professionals and knowledge workers\",\n",
        "        desired_action=\"Sign up for free trial or explore features\",\n",
        "    )\n",
        "    \n",
        "    # Add metadata\n",
        "    result[\"id\"] = sample[\"id\"]\n",
        "    result[\"product\"] = sample[\"product\"]\n",
        "    result[\"type\"] = sample[\"type\"]\n",
        "    result[\"expected_quality\"] = sample[\"expected_quality\"]\n",
        "    all_results.append(result)\n",
        "    \n",
        "    # Display results\n",
        "    overall_status = \"‚úÖ\" if result[\"overall_score\"] >= 4 else \"‚ö†Ô∏è\" if result[\"overall_score\"] >= 3 else \"‚ùå\"\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"{sample['product']} ({sample['id']})\")\n",
        "    print(f\"Type: {sample['type']} | Expected: {sample['expected_quality']}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"\\nContent: {sample['content'][:80]}...\")\n",
        "    print(f\"\\nScores:\")\n",
        "    print(f\"   Brand Voice:  {result['brand_voice_score']}/5\")\n",
        "    print(f\"   Clarity:      {result['clarity_score']}/5\")\n",
        "    print(f\"   CTA Strength: {result['cta_score']}/5\")\n",
        "    print(f\"   Overall:      {result['overall_score']:.1f}/5 {overall_status}\")\n",
        "    print(f\"\\nSummary: {result['summary']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 9: Compute and Display Aggregate Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute aggregate metrics\n",
        "metrics = judge.compute_aggregate_metrics(all_results)\n",
        "\n",
        "print(\"üìä Aggregate Creative Content Evaluation Metrics\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\")\n",
        "print(f\"Total Evaluated: {metrics['total_evaluated']}\")\n",
        "print(f\"Valid Evaluations: {metrics['valid_evaluations']}\")\n",
        "print(f\"\")\n",
        "print(f\"Average Scores:\")\n",
        "print(f\"   Brand Voice:  {metrics['avg_brand_voice']:.1f}/5\")\n",
        "print(f\"   Clarity:      {metrics['avg_clarity']:.1f}/5\")\n",
        "print(f\"   CTA Strength: {metrics['avg_cta_strength']:.1f}/5\")\n",
        "print(f\"   Overall:      {metrics['avg_overall']:.1f}/5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 10: Generate Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Evaluation Summary Table\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'#':<3} {'Product':<25} {'Type':<20} {'Voice':<6} {'Clarity':<8} {'CTA':<5} {'Overall':<8}\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "for i, r in enumerate(all_results, 1):\n",
        "    status = \"‚úÖ\" if r[\"overall_score\"] >= 4 else \"‚ö†Ô∏è\" if r[\"overall_score\"] >= 3 else \"‚ùå\"\n",
        "    product_short = r[\"product\"][:23] + \"..\" if len(r[\"product\"]) > 25 else r[\"product\"]\n",
        "    type_short = r[\"type\"][:18] + \"..\" if len(r[\"type\"]) > 20 else r[\"type\"]\n",
        "    \n",
        "    print(f\"{i:<3} {product_short:<25} {type_short:<20} {r['brand_voice_score']}/5   {r['clarity_score']}/5     {r['cta_score']}/5  {r['overall_score']:.1f}/5 {status}\")\n",
        "\n",
        "print(\"-\" * 100)\n",
        "print(f\"\")\n",
        "print(f\"Summary: {metrics['avg_overall']:.1f}/5 average overall score\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 11: Analyze High vs. Low Scoring Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sort results by overall score\n",
        "sorted_results = sorted(all_results, key=lambda x: x[\"overall_score\"], reverse=True)\n",
        "\n",
        "print(\"üîç Content Quality Analysis\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Identify best and worst\n",
        "print(\"\\nüìà HIGHEST SCORING CONTENT\")\n",
        "print(\"-\" * 50)\n",
        "best = sorted_results[0]\n",
        "print(f\"Product: {best['product']}\")\n",
        "print(f\"Overall Score: {best['overall_score']:.1f}/5\")\n",
        "print(f\"\")\n",
        "print(f\"Brand Voice ({best['brand_voice_score']}/5): {best['brand_voice_rationale']}\")\n",
        "print(f\"Clarity ({best['clarity_score']}/5): {best['clarity_rationale']}\")\n",
        "print(f\"CTA ({best['cta_score']}/5): {best['cta_rationale']}\")\n",
        "\n",
        "print(\"\\nüìâ LOWEST SCORING CONTENT\")\n",
        "print(\"-\" * 50)\n",
        "worst = sorted_results[-1]\n",
        "print(f\"Product: {worst['product']}\")\n",
        "print(f\"Overall Score: {worst['overall_score']:.1f}/5\")\n",
        "print(f\"\")\n",
        "print(f\"Brand Voice ({worst['brand_voice_score']}/5): {worst['brand_voice_rationale']}\")\n",
        "print(f\"Clarity ({worst['clarity_score']}/5): {worst['clarity_rationale']}\")\n",
        "print(f\"CTA ({worst['cta_score']}/5): {worst['cta_rationale']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 12: Score Distribution by Dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Score Distribution by Dimension\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Analyze each dimension\n",
        "dimensions = [\n",
        "    (\"Brand Voice\", \"brand_voice_score\"),\n",
        "    (\"Clarity\", \"clarity_score\"),\n",
        "    (\"CTA Strength\", \"cta_score\"),\n",
        "]\n",
        "\n",
        "for dim_name, dim_key in dimensions:\n",
        "    scores = [r[dim_key] for r in all_results if r[dim_key] > 0]\n",
        "    \n",
        "    print(f\"\\n{dim_name.upper()}\")\n",
        "    print(f\"   Average: {np.mean(scores):.1f}/5\")\n",
        "    print(f\"   Std Dev: {np.std(scores):.2f}\")\n",
        "    print(f\"   Min: {min(scores)}/5\")\n",
        "    print(f\"   Max: {max(scores)}/5\")\n",
        "    \n",
        "    # Distribution\n",
        "    print(f\"   Distribution:\")\n",
        "    for score in range(5, 0, -1):\n",
        "        count = sum(1 for s in scores if s == score)\n",
        "        bar = \"‚ñà\" * count\n",
        "        print(f\"      {score}/5: {bar} ({count})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ Step 13: Identify Improvement Areas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üéØ Improvement Areas Analysis\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Find content needing improvement for each dimension\n",
        "print(\"\\nüî¥ Content Needing Brand Voice Improvement (Score < 4):\")\n",
        "brand_issues = [r for r in all_results if r[\"brand_voice_score\"] < 4]\n",
        "if brand_issues:\n",
        "    for r in brand_issues:\n",
        "        print(f\"   ‚Ä¢ {r['product']}: {r['brand_voice_score']}/5 - {r['brand_voice_rationale'][:60]}...\")\n",
        "else:\n",
        "    print(\"   ‚úÖ All content meets brand voice standards\")\n",
        "\n",
        "print(\"\\nüî¥ Content Needing Clarity Improvement (Score < 4):\")\n",
        "clarity_issues = [r for r in all_results if r[\"clarity_score\"] < 4]\n",
        "if clarity_issues:\n",
        "    for r in clarity_issues:\n",
        "        print(f\"   ‚Ä¢ {r['product']}: {r['clarity_score']}/5 - {r['clarity_rationale'][:60]}...\")\n",
        "else:\n",
        "    print(\"   ‚úÖ All content meets clarity standards\")\n",
        "\n",
        "print(\"\\nüî¥ Content Needing CTA Improvement (Score < 4):\")\n",
        "cta_issues = [r for r in all_results if r[\"cta_score\"] < 4]\n",
        "if cta_issues:\n",
        "    for r in cta_issues:\n",
        "        print(f\"   ‚Ä¢ {r['product']}: {r['cta_score']}/5 - {r['cta_rationale'][:60]}...\")\n",
        "else:\n",
        "    print(\"   ‚úÖ All content meets CTA standards\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 14: Test Custom Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with custom content\n",
        "print(\"üß™ Test Custom Content\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "custom_content = \"\"\"\n",
        "Finally, a calendar that works the way you think.\n",
        "\n",
        "TechFlow Calendar learns your preferences, suggests optimal meeting times, \n",
        "and protects your focus time automatically. Because your best work happens \n",
        "when you're not fighting your schedule.\n",
        "\n",
        "See it in action ‚Äî book a 10-minute demo.\n",
        "\"\"\".strip()\n",
        "\n",
        "print(f\"\\nCustom Content:\")\n",
        "print(f\"-\" * 50)\n",
        "print(custom_content)\n",
        "print(f\"-\" * 50)\n",
        "\n",
        "# Evaluate\n",
        "result = judge.evaluate(\n",
        "    content=custom_content,\n",
        "    brand_guidelines=BRAND_GUIDELINES,\n",
        "    target_audience=\"Busy professionals\",\n",
        "    desired_action=\"Book a demo\",\n",
        ")\n",
        "\n",
        "print(f\"\\nEvaluation Results:\")\n",
        "print(f\"   Brand Voice:  {result['brand_voice_score']}/5 - {result['brand_voice_rationale']}\")\n",
        "print(f\"   Clarity:      {result['clarity_score']}/5 - {result['clarity_rationale']}\")\n",
        "print(f\"   CTA Strength: {result['cta_score']}/5 - {result['cta_rationale']}\")\n",
        "print(f\"   Overall:      {result['overall_score']:.1f}/5\")\n",
        "print(f\"\\nSummary: {result['summary']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîÑ Step 15: Compare Content Variants (A/B Testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compare_variants(\n",
        "    judge: CreativeContentJudge,\n",
        "    variant_a: str,\n",
        "    variant_b: str,\n",
        "    brand_guidelines: str,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Compare two content variants for A/B testing.\n",
        "    \"\"\"\n",
        "    result_a = judge.evaluate(variant_a, brand_guidelines)\n",
        "    result_b = judge.evaluate(variant_b, brand_guidelines)\n",
        "    \n",
        "    # Determine winner for each dimension\n",
        "    comparisons = {}\n",
        "    for dim, key in [(\"Brand Voice\", \"brand_voice_score\"), (\"Clarity\", \"clarity_score\"), (\"CTA\", \"cta_score\")]:\n",
        "        a_score = result_a[key]\n",
        "        b_score = result_b[key]\n",
        "        winner = \"A\" if a_score > b_score else \"B\" if b_score > a_score else \"Tie\"\n",
        "        comparisons[dim] = {\"a\": a_score, \"b\": b_score, \"winner\": winner}\n",
        "    \n",
        "    # Overall winner\n",
        "    a_wins = sum(1 for c in comparisons.values() if c[\"winner\"] == \"A\")\n",
        "    b_wins = sum(1 for c in comparisons.values() if c[\"winner\"] == \"B\")\n",
        "    overall_winner = \"A\" if a_wins > b_wins else \"B\" if b_wins > a_wins else \"Tie\"\n",
        "    \n",
        "    return {\n",
        "        \"result_a\": result_a,\n",
        "        \"result_b\": result_b,\n",
        "        \"comparisons\": comparisons,\n",
        "        \"overall_winner\": overall_winner,\n",
        "    }\n",
        "\n",
        "\n",
        "# Test variants\n",
        "variant_a = \"Start your free trial today and see why 50,000+ professionals trust TechFlow.\"\n",
        "variant_b = \"Join TechFlow now. It might help you be more productive someday.\"\n",
        "\n",
        "print(\"üîÑ A/B Test Comparison\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nVariant A: {variant_a}\")\n",
        "print(f\"Variant B: {variant_b}\")\n",
        "\n",
        "comparison = compare_variants(judge, variant_a, variant_b, BRAND_GUIDELINES)\n",
        "\n",
        "print(f\"\\nResults:\")\n",
        "print(f\"-\" * 50)\n",
        "for dim, data in comparison[\"comparisons\"].items():\n",
        "    print(f\"   {dim}: A={data['a']}/5, B={data['b']}/5 ‚Üí Winner: {data['winner']}\")\n",
        "\n",
        "print(f\"\\nüèÜ Overall Winner: Variant {comparison['overall_winner']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö Summary\n",
        "\n",
        "In this notebook, you learned how to:\n",
        "\n",
        "1. **Define evaluation criteria** for creative and marketing content\n",
        "2. **Design a multi-dimensional rubric** (1-5 scale for each dimension)\n",
        "3. **Implement the LLM-as-Judge pattern** for creative content\n",
        "4. **Analyze content quality** across brand voice, clarity, and CTA\n",
        "5. **Compare content variants** for A/B testing\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. Creative content requires multi-dimensional evaluation\n",
        "2. Brand voice alignment is critical for consistent messaging\n",
        "3. Clarity and CTA strength directly impact conversion\n",
        "4. LLM judges can scale evaluation but should be calibrated with human review\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Calibrate the judge** with human-rated examples\n",
        "2. **Add engagement prediction** based on content characteristics\n",
        "3. **Integrate with A/B testing** to validate scores against real performance\n",
        "4. **Build a content scoring dashboard** for marketing teams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úî Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 17-18, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand what creative and marketing content evaluation involves\n",
        "- [ ] I can define evaluation criteria: brand voice, clarity, CTA strength\n",
        "- [ ] I can design and apply a 1-5 rubric for each dimension\n",
        "- [ ] I understand how to implement the LLM-as-Judge pattern for multi-dimensional scoring\n",
        "- [ ] I can interpret evaluation results and identify patterns\n",
        "- [ ] I understand the limitations of automated creative evaluation\n",
        "- [ ] I can provide actionable recommendations based on evaluation results\n",
        "\n",
        "---\n",
        "\n",
        "**Week 16 Complete!**\n",
        "\n",
        "*Next: Week 17-18 ‚Äî Full System Architecture & Capstone*"
      ]
    }
  ]
}
