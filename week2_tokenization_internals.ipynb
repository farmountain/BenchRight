{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 2 ‚Äî Tokenization & ONNX Runtime Internals\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand what tokens are and how tokenization works\n",
        "2. Analyze how token count affects inference latency\n",
        "3. Enable ONNX Runtime profiling to inspect operator-level timings\n",
        "4. Recognize how tokenization choices can mislead evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† What is Tokenization? (Feynman Explanation)\n",
        "\n",
        "Imagine you're teaching a robot to read. The robot doesn't understand words‚Äîit only understands numbers. So before the robot can read a sentence, we need to break it into smaller pieces and assign each piece a number.\n",
        "\n",
        "**Tokenization** is this process:\n",
        "1. Take a sentence: `\"Hello world\"`\n",
        "2. Break it into pieces: `[\"Hello\", \" world\"]`\n",
        "3. Convert pieces to numbers: `[15496, 995]`\n",
        "\n",
        "**Key insight:** The number of tokens determines how much work the model does. More tokens = more computation = higher latency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install onnxruntime transformers pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "print(f\"‚úÖ ONNX Runtime version: {ort.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 2: Tokenization Analysis Function\n",
        "\n",
        "### What this function does:\n",
        "- Takes a list of prompts\n",
        "- For each prompt, shows the token IDs and how many tokens it creates\n",
        "- This helps us understand why some prompts are \"heavier\" than others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_tokenized_info(prompts: list, tokenizer):\n",
        "    \"\"\"\n",
        "    Print tokenization details for a list of prompts.\n",
        "    \n",
        "    For each prompt, displays:\n",
        "    - Original text\n",
        "    - Token IDs\n",
        "    - Decoded tokens (to see how text was split)\n",
        "    - Total token count\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"TOKENIZATION ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        # Tokenize the prompt\n",
        "        encoding = tokenizer(prompt, return_tensors=\"np\")\n",
        "        token_ids = encoding[\"input_ids\"][0].tolist()\n",
        "        \n",
        "        # Decode each token individually to see the pieces\n",
        "        decoded_tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
        "        \n",
        "        # Print details\n",
        "        print(f\"\\nüìù Prompt: \\\"{prompt}\\\"\")\n",
        "        print(f\"   Char count: {len(prompt)}\")\n",
        "        print(f\"   Token count: {len(token_ids)}\")\n",
        "        print(f\"   Token IDs: {token_ids}\")\n",
        "        print(f\"   Tokens: {decoded_tokens}\")\n",
        "        \n",
        "        results.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"char_count\": len(prompt),\n",
        "            \"token_count\": len(token_ids),\n",
        "            \"token_ids\": token_ids\n",
        "        })\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's analyze some prompts!\n",
        "\n",
        "We'll look at how different types of text tokenize:\n",
        "- Simple words\n",
        "- Technical terms\n",
        "- Numbers and symbols\n",
        "- Long uncommon words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load GPT-2 tokenizer (commonly used with many models)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Define prompts with different characteristics\n",
        "test_prompts = [\n",
        "    \"Hello world\",                                          # Simple, common words\n",
        "    \"Supercalifragilisticexpialidocious\",                   # Long uncommon word\n",
        "    \"GPT-4, BERT, and T5 are transformer models.\",          # Technical terms\n",
        "    \"The price is $1,234.56 USD.\",                          # Numbers and symbols\n",
        "    \"The cat sat on the mat.\",                              # Common short words\n",
        "]\n",
        "\n",
        "# Analyze tokenization\n",
        "token_results = print_tokenized_info(test_prompts, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° What did we learn?\n",
        "\n",
        "Notice how:\n",
        "- Common words like \"Hello\" and \"world\" are single tokens\n",
        "- Rare words get split into multiple pieces\n",
        "- Numbers and special characters often become separate tokens\n",
        "- Token count can vary dramatically for similar-length text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚è±Ô∏è Step 3: Measure Latency vs Token Count\n",
        "\n",
        "### Goal:\n",
        "Show that **token count** (not character count) is what determines latency.\n",
        "\n",
        "### Why this matters:\n",
        "When comparing model performance, you must consider token count. Two prompts with the same number of characters can have very different latencies if they tokenize differently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, load the ONNX model\n",
        "# Note: You need to upload your ONNX model to Colab.\n",
        "# To get a model:\n",
        "#   1. Use a pre-exported ONNX model from Hugging Face Hub\n",
        "#   2. Or export your own using: python -m transformers.onnx --model=gpt2 onnx_model/\n",
        "#   3. Upload the .onnx file to Colab using the file browser on the left\n",
        "model_path = \"/tmp/tinygpt.onnx\"\n",
        "\n",
        "# Create inference session\n",
        "session = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "print(\"‚úÖ Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def measure_latency_vs_tokens(prompts: list, session, tokenizer, num_runs: int = 5):\n",
        "    \"\"\"\n",
        "    Measure inference latency for prompts and correlate with token count.\n",
        "    \n",
        "    Args:\n",
        "        prompts: List of text prompts to test\n",
        "        session: ONNX Runtime InferenceSession\n",
        "        tokenizer: Hugging Face tokenizer\n",
        "        num_runs: Number of runs per prompt for averaging\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with latency measurements and token counts\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"np\")\n",
        "        token_count = inputs[\"input_ids\"].shape[1]\n",
        "        char_count = len(prompt)\n",
        "        \n",
        "        # Measure latency over multiple runs using perf_counter for accuracy\n",
        "        latencies = []\n",
        "        for _ in range(num_runs):\n",
        "            t0 = time.perf_counter()\n",
        "            _ = session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "            t1 = time.perf_counter()\n",
        "            latencies.append((t1 - t0) * 1000)  # Convert to ms\n",
        "        \n",
        "        mean_latency = np.mean(latencies)\n",
        "        std_latency = np.std(latencies)\n",
        "        \n",
        "        results.append({\n",
        "            \"prompt\": prompt[:40] + \"...\" if len(prompt) > 40 else prompt,\n",
        "            \"char_count\": char_count,\n",
        "            \"token_count\": token_count,\n",
        "            \"mean_latency_ms\": round(mean_latency, 2),\n",
        "            \"std_latency_ms\": round(std_latency, 2),\n",
        "            \"latency_per_token_ms\": round(mean_latency / token_count, 3)\n",
        "        })\n",
        "        \n",
        "        print(f\"‚úÖ Processed: '{prompt[:30]}...' - {token_count} tokens, {mean_latency:.2f}ms\")\n",
        "    \n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create test prompts with varying characteristics\n",
        "latency_test_prompts = [\n",
        "    # Short prompts\n",
        "    \"Hi\",\n",
        "    \"Hello world\",\n",
        "    \n",
        "    # Medium prompts - common words\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \n",
        "    # Medium prompts - technical terms (more tokens)\n",
        "    \"GPT-4 utilizes RLHF and PPO algorithms.\",\n",
        "    \n",
        "    # Longer prompts\n",
        "    \"Artificial intelligence is transforming how we work, live, and interact with technology.\",\n",
        "    \n",
        "    # Numbers heavy (often tokenize into many pieces)\n",
        "    \"The values are 123, 456, 789, 101112, and 131415.\",\n",
        "]\n",
        "\n",
        "# Run the latency measurement\n",
        "latency_df = measure_latency_vs_tokens(latency_test_prompts, session, tokenizer, num_runs=5)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nüìä Latency vs Token Count Results:\")\n",
        "display(latency_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° What did we learn?\n",
        "\n",
        "Look at the results table:\n",
        "- **Token count correlates with latency** better than character count\n",
        "- Technical terms and numbers often create more tokens\n",
        "- The `latency_per_token_ms` column shows relatively consistent per-token cost\n",
        "\n",
        "**Key takeaway:** When benchmarking, always report token counts alongside latency!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 4: ONNX Runtime Profiling\n",
        "\n",
        "### What is profiling?\n",
        "\n",
        "Profiling shows us **where time is spent** inside the model. Instead of just knowing \"inference took 50ms\", we can see:\n",
        "- MatMul (matrix multiplication): 30ms\n",
        "- Softmax: 10ms\n",
        "- Add operations: 5ms\n",
        "- etc.\n",
        "\n",
        "This helps identify bottlenecks for optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_with_profiling(model_path: str, prompt: str, tokenizer):\n",
        "    \"\"\"\n",
        "    Run inference with ONNX profiling enabled and return operator timings.\n",
        "    \n",
        "    Args:\n",
        "        model_path: Path to ONNX model\n",
        "        prompt: Text prompt to run\n",
        "        tokenizer: Hugging Face tokenizer\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with operator timing summary\n",
        "    \"\"\"\n",
        "    # Create session options with profiling enabled\n",
        "    options = ort.SessionOptions()\n",
        "    options.enable_profiling = True\n",
        "    \n",
        "    # Create session\n",
        "    session = ort.InferenceSession(\n",
        "        model_path, \n",
        "        options, \n",
        "        providers=[\"CPUExecutionProvider\"]\n",
        "    )\n",
        "    \n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
        "    \n",
        "    # Run inference (this generates the profile)\n",
        "    _ = session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "    \n",
        "    # Get the profile file path and end profiling\n",
        "    profile_file = session.end_profiling()\n",
        "    print(f\"üìÑ Profile saved to: {profile_file}\")\n",
        "    \n",
        "    return profile_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_profile_summary(profile_file: str):\n",
        "    \"\"\"\n",
        "    Parse ONNX profile JSON and return a summary of operator timings.\n",
        "    \n",
        "    Args:\n",
        "        profile_file: Path to the profile JSON file\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with aggregated operator timings\n",
        "    \"\"\"\n",
        "    # Read the profile JSON\n",
        "    with open(profile_file, 'r') as f:\n",
        "        profile_data = json.load(f)\n",
        "    \n",
        "    # Extract operator timings\n",
        "    op_timings = {}\n",
        "    \n",
        "    for event in profile_data:\n",
        "        # Profile events have 'cat' (category) and 'dur' (duration in microseconds)\n",
        "        if 'cat' in event and 'dur' in event:\n",
        "            op_type = event.get('name', 'unknown').split('_')[0]  # Get base operator name\n",
        "            duration_us = event['dur']\n",
        "            \n",
        "            if op_type not in op_timings:\n",
        "                op_timings[op_type] = {'total_us': 0, 'count': 0}\n",
        "            \n",
        "            op_timings[op_type]['total_us'] += duration_us\n",
        "            op_timings[op_type]['count'] += 1\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    rows = []\n",
        "    for op_name, stats in op_timings.items():\n",
        "        rows.append({\n",
        "            'operator': op_name,\n",
        "            'total_time_ms': round(stats['total_us'] / 1000, 3),\n",
        "            'call_count': stats['count'],\n",
        "            'avg_time_ms': round(stats['total_us'] / stats['count'] / 1000, 3)\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(rows)\n",
        "    df = df.sort_values('total_time_ms', ascending=False)\n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run profiling on a test prompt\n",
        "test_prompt = \"Explain the concept of machine learning in simple terms.\"\n",
        "profile_file = run_with_profiling(model_path, test_prompt, tokenizer)\n",
        "\n",
        "# Parse and display the profile summary\n",
        "print(\"\\nüìä Operator Timing Summary (Top 10):\")\n",
        "profile_df = parse_profile_summary(profile_file)\n",
        "display(profile_df.head(10))\n",
        "\n",
        "# Calculate total time\n",
        "total_time = profile_df['total_time_ms'].sum()\n",
        "print(f\"\\n‚è±Ô∏è Total profiled time: {total_time:.2f} ms\")\n",
        "\n",
        "# Show percentage breakdown\n",
        "print(\"\\nüìà Top 5 operators by time percentage:\")\n",
        "top5 = profile_df.head(5).copy()\n",
        "top5['percentage'] = (top5['total_time_ms'] / total_time * 100).round(1)\n",
        "for _, row in top5.iterrows():\n",
        "    print(f\"   {row['operator']}: {row['percentage']}% ({row['total_time_ms']}ms)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üí° What did we learn?\n",
        "\n",
        "The profile shows us:\n",
        "- Which operators consume the most time (usually MatMul, Attention)\n",
        "- How many times each operator is called\n",
        "- Where to focus optimization efforts\n",
        "\n",
        "**Key insight:** Most time is spent in a few heavy operators. Optimizing these (e.g., with GPU acceleration) gives the biggest speedups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîÑ Inversion Thinking: How Can Tokenization Mislead Evaluation?\n",
        "\n",
        "Instead of asking \"How does tokenization help?\", let's ask:\n",
        "\n",
        "> **\"How can tokenization mislead our evaluation results?\"**\n",
        "\n",
        "### Common pitfalls:\n",
        "\n",
        "1. **Comparing prompts by character length** - Two 100-character prompts can have very different token counts\n",
        "2. **Ignoring tokenizer differences** - Different models use different tokenizers\n",
        "3. **Forgetting tokenization overhead** - Tokenization itself takes time\n",
        "4. **Multilingual bias** - English text often tokenizes more efficiently than other languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstration: Same character count, different token counts\n",
        "prompt_a = \"The sun rises in the east and sets in the west every day.\"  # Common words\n",
        "prompt_b = \"GPT-4's RLHF utilizes PPO with KL-divergence constraints.\"   # Technical terms\n",
        "\n",
        "print(f\"Prompt A: \\\"{prompt_a}\\\"\")\n",
        "print(f\"  Characters: {len(prompt_a)}\")\n",
        "print(f\"  Tokens: {len(tokenizer(prompt_a)['input_ids'])}\")\n",
        "\n",
        "print(f\"\\nPrompt B: \\\"{prompt_b}\\\"\")\n",
        "print(f\"  Characters: {len(prompt_b)}\")\n",
        "print(f\"  Tokens: {len(tokenizer(prompt_b)['input_ids'])}\")\n",
        "\n",
        "print(\"\\n‚ö†Ô∏è Notice: Similar character counts but different token counts!\")\n",
        "print(\"   This means latency comparisons based only on character count are misleading.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Mini-Project: Compare Same-Length Prompts\n",
        "\n",
        "### Your task:\n",
        "1. Create two prompts with the same character count (~100 chars)\n",
        "2. One should use common words, one should use technical/rare terms\n",
        "3. Measure and compare their latencies\n",
        "4. Calculate latency per token\n",
        "5. Document your findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create your two prompts (aim for ~100 characters each)\n",
        "\n",
        "your_prompt_a = \"\"  # Common words - should have fewer tokens\n",
        "your_prompt_b = \"\"  # Technical terms - should have more tokens\n",
        "\n",
        "# Measure latency for each (uncomment and modify)\n",
        "# your_prompts = [your_prompt_a, your_prompt_b]\n",
        "# results_df = measure_latency_vs_tokens(your_prompts, session, tokenizer, num_runs=10)\n",
        "# display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 3, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I can explain what a token is in simple terms\n",
        "- [ ] I understand why token count matters more than character count\n",
        "- [ ] I can inspect tokenization using the transformers library\n",
        "- [ ] I can enable ONNX profiling and interpret operator timings\n",
        "- [ ] I understand how tokenization can mislead evaluation\n",
        "- [ ] I completed the mini-project\n",
        "\n",
        "---\n",
        "\n",
        "**Week 2 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 3 ‚Äî Perplexity & Basic Benchmarks*"
      ]
    }
  ]
}
