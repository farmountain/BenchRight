# BenchRight Syllabus

An 18-week curriculum for mastering LLM evaluation and benchmarking.

## Week 1: Foundations of LLM Evaluation & First Principles
- Understand what LLM evaluation is and why it matters
- Learn the 4 pillars of LLM evaluation: Quantitative, Qualitative, Safety, Performance
- Use Google Colab to run your first ONNX LLM benchmark
- Apply First Principles, Feynman Technique, Design Thinking, Paulâ€“Elder Critical Thinking, Inversion Thinking, and Reflexion Loops
- Run ONNX model load, first inference, and basic latency measurement

## Week 2: Tokenization & ONNX Runtime Internals
- Understand tokenization algorithms (BPE, WordPiece, SentencePiece)
- Explore ONNX Runtime architecture and execution providers
- Profile tokenization performance
- Compare different tokenizer implementations

## Week 3: Perplexity & Basic Benchmarks
- Calculate and interpret perplexity scores
- Implement basic accuracy benchmarks
- Understand cross-entropy loss in evaluation
- Build reproducible benchmark pipelines

## Week 4: Industry Benchmark Suites (MMLU, HellaSwag, BBH)
- Implement MMLU (Massive Multitask Language Understanding)
- Run HellaSwag commonsense reasoning benchmarks
- Execute BBH (BIG-Bench Hard) evaluations
- Compare model performance across benchmark suites

## Week 5: LLM-as-Judge Evaluation
- Design LLM-as-judge scoring systems
- Implement pairwise comparison frameworks
- Build automated quality assessment pipelines
- Handle evaluation biases and calibration

## Week 6: Safety Evaluation & Red-Teaming
- Implement TruthfulQA benchmarks
- Run toxicity detection tests
- Design red-teaming prompt sets
- Build safety guardrail evaluation systems

## Week 7: Robustness Testing
- Implement adversarial input generation
- Test model consistency across paraphrases
- Evaluate out-of-distribution performance
- Build robustness regression tests

## Week 8: Performance Benchmarking
- Measure latency and throughput
- Profile memory usage and optimization
- Implement ONNX operator-level profiling
- Build performance monitoring dashboards

## Week 9: Regression Testing Pipelines
- Design continuous evaluation workflows
- Implement automated benchmark suites
- Build alerting systems for model degradation
- Create reproducible evaluation environments

## Week 10: Evaluation Infrastructure
- Build scalable evaluation pipelines
- Implement distributed benchmarking
- Design result aggregation systems
- Create evaluation APIs and dashboards

## Week 11: Banking & Finance Use Cases
- Evaluate models for financial document analysis
- Build compliance-focused benchmarks
- Implement risk assessment evaluations
- Design fraud detection model tests

## Week 12: Healthcare Use Cases
- Evaluate medical text understanding
- Build clinical NLP benchmarks
- Implement safety-critical evaluation protocols
- Design patient data handling tests

## Week 13: Software Engineering Use Cases
- Evaluate code generation quality
- Build code review automation benchmarks
- Implement bug detection model tests
- Design documentation generation evaluations

## Week 14: Data Analytics Use Cases
- Evaluate data summarization models
- Build SQL generation benchmarks
- Implement data visualization model tests
- Design report generation evaluations

## Week 15: RAG (Retrieval-Augmented Generation) Use Cases
- Evaluate retrieval quality and relevance
- Build RAG pipeline benchmarks
- Implement citation accuracy tests
- Design hallucination detection for RAG

## Week 16: Marketing & Content Use Cases
- Evaluate content generation quality
- Build brand voice consistency benchmarks
- Implement A/B testing frameworks
- Design engagement prediction evaluations

## Week 17: Full System Architecture
- Design end-to-end evaluation systems
- Build multi-model comparison frameworks
- Implement production monitoring systems
- Create comprehensive model cards

## Week 18: Capstone Project
- Build a complete evaluation system
- Implement custom benchmark suite
- Create production-ready evaluation pipeline
- Present findings and recommendations
