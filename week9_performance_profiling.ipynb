{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 9 ‚Äî Performance Profiling\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand performance profiling concepts and metrics\n",
        "2. Use the `PerformanceProfiler` class to wrap an ONNX Runtime session\n",
        "3. Use the `profile_model` function to measure per-prompt metrics\n",
        "4. Analyze performance data using pandas DataFrames\n",
        "5. Generate and interpret summary statistics (mean, std)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† Why Performance Profiling Matters\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "Deploying LLMs to production requires understanding their performance characteristics:\n",
        "\n",
        "| Metric | What It Measures | Business Impact |\n",
        "|--------|------------------|------------------|\n",
        "| **Latency** | Time from input to output | User experience, SLAs |\n",
        "| **Throughput** | Tokens processed per second | Cost efficiency |\n",
        "| **Memory** | RAM/VRAM consumption | Hardware requirements |\n",
        "\n",
        "### Why Profile?\n",
        "\n",
        "- Predict infrastructure costs\n",
        "- Set realistic SLAs\n",
        "- Compare model configurations\n",
        "- Identify optimization opportunities\n",
        "- Plan capacity for production"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import time\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# Install dependencies if needed\n",
        "# !pip install pandas numpy psutil\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Step 2: Import the Performance Profiler Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the performance profiling functions\n",
        "from src.benchmark_engine.performance_profiler import (\n",
        "    PerformanceProfiler,\n",
        "    profile_model,\n",
        "    create_mock_profiler,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Performance profiler module imported successfully!\")\n",
        "print(\"\\nüìã Available components:\")\n",
        "print(\"   - PerformanceProfiler: Class for profiling ONNX models\")\n",
        "print(\"   - profile_model: Function that returns a DataFrame with metrics\")\n",
        "print(\"   - create_mock_profiler: Helper for testing without real models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 3: Understanding Performance Metrics\n",
        "\n",
        "The `profile_model` function measures:\n",
        "- **latency_ms**: Wall-clock time in milliseconds\n",
        "- **tokens_per_second**: Throughput (input + output tokens / time)\n",
        "- **memory_usage_mb**: Memory consumption in megabytes (if available)\n",
        "- **input_tokens**: Number of input tokens\n",
        "- **output_tokens**: Number of output tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a mock model for demonstration\n",
        "# In production, you would use a real ONNX model\n",
        "\n",
        "def mock_model(prompt: str) -> str:\n",
        "    \"\"\"A mock model that simulates varying inference times.\"\"\"\n",
        "    # Simulate longer inference for longer prompts\n",
        "    base_delay = 0.01  # 10ms base\n",
        "    length_factor = len(prompt.split()) * 0.002  # 2ms per word\n",
        "    time.sleep(base_delay + length_factor)\n",
        "    return f\"Response to: {prompt[:30]}...\"\n",
        "\n",
        "# Create a mock profiler\n",
        "profiler_fn = create_mock_profiler(mock_model)\n",
        "\n",
        "print(\"‚úÖ Mock model created!\")\n",
        "print(\"   This simulates varying latency based on prompt length.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 4: Profile Individual Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Profile a single prompt\n",
        "print(\"üìä Single Prompt Profiling\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_prompt = \"What is the capital of France?\"\n",
        "result = profiler_fn(test_prompt)\n",
        "\n",
        "print(f\"\\nüìù Prompt: {test_prompt}\")\n",
        "print(f\"\\nüìà Metrics:\")\n",
        "print(f\"   Input tokens:  {result['input_tokens']}\")\n",
        "print(f\"   Output tokens: {result['output_tokens']}\")\n",
        "print(f\"   Latency:       {result['latency_ms']:.2f} ms\")\n",
        "print(f\"   Tokens/sec:    {result['tokens_per_second']:.2f}\")\n",
        "if result['memory_usage_mb'] is not None:\n",
        "    print(f\"   Memory:        {result['memory_usage_mb']:.2f} MB\")\n",
        "else:\n",
        "    print(f\"   Memory:        N/A (psutil not available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 5: Profile Multiple Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test prompts of varying complexity\n",
        "test_prompts = [\n",
        "    \"Hello world\",\n",
        "    \"What is the capital of France?\",\n",
        "    \"Explain machine learning in simple terms for a beginner.\",\n",
        "    \"Write a short poem about the ocean and its waves crashing on the shore.\",\n",
        "    \"Summarize the key principles of software engineering, including topics like abstraction, modularity, and testing.\",\n",
        "]\n",
        "\n",
        "print(\"üìä Multi-Prompt Profiling\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Profile all prompts\n",
        "results = []\n",
        "for i, prompt in enumerate(test_prompts, 1):\n",
        "    result = profiler_fn(prompt)\n",
        "    results.append(result)\n",
        "    print(f\"\\n[{i}/{len(test_prompts)}] Profiled: '{prompt[:40]}...'\")\n",
        "    print(f\"    Latency: {result['latency_ms']:.2f} ms | Tokens/s: {result['tokens_per_second']:.2f}\")\n",
        "\n",
        "print(\"\\n‚úÖ All prompts profiled!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 6: Create and Analyze DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create DataFrame from results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"üìä Performance Results DataFrame\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display the full DataFrame\n",
        "display(df[['prompt', 'input_tokens', 'latency_ms', 'tokens_per_second']])\n",
        "\n",
        "# Add a shortened prompt column for display\n",
        "df['prompt_short'] = df['prompt'].apply(lambda x: x[:30] + '...' if len(x) > 30 else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìà Step 7: Generate Summary Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Summary Statistics\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Latency stats\n",
        "print(\"\\n‚è±Ô∏è Latency (ms):\")\n",
        "print(f\"   Mean: {df['latency_ms'].mean():.2f}\")\n",
        "print(f\"   Std:  {df['latency_ms'].std():.2f}\")\n",
        "print(f\"   Min:  {df['latency_ms'].min():.2f}\")\n",
        "print(f\"   Max:  {df['latency_ms'].max():.2f}\")\n",
        "\n",
        "# Tokens per second stats\n",
        "print(\"\\nüöÄ Tokens per second:\")\n",
        "print(f\"   Mean: {df['tokens_per_second'].mean():.2f}\")\n",
        "print(f\"   Std:  {df['tokens_per_second'].std():.2f}\")\n",
        "\n",
        "# Token counts\n",
        "print(\"\\nüìù Token counts:\")\n",
        "print(f\"   Total input tokens:  {df['input_tokens'].sum()}\")\n",
        "print(f\"   Total output tokens: {df['output_tokens'].sum()}\")\n",
        "print(f\"   Total time: {df['inference_time_seconds'].sum():.4f} seconds\")\n",
        "\n",
        "# Memory stats (if available)\n",
        "if df['memory_usage_mb'].notna().any():\n",
        "    print(\"\\nüíæ Memory usage (MB):\")\n",
        "    print(f\"   Mean: {df['memory_usage_mb'].mean():.2f}\")\n",
        "    print(f\"   Max:  {df['memory_usage_mb'].max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 8: Analyze Performance Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Performance Pattern Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Find slowest and fastest prompts\n",
        "slowest_idx = df['latency_ms'].idxmax()\n",
        "fastest_idx = df['latency_ms'].idxmin()\n",
        "\n",
        "print(\"\\nüê¢ Slowest prompt:\")\n",
        "print(f\"   Prompt: {df.loc[slowest_idx, 'prompt'][:50]}...\")\n",
        "print(f\"   Latency: {df.loc[slowest_idx, 'latency_ms']:.2f} ms\")\n",
        "print(f\"   Input tokens: {df.loc[slowest_idx, 'input_tokens']}\")\n",
        "\n",
        "print(\"\\nüöÄ Fastest prompt:\")\n",
        "print(f\"   Prompt: {df.loc[fastest_idx, 'prompt'][:50]}...\")\n",
        "print(f\"   Latency: {df.loc[fastest_idx, 'latency_ms']:.2f} ms\")\n",
        "print(f\"   Input tokens: {df.loc[fastest_idx, 'input_tokens']}\")\n",
        "\n",
        "# Correlation analysis\n",
        "correlation = df['input_tokens'].corr(df['latency_ms'])\n",
        "print(f\"\\nüìà Correlation (input_tokens vs latency): {correlation:.3f}\")\n",
        "\n",
        "if correlation > 0.7:\n",
        "    print(\"   Strong positive correlation: Longer prompts take more time.\")\n",
        "elif correlation > 0.3:\n",
        "    print(\"   Moderate positive correlation: Some relationship between length and latency.\")\n",
        "else:\n",
        "    print(\"   Weak or no correlation: Latency may depend on other factors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 9: Percentile Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Percentile Analysis\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate percentiles for latency\n",
        "percentiles = {\n",
        "    'P50': df['latency_ms'].quantile(0.50),\n",
        "    'P75': df['latency_ms'].quantile(0.75),\n",
        "    'P90': df['latency_ms'].quantile(0.90),\n",
        "    'P95': df['latency_ms'].quantile(0.95),\n",
        "    'P99': df['latency_ms'].quantile(0.99),\n",
        "}\n",
        "\n",
        "print(\"\\n‚è±Ô∏è Latency Percentiles:\")\n",
        "for name, value in percentiles.items():\n",
        "    print(f\"   {name}: {value:.2f} ms\")\n",
        "\n",
        "print(\"\\nüìù Interpretation:\")\n",
        "print(f\"   - 50% of requests complete in under {percentiles['P50']:.2f} ms (P50)\")\n",
        "print(f\"   - 95% of requests complete in under {percentiles['P95']:.2f} ms (P95)\")\n",
        "print(f\"   - The slowest 1% take over {percentiles['P99']:.2f} ms (P99)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 10: Using profile_model with Real ONNX Models\n",
        "\n",
        "When you have a real ONNX model, use `profile_model` directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example code for profiling a real ONNX model\n",
        "# Uncomment and modify when you have an actual model\n",
        "\n",
        "# from src.benchmark_engine.performance_profiler import profile_model\n",
        "#\n",
        "# prompts = [\n",
        "#     \"What is the capital of France?\",\n",
        "#     \"Explain machine learning.\",\n",
        "#     \"Write a poem about the ocean.\",\n",
        "# ]\n",
        "#\n",
        "# # Profile the model with summary statistics\n",
        "# df = profile_model(\n",
        "#     model_path=\"/path/to/model.onnx\",\n",
        "#     prompts=prompts,\n",
        "#     tokenizer_name=\"gpt2\",\n",
        "#     warmup_runs=1,\n",
        "#     num_runs=3,  # Average over 3 runs\n",
        "#     print_summary=True\n",
        "# )\n",
        "#\n",
        "# print(df)\n",
        "\n",
        "print(\"üìù Note: Replace with your ONNX model path when available.\")\n",
        "print(\"   The profile_model function will:\")\n",
        "print(\"   1. Load the ONNX model and tokenizer\")\n",
        "print(\"   2. Profile each prompt\")\n",
        "print(\"   3. Return a DataFrame with all metrics\")\n",
        "print(\"   4. Optionally print summary statistics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Mini-Project: Performance Audit\n",
        "\n",
        "### Task\n",
        "\n",
        "Create a comprehensive performance audit of a model.\n",
        "\n",
        "### Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your performance audit code here\n",
        "\n",
        "# Step 1: Define your prompts\n",
        "# audit_prompts = [\n",
        "#     # Short prompts\n",
        "#     \"Hello world\",\n",
        "#     \"What is AI?\",\n",
        "#     # Medium prompts\n",
        "#     ...\n",
        "#     # Long prompts\n",
        "#     ...\n",
        "# ]\n",
        "\n",
        "# Step 2: Profile the model\n",
        "# df = profile_model(\n",
        "#     model_path=\"your_model.onnx\",\n",
        "#     prompts=audit_prompts,\n",
        "#     print_summary=True\n",
        "# )\n",
        "\n",
        "# Step 3: Analyze results\n",
        "# - Calculate percentiles\n",
        "# - Find correlations\n",
        "# - Identify outliers\n",
        "\n",
        "# Step 4: Create your audit report\n",
        "# Export to /examples/week09_performance_audit.md\n",
        "\n",
        "print(\"üìù Complete the mini-project using the template above.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§î Paul-Elder Critical Thinking Questions\n",
        "\n",
        "Reflect on these questions:\n",
        "\n",
        "### Question 1: EVIDENCE\n",
        "**If latency varies significantly between runs, what evidence would help explain this?**\n",
        "*Consider: System load, garbage collection, JIT compilation, memory pressure.*\n",
        "\n",
        "### Question 2: ASSUMPTIONS\n",
        "**What assumptions are we making when we profile on a single machine?**\n",
        "*Consider: Production hardware, network latency, concurrent users, cold starts.*\n",
        "\n",
        "### Question 3: IMPLICATIONS\n",
        "**If we set an SLA based on mean latency, what could go wrong?**\n",
        "*Consider: Tail latency (P99), outliers, worst-case scenarios.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitations of Performance Profiling\n",
        "\n",
        "### What These Metrics DON'T Cover\n",
        "\n",
        "1. **Quality Trade-offs:** Fast inference doesn't mean good quality\n",
        "2. **Concurrent Load:** Single-request latency differs from production\n",
        "3. **Cold Start:** First-request latency after model load\n",
        "4. **Network Latency:** Real deployments include API overhead\n",
        "5. **Memory Leaks:** Need long-running tests to detect\n",
        "\n",
        "### Future Improvements (TODO)\n",
        "\n",
        "- GPU memory tracking\n",
        "- Concurrent request simulation\n",
        "- Cold start analysis\n",
        "- Batch size optimization\n",
        "- Power consumption metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 10, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand why performance profiling is critical for LLM deployment\n",
        "- [ ] I can use `PerformanceProfiler` to profile an ONNX model\n",
        "- [ ] I can use `profile_model` to generate a metrics DataFrame\n",
        "- [ ] I can calculate and interpret summary statistics (mean, std)\n",
        "- [ ] I understand percentile metrics (P50, P95, P99)\n",
        "- [ ] I know the limitations of performance profiling\n",
        "\n",
        "---\n",
        "\n",
        "**Week 9 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 10 ‚Äî Regression Tests*"
      ]
    }
  ]
}
