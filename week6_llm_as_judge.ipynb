{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6 â€” LLM-as-Judge: Automated Evaluation with Language Models\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand the LLM-as-Judge evaluation paradigm\n",
        "2. Use the `LLMJudge` class from `src/benchmark_engine`\n",
        "3. Evaluate model answers using an LLM as the judge\n",
        "4. Compare LLM-as-Judge with exact match metrics\n",
        "5. Analyze when different metrics disagree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ§  What is LLM-as-Judge?\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "Traditional metrics like exact match are too rigid:\n",
        "\n",
        "| Question | Reference | Model Answer | Exact Match | Correct? |\n",
        "|----------|-----------|--------------|-------------|----------|\n",
        "| Capital of France? | Paris | Paris | 1.0 âœ“ | Yes |\n",
        "| Capital of France? | Paris | The capital is Paris | 0.0 âœ— | Yes! |\n",
        "| Capital of France? | Paris | Paris, France | 0.0 âœ— | Yes! |\n",
        "\n",
        "### The Solution\n",
        "\n",
        "Use a powerful LLM (like GPT-4) to evaluate answers like a human would:\n",
        "- Understands semantics, not just string matching\n",
        "- Can handle paraphrases and elaborations\n",
        "- Provides rationale for the score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ› ï¸ Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import json\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(\"âœ… Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“¦ Step 2: Import the LLMJudge Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the LLMJudge class\n",
        "from src.benchmark_engine.llm_judge import LLMJudge, JUDGE_SYSTEM_PROMPT\n",
        "\n",
        "print(\"âœ… LLMJudge imported successfully!\")\n",
        "print(f\"\\nðŸ“‹ System Prompt Preview (first 200 chars):\")\n",
        "print(JUDGE_SYSTEM_PROMPT[:200] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ¤– Step 3: Create an LLM Client\n",
        "\n",
        "### Option A: Using OpenAI API (requires API key)\n",
        "\n",
        "```python\n",
        "# Uncomment and run if you have an OpenAI API key\n",
        "# !pip install openai\n",
        "# from openai import OpenAI\n",
        "# client = OpenAI()  # Uses OPENAI_API_KEY env variable\n",
        "```\n",
        "\n",
        "### Option B: Using a Mock Client (for demonstration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockChatCompletions:\n",
        "    \"\"\"Mock chat completions that returns predefined responses.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Define expected responses for different scenarios\n",
        "        self.responses = {\n",
        "            # Correct answers\n",
        "            \"paris\": '{\"score\": 1.0, \"rationale\": \"The answer correctly identifies Paris as the capital of France, matching the reference answer perfectly.\"}',\n",
        "            \"the capital is paris\": '{\"score\": 0.95, \"rationale\": \"The answer correctly identifies Paris as the capital. The additional phrasing provides context without reducing accuracy.\"}',\n",
        "            \"4\": '{\"score\": 1.0, \"rationale\": \"The answer correctly states that 2+2 equals 4, matching the reference exactly.\"}',\n",
        "            \"the sum is 4\": '{\"score\": 0.95, \"rationale\": \"The answer correctly provides the sum of 2+2 as 4. The phrasing is clear and helpful.\"}',\n",
        "            \"jupiter\": '{\"score\": 1.0, \"rationale\": \"The answer correctly identifies Jupiter as the largest planet in our solar system.\"}',\n",
        "            \n",
        "            # Incorrect answers\n",
        "            \"mars\": '{\"score\": 0.0, \"rationale\": \"The answer is incorrect. Mars is not the largest planet; Jupiter is the largest planet in our solar system.\"}',\n",
        "            \"london\": '{\"score\": 0.0, \"rationale\": \"The answer is incorrect. London is not the capital of France; Paris is the capital.\"}',\n",
        "            \"5\": '{\"score\": 0.0, \"rationale\": \"The answer is mathematically incorrect. 2+2 equals 4, not 5.\"}',\n",
        "            \n",
        "            # Partial credit\n",
        "            \"i think it might be paris\": '{\"score\": 0.7, \"rationale\": \"The answer correctly identifies Paris but expresses unnecessary uncertainty, reducing confidence.\"}',\n",
        "            \"h2o\": '{\"score\": 1.0, \"rationale\": \"The answer correctly identifies H2O as the chemical formula for water.\"}',\n",
        "            \"water is h2o\": '{\"score\": 0.95, \"rationale\": \"The answer correctly identifies H2O as the formula for water with helpful context.\"}',\n",
        "        }\n",
        "        \n",
        "        # Default response for unknown inputs\n",
        "        self.default_response = '{\"score\": 0.5, \"rationale\": \"The answer partially addresses the question but could be more precise.\"}'\n",
        "    \n",
        "    def create(self, model: str, messages: List[Dict], temperature: float = 0.0):\n",
        "        \"\"\"Simulate a chat completion API call.\"\"\"\n",
        "        # Extract the user message to determine the answer being evaluated\n",
        "        user_msg = messages[-1][\"content\"].lower()\n",
        "        \n",
        "        # Find the answer in the user message\n",
        "        response_text = self.default_response\n",
        "        for key, value in self.responses.items():\n",
        "            if f\"answer: {key}\" in user_msg:\n",
        "                response_text = value\n",
        "                break\n",
        "        \n",
        "        # Create mock response object\n",
        "        class MockMessage:\n",
        "            content = response_text\n",
        "        \n",
        "        class MockChoice:\n",
        "            message = MockMessage()\n",
        "        \n",
        "        class MockResponse:\n",
        "            choices = [MockChoice()]\n",
        "        \n",
        "        return MockResponse()\n",
        "\n",
        "\n",
        "class MockChat:\n",
        "    \"\"\"Mock chat interface.\"\"\"\n",
        "    completions = MockChatCompletions()\n",
        "\n",
        "\n",
        "class MockOpenAIClient:\n",
        "    \"\"\"Mock OpenAI client for demonstration without API key.\"\"\"\n",
        "    chat = MockChat()\n",
        "\n",
        "\n",
        "# Create the mock client\n",
        "mock_client = MockOpenAIClient()\n",
        "print(\"âœ… Mock OpenAI client created!\")\n",
        "print(\"   (Replace with real OpenAI client for production use)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ›ï¸ Step 4: Initialize the LLMJudge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the LLMJudge with our client\n",
        "judge = LLMJudge(\n",
        "    client=mock_client,\n",
        "    model=\"gpt-4o-mini\",  # Model name (mock ignores this)\n",
        ")\n",
        "\n",
        "print(\"âœ… LLMJudge initialized!\")\n",
        "print(f\"   Model: {judge.model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ§ª Step 5: Test Single Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test a single evaluation\n",
        "result = judge.score_answer(\n",
        "    question=\"What is the capital of France?\",\n",
        "    answer=\"Paris\",\n",
        "    reference=\"Paris\"\n",
        ")\n",
        "\n",
        "print(\"ðŸ“ Single Evaluation Test:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Question:  What is the capital of France?\")\n",
        "print(f\"Answer:    Paris\")\n",
        "print(f\"Reference: Paris\")\n",
        "print(f\"\\nðŸ“Š Result:\")\n",
        "print(f\"   Score:     {result['score']:.2f}\")\n",
        "print(f\"   Rationale: {result['rationale']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š Step 6: Evaluate Multiple Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define test cases with varying correctness levels\n",
        "test_cases = [\n",
        "    # Exact matches (should score 1.0)\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"answer\": \"Paris\",\n",
        "        \"reference\": \"Paris\",\n",
        "        \"expected_high\": True\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is 2+2?\",\n",
        "        \"answer\": \"4\",\n",
        "        \"reference\": \"4\",\n",
        "        \"expected_high\": True\n",
        "    },\n",
        "    # Correct with elaboration (should score high)\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"answer\": \"The capital is Paris\",\n",
        "        \"reference\": \"Paris\",\n",
        "        \"expected_high\": True\n",
        "    },\n",
        "    # Incorrect answers (should score low)\n",
        "    {\n",
        "        \"question\": \"What is the largest planet?\",\n",
        "        \"answer\": \"Mars\",\n",
        "        \"reference\": \"Jupiter\",\n",
        "        \"expected_high\": False\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"answer\": \"London\",\n",
        "        \"reference\": \"Paris\",\n",
        "        \"expected_high\": False\n",
        "    },\n",
        "    # Uncertain but correct (partial credit)\n",
        "    {\n",
        "        \"question\": \"What is the capital of France?\",\n",
        "        \"answer\": \"I think it might be Paris\",\n",
        "        \"reference\": \"Paris\",\n",
        "        \"expected_high\": True  # Partially\n",
        "    },\n",
        "]\n",
        "\n",
        "print(\"ðŸ“Š Evaluating Multiple Test Cases:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "for i, tc in enumerate(test_cases, 1):\n",
        "    result = judge.score_answer(\n",
        "        question=tc[\"question\"],\n",
        "        answer=tc[\"answer\"],\n",
        "        reference=tc[\"reference\"]\n",
        "    )\n",
        "    \n",
        "    # Store result\n",
        "    results.append({\n",
        "        \"question\": tc[\"question\"],\n",
        "        \"answer\": tc[\"answer\"],\n",
        "        \"reference\": tc[\"reference\"],\n",
        "        \"score\": result[\"score\"],\n",
        "        \"rationale\": result[\"rationale\"],\n",
        "        \"expected_high\": tc[\"expected_high\"]\n",
        "    })\n",
        "    \n",
        "    # Display\n",
        "    status = \"âœ“\" if (result[\"score\"] >= 0.7) == tc[\"expected_high\"] else \"âœ—\"\n",
        "    print(f\"\\n[{status}] Test {i}:\")\n",
        "    print(f\"    Q: {tc['question']}\")\n",
        "    print(f\"    A: {tc['answer']}\")\n",
        "    print(f\"    R: {tc['reference']}\")\n",
        "    print(f\"    Score: {result['score']:.2f}\")\n",
        "    print(f\"    Rationale: {result['rationale'][:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ˆ Step 7: Compare with Exact Match Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import exact match metric from Week 5\n",
        "from src.benchmark_engine.engine import exact_match_metric\n",
        "\n",
        "print(\"ðŸ“Š Comparison: LLM-as-Judge vs Exact Match\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Answer':<30} {'Reference':<15} {'Exact':<8} {'LLM':<8} {'Diff':<8}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for r in results:\n",
        "    exact_score = exact_match_metric(r[\"answer\"], r[\"reference\"])\n",
        "    llm_score = r[\"score\"]\n",
        "    diff = llm_score - exact_score\n",
        "    \n",
        "    # Highlight disagreements\n",
        "    marker = \"âš ï¸\" if abs(diff) > 0.5 else \"  \"\n",
        "    \n",
        "    print(f\"{r['answer'][:28]:<30} {r['reference']:<15} {exact_score:<8.2f} {llm_score:<8.2f} {diff:+.2f} {marker}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ” Step 8: Analyze Disagreements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nðŸ” Analysis of Metric Disagreements:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "disagreements = []\n",
        "for r in results:\n",
        "    exact_score = exact_match_metric(r[\"answer\"], r[\"reference\"])\n",
        "    llm_score = r[\"score\"]\n",
        "    \n",
        "    if abs(llm_score - exact_score) > 0.3:\n",
        "        disagreements.append({\n",
        "            \"answer\": r[\"answer\"],\n",
        "            \"reference\": r[\"reference\"],\n",
        "            \"exact\": exact_score,\n",
        "            \"llm\": llm_score,\n",
        "            \"rationale\": r[\"rationale\"]\n",
        "        })\n",
        "\n",
        "if disagreements:\n",
        "    print(f\"\\nFound {len(disagreements)} significant disagreements:\\n\")\n",
        "    for i, d in enumerate(disagreements, 1):\n",
        "        print(f\"  {i}. Answer: \\\"{d['answer']}\\\"\")\n",
        "        print(f\"     Reference: \\\"{d['reference']}\\\"\")\n",
        "        print(f\"     Exact Match: {d['exact']:.2f}, LLM Judge: {d['llm']:.2f}\")\n",
        "        print(f\"     Rationale: {d['rationale'][:100]}...\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"No significant disagreements found.\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Key Insight:\")\n",
        "print(\"   LLM-as-Judge can recognize semantically correct answers that\")\n",
        "print(\"   exact match would fail. This is especially valuable when:\")\n",
        "print(\"   - Answers are paraphrased but correct\")\n",
        "print(\"   - Answers include helpful context\")\n",
        "print(\"   - The reference is a short form of a longer valid answer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”§ Step 9: Using LLMJudge with the Benchmark Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.benchmark_engine import run_benchmark\n",
        "\n",
        "# Create a simple mock model\n",
        "def mock_model(prompt: str) -> str:\n",
        "    \"\"\"A mock model that returns somewhat elaborated answers.\"\"\"\n",
        "    answers = {\n",
        "        \"capital of france\": \"The capital is Paris\",\n",
        "        \"2+2\": \"The sum is 4\",\n",
        "        \"largest planet\": \"Jupiter\",\n",
        "        \"formula for water\": \"Water is H2O\",\n",
        "    }\n",
        "    prompt_lower = prompt.lower()\n",
        "    for key, answer in answers.items():\n",
        "        if key in prompt_lower:\n",
        "            return answer\n",
        "    return \"I don't know\"\n",
        "\n",
        "\n",
        "# Dataset\n",
        "dataset = [\n",
        "    (\"What is the capital of France?\", \"Paris\"),\n",
        "    (\"What is 2+2?\", \"4\"),\n",
        "    (\"What is the largest planet?\", \"Jupiter\"),\n",
        "    (\"What is the chemical formula for water?\", \"H2O\"),\n",
        "]\n",
        "\n",
        "# Run with exact match\n",
        "print(\"ðŸ“Š Benchmark Results: Exact Match vs LLM-as-Judge\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Exact match benchmark\n",
        "exact_results = run_benchmark(\n",
        "    model_fn=mock_model,\n",
        "    dataset=iter(dataset),\n",
        "    metric_fn=exact_match_metric\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“ Exact Match Results:\")\n",
        "print(f\"   Mean Score: {exact_results['mean_score']:.2%}\")\n",
        "\n",
        "# LLM-as-Judge evaluation (manual loop since metric needs question context)\n",
        "llm_scores = []\n",
        "print(f\"\\nðŸ¤– LLM-as-Judge Results:\")\n",
        "for question, reference in dataset:\n",
        "    answer = mock_model(question)\n",
        "    result = judge.score_answer(question, answer, reference)\n",
        "    llm_scores.append(result[\"score\"])\n",
        "    print(f\"   {question[:30]}... -> {result['score']:.2f}\")\n",
        "\n",
        "llm_mean = sum(llm_scores) / len(llm_scores)\n",
        "print(f\"\\n   Mean Score: {llm_mean:.2%}\")\n",
        "\n",
        "print(f\"\\nðŸ“ˆ Comparison:\")\n",
        "print(f\"   Exact Match: {exact_results['mean_score']:.2%}\")\n",
        "print(f\"   LLM-as-Judge: {llm_mean:.2%}\")\n",
        "print(f\"   Difference: {(llm_mean - exact_results['mean_score'])*100:+.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“‹ Step 10: View the System Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ðŸ“‹ Full System Prompt Used by LLMJudge:\")\n",
        "print(\"=\" * 80)\n",
        "print(JUDGE_SYSTEM_PROMPT)\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ“ Mini-Project: Create Your Own Evaluation Pipeline\n",
        "\n",
        "### Task\n",
        "\n",
        "Create a dataset of 10 QA pairs and compare exact match with LLM-as-Judge.\n",
        "\n",
        "### Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your custom evaluation dataset\n",
        "my_dataset = [\n",
        "    # Add 10 QA pairs\n",
        "    # (\"Question 1?\", \"Reference 1\"),\n",
        "    # (\"Question 2?\", \"Reference 2\"),\n",
        "    # ...\n",
        "]\n",
        "\n",
        "# Your model function (or use mock_model)\n",
        "def my_model(prompt: str) -> str:\n",
        "    \"\"\"Your model implementation here.\"\"\"\n",
        "    pass\n",
        "\n",
        "# Evaluate with both metrics\n",
        "# for question, reference in my_dataset:\n",
        "#     answer = my_model(question)\n",
        "#     exact = exact_match_metric(answer, reference)\n",
        "#     llm_result = judge.score_answer(question, answer, reference)\n",
        "#     print(f\"Q: {question}\")\n",
        "#     print(f\"A: {answer}\")\n",
        "#     print(f\"Exact: {exact:.2f}, LLM: {llm_result['score']:.2f}\")\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ¤” Paul-Elder Critical Thinking Questions\n",
        "\n",
        "Reflect on these questions:\n",
        "\n",
        "### Question 1: EVIDENCE\n",
        "**If LLM-as-Judge gives a high score to an incorrect answer, what might explain this?**\n",
        "*Consider: Model hallucinations, prompt engineering, domain expertise.*\n",
        "\n",
        "### Question 2: ASSUMPTIONS\n",
        "**What assumptions are we making when using GPT-4 to judge GPT-3.5 outputs?**\n",
        "*Consider: Model biases, shared training data, capability differences.*\n",
        "\n",
        "### Question 3: IMPLICATIONS\n",
        "**If an organization uses LLM-as-Judge as the only evaluation metric, what could go wrong?**\n",
        "*Consider: Adversarial inputs, edge cases, accountability.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 7, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand what LLM-as-Judge is and when to use it\n",
        "- [ ] I can use the `LLMJudge` class to evaluate answers\n",
        "- [ ] I understand the three evaluation criteria (correctness, coherence, helpfulness)\n",
        "- [ ] I can compare LLM-as-Judge with exact match metrics\n",
        "- [ ] I understand potential failure modes of LLM-as-Judge\n",
        "- [ ] I know how to customize the system prompt for different tasks\n",
        "\n",
        "---\n",
        "\n",
        "**Week 6 Complete!** ðŸŽ‰\n",
        "\n",
        "**Next:** *Week 7 â€” Semantic Similarity Metrics (Embeddings, Cosine Similarity)*"
      ]
    }
  ]
}
