{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 5 ‚Äî Building a Generic Benchmark Engine\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Use the generic benchmark engine from `src/benchmark_engine`\n",
        "2. Create custom model functions that follow the callable interface\n",
        "3. Build synthetic datasets for testing\n",
        "4. Implement and compare different metric functions\n",
        "5. Analyze benchmark results including timing and throughput"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† The Generic Benchmark Engine\n",
        "\n",
        "### Why Generic?\n",
        "\n",
        "| Before (Ad-Hoc) | After (Generic Engine) |\n",
        "|-----------------|------------------------|\n",
        "| Different code for each benchmark | One `run_benchmark` function |\n",
        "| Inconsistent timing measurement | Built-in timing for all |\n",
        "| Copy-paste evaluation loops | Reusable, tested engine |\n",
        "| Tightly coupled to model type | Works with any callable |\n",
        "\n",
        "### Core Components\n",
        "\n",
        "```python\n",
        "run_benchmark(\n",
        "    model_fn,    # (str) -> str\n",
        "    dataset,     # Iterator[(str, str)]\n",
        "    metric_fn,   # (str, str) -> float\n",
        "    batch_size=1\n",
        ") -> Dict[str, Any]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import time\n",
        "from typing import Callable, Iterator, Tuple, Dict, Any, List\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "# In a local environment, you may need to adjust this path\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Step 2: Import the Benchmark Engine\n",
        "\n",
        "The benchmark engine is located in `src/benchmark_engine/engine.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the benchmark engine\n",
        "from src.benchmark_engine.engine import (\n",
        "    run_benchmark,\n",
        "    exact_match_metric,\n",
        "    contains_metric\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Benchmark engine imported successfully!\")\n",
        "print(f\"   - run_benchmark: {run_benchmark.__doc__.split(chr(10))[1].strip()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§ñ Step 3: Create a Model Function\n",
        "\n",
        "A model function must follow this interface:\n",
        "- **Input:** A string (the prompt)\n",
        "- **Output:** A string (the model's response)\n",
        "\n",
        "Let's create a simple mock model for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mock_model(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    A simple mock model that returns predefined answers.\n",
        "    \n",
        "    This simulates an LLM by matching keywords in the prompt\n",
        "    and returning corresponding answers.\n",
        "    \n",
        "    Args:\n",
        "        prompt: The input question or prompt\n",
        "    \n",
        "    Returns:\n",
        "        The model's answer\n",
        "    \"\"\"\n",
        "    # Define a knowledge base of answers\n",
        "    answers = {\n",
        "        \"capital of france\": \"Paris\",\n",
        "        \"2+2\": \"4\",\n",
        "        \"largest planet\": \"Jupiter\",\n",
        "        \"formula for water\": \"H2O\",\n",
        "        \"speed of light\": \"299792458 m/s\",\n",
        "    }\n",
        "    \n",
        "    # Search for matching keywords\n",
        "    prompt_lower = prompt.lower()\n",
        "    for key, answer in answers.items():\n",
        "        if key in prompt_lower:\n",
        "            return answer\n",
        "    \n",
        "    return \"I don't know\"\n",
        "\n",
        "\n",
        "# Test the mock model\n",
        "print(\"üß™ Testing mock model:\")\n",
        "print(f\"   Q: What is the capital of France?\")\n",
        "print(f\"   A: {mock_model('What is the capital of France?')}\")\n",
        "print(f\"   Q: What is 2+2?\")\n",
        "print(f\"   A: {mock_model('What is 2+2?')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 4: Create a Synthetic Dataset\n",
        "\n",
        "A dataset is an iterator of `(input, reference)` tuples:\n",
        "- **input:** The prompt to send to the model\n",
        "- **reference:** The expected/correct answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create 5 synthetic QA pairs\n",
        "synthetic_dataset = [\n",
        "    (\"What is the capital of France?\", \"Paris\"),\n",
        "    (\"What is 2+2?\", \"4\"),\n",
        "    (\"What is the largest planet in our solar system?\", \"Jupiter\"),\n",
        "    (\"What is the chemical formula for water?\", \"H2O\"),\n",
        "    (\"What is the speed of light?\", \"299792458 m/s\"),\n",
        "]\n",
        "\n",
        "print(f\"üìù Created dataset with {len(synthetic_dataset)} examples:\")\n",
        "print(\"=\" * 60)\n",
        "for i, (question, answer) in enumerate(synthetic_dataset, 1):\n",
        "    print(f\"   Q{i}: {question}\")\n",
        "    print(f\"   A{i}: {answer}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéØ Step 5: Understand the Metric Functions\n",
        "\n",
        "The engine comes with two built-in metrics:\n",
        "\n",
        "1. **exact_match_metric:** Returns 1.0 if output exactly matches reference (case-insensitive)\n",
        "2. **contains_metric:** Returns 1.0 if reference is found anywhere in output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the metric functions\n",
        "print(\"üìê Testing Metric Functions:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_cases = [\n",
        "    (\"Paris\", \"Paris\"),           # Exact match\n",
        "    (\"paris\", \"Paris\"),           # Case insensitive\n",
        "    (\"  Paris  \", \"Paris\"),       # Whitespace handling\n",
        "    (\"The answer is Paris\", \"Paris\"),  # Contains\n",
        "    (\"London\", \"Paris\"),          # No match\n",
        "]\n",
        "\n",
        "print(f\"{'Output':<25} {'Reference':<15} {'Exact':<8} {'Contains':<8}\")\n",
        "print(\"-\" * 60)\n",
        "for output, reference in test_cases:\n",
        "    exact = exact_match_metric(output, reference)\n",
        "    contains = contains_metric(output, reference)\n",
        "    print(f\"{output:<25} {reference:<15} {exact:<8.1f} {contains:<8.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üöÄ Step 6: Run the Benchmark!\n",
        "\n",
        "Now let's put it all together and run the benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the benchmark\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üöÄ Running Benchmark on 5 Synthetic QA Pairs\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = run_benchmark(\n",
        "    model_fn=mock_model,\n",
        "    dataset=iter(synthetic_dataset),  # Convert list to iterator\n",
        "    metric_fn=exact_match_metric,\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Benchmark complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 7: Analyze the Results\n",
        "\n",
        "The engine returns a comprehensive result dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "print(\"\\nüìä Results Summary:\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"   Total examples:        {results['total_examples']}\")\n",
        "print(f\"   Mean score (accuracy): {results['mean_score']:.2%}\")\n",
        "print(f\"   Total time:            {results['total_time_seconds']:.4f} seconds\")\n",
        "print(f\"   Throughput:            {results['examples_per_second']:.2f} examples/second\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display detailed per-example results\n",
        "print(\"\\nüìã Detailed Results:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, result in enumerate(results['results'], 1):\n",
        "    status = \"‚úì\" if result['score'] == 1.0 else \"‚úó\"\n",
        "    print(f\"\\n[{status}] Example {i}:\")\n",
        "    print(f\"    Input:    {result['input']}\")\n",
        "    print(f\"    Expected: {result['reference']}\")\n",
        "    print(f\"    Got:      {result['model_output']}\")\n",
        "    print(f\"    Score:    {result['score']:.2f}\")\n",
        "    print(f\"    Time:     {result['inference_time_seconds']*1000:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 8: Try Different Metrics\n",
        "\n",
        "Let's see how results differ with the `contains_metric`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a \"verbose\" model that adds extra text\n",
        "def verbose_model(prompt: str) -> str:\n",
        "    \"\"\"A model that gives verbose answers.\"\"\"\n",
        "    answers = {\n",
        "        \"capital of france\": \"The capital of France is Paris, a beautiful city.\",\n",
        "        \"2+2\": \"The answer to 2+2 is 4, of course!\",\n",
        "        \"largest planet\": \"Jupiter is the largest planet in our solar system.\",\n",
        "        \"formula for water\": \"The chemical formula for water is H2O.\",\n",
        "        \"speed of light\": \"The speed of light is approximately 299792458 m/s.\",\n",
        "    }\n",
        "    prompt_lower = prompt.lower()\n",
        "    for key, answer in answers.items():\n",
        "        if key in prompt_lower:\n",
        "            return answer\n",
        "    return \"I don't know the answer to that question.\"\n",
        "\n",
        "\n",
        "print(\"üîç Comparing metrics with verbose model:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run with exact_match\n",
        "exact_results = run_benchmark(\n",
        "    model_fn=verbose_model,\n",
        "    dataset=iter(synthetic_dataset),\n",
        "    metric_fn=exact_match_metric\n",
        ")\n",
        "\n",
        "# Run with contains_metric\n",
        "contains_results = run_benchmark(\n",
        "    model_fn=verbose_model,\n",
        "    dataset=iter(synthetic_dataset),\n",
        "    metric_fn=contains_metric\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Comparison:\")\n",
        "print(f\"   Exact Match Accuracy:    {exact_results['mean_score']:.2%}\")\n",
        "print(f\"   Contains Match Accuracy: {contains_results['mean_score']:.2%}\")\n",
        "print(f\"\\nüí° Insight: The verbose model fails exact match but passes contains!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 9: Create Your Own Custom Metric\n",
        "\n",
        "Let's implement a **partial match metric** that gives partial credit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def partial_match_metric(output: str, reference: str) -> float:\n",
        "    \"\"\"\n",
        "    Compute partial match score using word overlap.\n",
        "    \n",
        "    Returns the proportion of reference words found in output.\n",
        "    This gives partial credit for partially correct answers.\n",
        "    \n",
        "    Args:\n",
        "        output: Model generated text\n",
        "        reference: Expected/correct answer\n",
        "    \n",
        "    Returns:\n",
        "        Score between 0.0 and 1.0\n",
        "    \"\"\"\n",
        "    output_words = set(output.strip().lower().split())\n",
        "    reference_words = set(reference.strip().lower().split())\n",
        "    \n",
        "    if not reference_words:\n",
        "        return 1.0  # Empty reference is always matched\n",
        "    \n",
        "    overlap = output_words & reference_words\n",
        "    return len(overlap) / len(reference_words)\n",
        "\n",
        "\n",
        "# Test the partial match metric\n",
        "print(\"üß™ Testing Partial Match Metric:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_cases = [\n",
        "    (\"Paris\", \"Paris\"),                    # Perfect match\n",
        "    (\"The city is Paris\", \"Paris\"),       # Contains\n",
        "    (\"New York City\", \"New York\"),        # Partial (2/2)\n",
        "    (\"I think New is the answer\", \"New York\"),  # Partial (1/2)\n",
        "    (\"London\", \"Paris\"),                  # No match\n",
        "]\n",
        "\n",
        "print(f\"{'Output':<30} {'Reference':<15} {'Score':<8}\")\n",
        "print(\"-\" * 60)\n",
        "for output, reference in test_cases:\n",
        "    score = partial_match_metric(output, reference)\n",
        "    print(f\"{output:<30} {reference:<15} {score:<8.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run benchmark with custom metric\n",
        "print(\"\\nüöÄ Running Benchmark with Partial Match Metric:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "partial_results = run_benchmark(\n",
        "    model_fn=verbose_model,\n",
        "    dataset=iter(synthetic_dataset),\n",
        "    metric_fn=partial_match_metric\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Results with Partial Match:\")\n",
        "print(f\"   Mean score: {partial_results['mean_score']:.2%}\")\n",
        "print(f\"\\nüìã Per-example scores:\")\n",
        "for i, result in enumerate(partial_results['results'], 1):\n",
        "    print(f\"   Q{i}: {result['score']:.2f} - {result['input'][:40]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 10: Integration with Real Models\n",
        "\n",
        "The benchmark engine works with any model. Here's how you would integrate with:\n",
        "\n",
        "### ONNX Runtime\n",
        "\n",
        "```python\n",
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "session = ort.InferenceSession(\"model.onnx\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "def onnx_model(prompt: str) -> str:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
        "    outputs = session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "    # Decode and return\n",
        "    return tokenizer.decode(outputs[0][0])\n",
        "\n",
        "results = run_benchmark(onnx_model, dataset, exact_match_metric)\n",
        "```\n",
        "\n",
        "### Hugging Face Transformers\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "def hf_model(prompt: str) -> str:\n",
        "    result = generator(prompt, max_length=50)\n",
        "    return result[0][\"generated_text\"]\n",
        "\n",
        "results = run_benchmark(hf_model, dataset, contains_metric)\n",
        "```\n",
        "\n",
        "### OpenAI API\n",
        "\n",
        "```python\n",
        "import openai\n",
        "\n",
        "def openai_model(prompt: str) -> str:\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "results = run_benchmark(openai_model, dataset, exact_match_metric)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìà Step 11: Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simple text-based visualization\n",
        "print(\"\\nüìä Score Distribution:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, result in enumerate(results['results'], 1):\n",
        "    score = result['score']\n",
        "    bar = \"‚ñà\" * int(score * 20) + \"‚ñë\" * (20 - int(score * 20))\n",
        "    status = \"‚úì\" if score == 1.0 else \"‚úó\"\n",
        "    print(f\"Q{i} [{status}] {bar} {score:.0%}\")\n",
        "\n",
        "print(\"\\nüìä Inference Time Distribution:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "max_time = max(r['inference_time_seconds'] for r in results['results'])\n",
        "for i, result in enumerate(results['results'], 1):\n",
        "    time_ms = result['inference_time_seconds'] * 1000\n",
        "    ratio = result['inference_time_seconds'] / max_time if max_time > 0 else 0\n",
        "    bar = \"‚ñà\" * int(ratio * 20) + \"‚ñë\" * (20 - int(ratio * 20))\n",
        "    print(f\"Q{i}     {bar} {time_ms:.2f}ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Mini-Project: Build Your Own Benchmark Suite\n",
        "\n",
        "### Task\n",
        "\n",
        "Create a benchmark suite with:\n",
        "1. At least 10 QA pairs\n",
        "2. A custom metric function\n",
        "3. Compare results across different metrics\n",
        "\n",
        "### Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your custom benchmark suite\n",
        "my_benchmark = [\n",
        "    # Add your QA pairs here\n",
        "    (\"What is the capital of Germany?\", \"Berlin\"),\n",
        "    (\"What is 10 * 5?\", \"50\"),\n",
        "    (\"What color is the sky?\", \"blue\"),\n",
        "    # ... add more\n",
        "]\n",
        "\n",
        "# Your custom model (extend the mock_model)\n",
        "def my_model(prompt: str) -> str:\n",
        "    # Implement your model here\n",
        "    pass\n",
        "\n",
        "# Your custom metric\n",
        "def my_metric(output: str, reference: str) -> float:\n",
        "    # Implement your metric here\n",
        "    pass\n",
        "\n",
        "# Run and analyze\n",
        "# results = run_benchmark(my_model, iter(my_benchmark), my_metric)\n",
        "# print(f\"My Benchmark Score: {results['mean_score']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 6, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I can import and use `run_benchmark` from the engine module\n",
        "- [ ] I understand the callable interface for model functions\n",
        "- [ ] I can create synthetic datasets as iterators of tuples\n",
        "- [ ] I can use `exact_match_metric` and `contains_metric`\n",
        "- [ ] I can implement custom metric functions\n",
        "- [ ] I can interpret the results dictionary\n",
        "- [ ] I understand when to use different metrics\n",
        "\n",
        "---\n",
        "\n",
        "**Week 5 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 6 ‚Äî Automated Evaluation Pipelines (LLM-as-Judge)*"
      ]
    }
  ]
}
