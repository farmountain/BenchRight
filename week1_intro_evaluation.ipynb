{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 ‚Äî Foundations of LLM Evaluation & First Principles\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand *what LLM evaluation is* and why it is the \"currency\" of model quality\n",
        "2. Learn the *4 pillars of LLM evaluation*: Quantitative, Qualitative, Safety, Performance\n",
        "3. Run your first ONNX LLM benchmark in Google Colab\n",
        "4. Measure latency on multiple prompts\n",
        "5. Display results in a structured DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† Section 1: What is LLM Evaluation? (Feynman Technique)\n",
        "\n",
        "### Simple Explanation\n",
        "\n",
        "Imagine you're buying a car. You'd want to know:\n",
        "- **How fast does it go?** ‚Üí This is like measuring *throughput* and *latency*\n",
        "- **How well does it steer?** ‚Üí This is like checking *correctness* and *reasoning*\n",
        "- **How safe is it?** ‚Üí This is like testing for *hallucinations* and *toxicity*\n",
        "- **How much fuel does it need?** ‚Üí This is like measuring *memory* and *compute cost*\n",
        "\n",
        "**LLM evaluation** is the process of systematically answering one core question:\n",
        "\n",
        "> *\"How good is this model‚Äîobjectively, reproducibly, and safely?\"*\n",
        "\n",
        "If you can't explain your evaluation in simple terms, you don't truly understand it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèõÔ∏è Section 2: The 4 Pillars of LLM Evaluation\n",
        "\n",
        "Every evaluation falls into one of these four categories:\n",
        "\n",
        "| Pillar | Question | Metrics |\n",
        "|--------|----------|----------|\n",
        "| **Quantitative** | What does the model understand? | Perplexity, accuracy, benchmarks (MMLU, HellaSwag) |\n",
        "| **Qualitative** | How does it behave? | Coherence, LLM-as-judge scoring |\n",
        "| **Safety** | Is it safe? | TruthfulQA, toxicity checks, hallucination detection |\n",
        "| **Performance** | Is it usable? | Latency, throughput, memory usage |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Install Dependencies\n",
        "\n",
        "First, we need to install the ONNX Runtime and the transformers library for tokenization.\n",
        "\n",
        "**Why these libraries?**\n",
        "- `onnxruntime`: Runs ONNX models efficiently on CPU/GPU\n",
        "- `transformers`: Provides pre-trained tokenizers that convert text to model inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install onnxruntime transformers pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì¶ Step 2: Load tinyGPT ONNX Model\n",
        "\n",
        "### Feynman Explanation\n",
        "\n",
        "Think of loading a model like opening a cookbook:\n",
        "1. The **ONNX file** is the cookbook (contains all the recipes/weights)\n",
        "2. The **InferenceSession** is you reading and following the recipes\n",
        "3. The **tokenizer** is like a translator that converts your ingredients (text) into a format the cookbook understands (numbers)\n",
        "\n",
        "We use ONNX because it's a universal format that runs the same model on any platform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "\n",
        "# Path to the ONNX model (you should upload your model to /tmp/ in Colab)\n",
        "model_path = \"/tmp/tinygpt.onnx\"\n",
        "\n",
        "# Load the tokenizer (GPT-2 tokenizer works with tinyGPT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create an inference session with CPU provider\n",
        "session = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"Input names: {[inp.name for inp in session.get_inputs()]}\")\n",
        "print(f\"Output names: {[out.name for out in session.get_outputs()]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üöÄ Step 3: Run First Inference\n",
        "\n",
        "### Feynman Explanation\n",
        "\n",
        "Running inference is like asking the model a question:\n",
        "1. **Input**: Your text prompt (e.g., \"Explain artificial intelligence.\")\n",
        "2. **Tokenization**: Convert text to numbers the model understands\n",
        "3. **Forward pass**: The model processes the numbers through its neural network\n",
        "4. **Output**: The model returns probabilities for each possible next token\n",
        "5. **Decode**: We convert the most likely tokens back to readable text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a test prompt\n",
        "prompt = \"Explain artificial intelligence.\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"np\")\n",
        "\n",
        "# Run inference\n",
        "outputs = session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "\n",
        "# Get the most likely next tokens\n",
        "result_ids = np.argmax(outputs[0], axis=-1)[0]\n",
        "\n",
        "# Decode back to text\n",
        "result_text = tokenizer.decode(result_ids)\n",
        "\n",
        "print(f\"üìù Prompt: {prompt}\")\n",
        "print(f\"ü§ñ Model output: {result_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚è±Ô∏è Step 4: Measure Latency on 3 Prompts\n",
        "\n",
        "### Feynman Explanation\n",
        "\n",
        "**Latency** is like measuring how long it takes a chef to prepare a dish:\n",
        "- Start the timer when you give the order (input)\n",
        "- Stop the timer when the dish arrives (output)\n",
        "- The time in between is the latency\n",
        "\n",
        "Lower latency = faster responses = better user experience.\n",
        "\n",
        "We'll measure latency for 3 different prompts to understand how input affects performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def benchmark(model_session, text, tokenizer_instance):\n",
        "    \"\"\"Measure inference latency for a given text prompt.\"\"\"\n",
        "    t0 = time.time()\n",
        "    inputs = tokenizer_instance(text, return_tensors=\"np\")\n",
        "    _ = model_session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "    t1 = time.time()\n",
        "    return (t1 - t0) * 1000  # Convert to milliseconds\n",
        "\n",
        "# Define 3 test prompts\n",
        "prompts = [\n",
        "    \"Explain machine learning.\",\n",
        "    \"Summarize the Singapore financial system.\",\n",
        "    \"Describe a robot to a child.\"\n",
        "]\n",
        "\n",
        "# Run benchmark for each prompt\n",
        "results = []\n",
        "for prompt in prompts:\n",
        "    latency = benchmark(session, prompt, tokenizer)\n",
        "    # Get output for display\n",
        "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
        "    outputs = session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "    result_ids = np.argmax(outputs[0], axis=-1)[0]\n",
        "    output_text = tokenizer.decode(result_ids)\n",
        "    results.append({\n",
        "        \"Prompt\": prompt,\n",
        "        \"Output\": output_text[:100] + \"...\" if len(output_text) > 100 else output_text,\n",
        "        \"Latency (ms)\": round(latency, 2)\n",
        "    })\n",
        "    print(f\"‚úÖ Processed: '{prompt[:30]}...' - Latency: {latency:.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 5: Display Results as DataFrame\n",
        "\n",
        "### Feynman Explanation\n",
        "\n",
        "A **DataFrame** is like a spreadsheet:\n",
        "- Rows represent each experiment (prompt)\n",
        "- Columns represent measurements (prompt text, output, latency)\n",
        "\n",
        "Using DataFrames makes it easy to:\n",
        "- Compare results side-by-side\n",
        "- Export to CSV/Excel for reports\n",
        "- Calculate statistics (mean, std, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create DataFrame from results\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Display the results table\n",
        "print(\"\\nüìä Benchmark Results:\")\n",
        "print(\"=\" * 80)\n",
        "display(df)\n",
        "\n",
        "# Calculate summary statistics\n",
        "print(\"\\nüìà Summary Statistics:\")\n",
        "print(f\"  Mean Latency: {df['Latency (ms)'].mean():.2f} ms\")\n",
        "print(f\"  Min Latency:  {df['Latency (ms)'].min():.2f} ms\")\n",
        "print(f\"  Max Latency:  {df['Latency (ms)'].max():.2f} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§î Paul-Elder Critical Thinking Questions\n",
        "\n",
        "Reflect on these questions using the Paul-Elder framework:\n",
        "\n",
        "### Question 1: CLAIM\n",
        "**What quality does measuring latency alone tell us about the model?**\n",
        "*Consider: Does low latency mean the model is \"good\"? What's missing?*\n",
        "\n",
        "### Question 2: EVIDENCE\n",
        "**If latency varies between prompts, what evidence would explain the difference?**\n",
        "*Consider: Input length, tokenization, model architecture.*\n",
        "\n",
        "### Question 3: REASONING\n",
        "**Why do we use ONNX Runtime instead of running the model natively in PyTorch?**\n",
        "*Consider: Portability, optimization, deployment scenarios.*\n",
        "\n",
        "### Question 4: ASSUMPTIONS\n",
        "**What assumptions are we making when we benchmark on only 3 prompts?**\n",
        "*Consider: Statistical significance, prompt diversity, real-world usage.*\n",
        "\n",
        "### Question 5: IMPLICATIONS\n",
        "**If we deployed this model to production based solely on these latency results, what could go wrong?**\n",
        "*Consider: Quality, safety, edge cases.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîÑ Inversion Thinking Exercise\n",
        "\n",
        "**Instead of asking:** \"Is my model good?\"\n",
        "\n",
        "**Ask:** \"How can my model fail?\"\n",
        "\n",
        "### Exercise\n",
        "\n",
        "List at least 5 ways this evaluation approach could fail or give misleading results:\n",
        "\n",
        "1. ________________________________________________\n",
        "2. ________________________________________________\n",
        "3. ________________________________________________\n",
        "4. ________________________________________________\n",
        "5. ________________________________________________\n",
        "\n",
        "**Hint:** Think about:\n",
        "- Hallucinations in output\n",
        "- Cold start vs. warm start latency\n",
        "- CPU vs. GPU differences\n",
        "- Memory constraints\n",
        "- Prompt sensitivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Mini-Project\n",
        "\n",
        "### Task\n",
        "\n",
        "Evaluate tinyGPT on the 3 prompts provided and measure latency. Create a comprehensive results table.\n",
        "\n",
        "### Requirements\n",
        "\n",
        "1. Run inference on each of the 3 prompts\n",
        "2. Measure and record latency for each\n",
        "3. Create a results table with columns: Prompt, Output, Latency (ms)\n",
        "4. Write a brief interpretation of your results\n",
        "5. Upload your results to `/examples/week01_results.md`\n",
        "\n",
        "### Submission Format\n",
        "\n",
        "```markdown\n",
        "# Week 1 Mini-Project Results\n",
        "\n",
        "## Results Table\n",
        "| Prompt | Output | Latency (ms) |\n",
        "|--------|--------|---------------|\n",
        "| ... | ... | ... |\n",
        "\n",
        "## Interpretation\n",
        "[Your analysis here]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 2, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I can explain what LLM evaluation is in simple terms\n",
        "- [ ] I know the 4 pillars of evaluation (Quantitative, Qualitative, Safety, Performance)\n",
        "- [ ] I successfully ran an ONNX model in Colab\n",
        "- [ ] I measured and understand latency metrics\n",
        "- [ ] I can apply Feynman, Paul-Elder, and Inversion thinking to evaluation\n",
        "- [ ] I completed the mini-project\n",
        "\n",
        "---\n",
        "\n",
        "**Week 1 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 2 ‚Äî Tokenization & ONNX Runtime Internals*"
      ]
    }
  ]
}
