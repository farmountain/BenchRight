{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 13 ‚Äî Software Engineering Evaluation\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand how to evaluate LLM code generation and debugging capabilities\n",
        "2. Use a synthetic bug-fix dataset for evaluation\n",
        "3. Implement pass/fail metrics using unit tests\n",
        "4. Automatically score model outputs using BenchRight's engine\n",
        "5. Analyze which types of bugs are hardest for models to fix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† Why Code Evaluation is Different\n",
        "\n",
        "### The Challenge\n",
        "\n",
        "Unlike natural language tasks, code has a **strict correctness criterion**:\n",
        "\n",
        "| Aspect | Natural Language | Code |\n",
        "|--------|------------------|------|\n",
        "| Correctness | Multiple phrasings OK | Must pass all tests |\n",
        "| Evaluation | Human judgment needed | Automated testing possible |\n",
        "| Errors | Graceful degradation | Syntax error = complete failure |\n",
        "\n",
        "### What We Evaluate\n",
        "\n",
        "1. **Functional Correctness:** Does the code pass unit tests?\n",
        "2. **Bug Detection:** Can the model identify what's wrong?\n",
        "3. **Fix Quality:** Does the fix resolve the issue without side effects?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import json\n",
        "import subprocess\n",
        "import tempfile\n",
        "import os\n",
        "from typing import Dict, List, Any, Callable, Iterator, Tuple\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# For data display\n",
        "try:\n",
        "    from IPython.display import display, HTML\n",
        "except ImportError:\n",
        "    display = print\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üêõ Step 2: Define the Synthetic Bug-Fix Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a comprehensive synthetic bug-fix dataset\n",
        "# Each entry contains: description, buggy code, expected fix, and unit tests\n",
        "\n",
        "BUG_FIX_DATASET = [\n",
        "    {\n",
        "        \"name\": \"Off-by-One Error\",\n",
        "        \"description\": \"Calculate the sum of all numbers from 1 to n (inclusive).\",\n",
        "        \"buggy_code\": '''def sum_to_n(n):\n",
        "    total = 0\n",
        "    for i in range(n):  # Bug: should be range(1, n+1)\n",
        "        total += i\n",
        "    return total''',\n",
        "        \"expected_fix\": '''def sum_to_n(n):\n",
        "    total = 0\n",
        "    for i in range(1, n + 1):\n",
        "        total += i\n",
        "    return total''',\n",
        "        \"test_code\": '''def test_sum_to_n():\n",
        "    assert sum_to_n(5) == 15, \"sum_to_n(5) should be 15\"\n",
        "    assert sum_to_n(1) == 1, \"sum_to_n(1) should be 1\"\n",
        "    assert sum_to_n(10) == 55, \"sum_to_n(10) should be 55\"''',\n",
        "        \"bug_type\": \"off-by-one\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Wrong Operator\",\n",
        "        \"description\": \"Check if a number is even.\",\n",
        "        \"buggy_code\": '''def is_even(n):\n",
        "    return n % 2 == 1  # Bug: should check == 0, not == 1''',\n",
        "        \"expected_fix\": '''def is_even(n):\n",
        "    return n % 2 == 0''',\n",
        "        \"test_code\": '''def test_is_even():\n",
        "    assert is_even(2) == True, \"2 should be even\"\n",
        "    assert is_even(3) == False, \"3 should be odd\"\n",
        "    assert is_even(0) == True, \"0 should be even\"\n",
        "    assert is_even(-4) == True, \"-4 should be even\"''',\n",
        "        \"bug_type\": \"wrong-operator\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Missing Return Statement\",\n",
        "        \"description\": \"Find the maximum value in a list.\",\n",
        "        \"buggy_code\": '''def find_max(lst):\n",
        "    if not lst:\n",
        "        return None\n",
        "    max_val = lst[0]\n",
        "    for item in lst:\n",
        "        if item > max_val:\n",
        "            max_val = item\n",
        "    # Bug: missing return statement''',\n",
        "        \"expected_fix\": '''def find_max(lst):\n",
        "    if not lst:\n",
        "        return None\n",
        "    max_val = lst[0]\n",
        "    for item in lst:\n",
        "        if item > max_val:\n",
        "            max_val = item\n",
        "    return max_val''',\n",
        "        \"test_code\": '''def test_find_max():\n",
        "    assert find_max([1, 5, 3, 9, 2]) == 9, \"max of [1,5,3,9,2] should be 9\"\n",
        "    assert find_max([42]) == 42, \"max of [42] should be 42\"\n",
        "    assert find_max([]) == None, \"max of [] should be None\"\n",
        "    assert find_max([-1, -5, -2]) == -1, \"max of negative list\"''',\n",
        "        \"bug_type\": \"missing-return\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Wrong Comparison Direction\",\n",
        "        \"description\": \"Return a list of numbers greater than a threshold.\",\n",
        "        \"buggy_code\": '''def filter_greater_than(numbers, threshold):\n",
        "    result = []\n",
        "    for n in numbers:\n",
        "        if n < threshold:  # Bug: should be > not <\n",
        "            result.append(n)\n",
        "    return result''',\n",
        "        \"expected_fix\": '''def filter_greater_than(numbers, threshold):\n",
        "    result = []\n",
        "    for n in numbers:\n",
        "        if n > threshold:\n",
        "            result.append(n)\n",
        "    return result''',\n",
        "        \"test_code\": '''def test_filter_greater_than():\n",
        "    assert filter_greater_than([1, 5, 10, 3], 4) == [5, 10]\n",
        "    assert filter_greater_than([1, 2, 3], 10) == []\n",
        "    assert filter_greater_than([], 5) == []''',\n",
        "        \"bug_type\": \"wrong-comparison\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"String Concatenation Order\",\n",
        "        \"description\": \"Reverse a string.\",\n",
        "        \"buggy_code\": '''def reverse_string(s):\n",
        "    result = \"\"\n",
        "    for char in s:\n",
        "        result = result + char  # Bug: should prepend, not append\n",
        "    return result''',\n",
        "        \"expected_fix\": '''def reverse_string(s):\n",
        "    result = \"\"\n",
        "    for char in s:\n",
        "        result = char + result\n",
        "    return result''',\n",
        "        \"test_code\": '''def test_reverse_string():\n",
        "    assert reverse_string(\"hello\") == \"olleh\"\n",
        "    assert reverse_string(\"a\") == \"a\"\n",
        "    assert reverse_string(\"\") == \"\"\n",
        "    assert reverse_string(\"ab\") == \"ba\"''',\n",
        "        \"bug_type\": \"string-order\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìä Defined {len(BUG_FIX_DATASET)} bug-fix test cases:\")\n",
        "for i, tc in enumerate(BUG_FIX_DATASET, 1):\n",
        "    print(f\"   {i}. {tc['name']} (type: {tc['bug_type']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 3: Implement the CodeEvaluator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CodeEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluator for code generation and bug-fix tasks.\n",
        "    \n",
        "    Uses unit tests to determine pass/fail for generated code.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, timeout_seconds: int = 5):\n",
        "        \"\"\"\n",
        "        Initialize the CodeEvaluator.\n",
        "        \n",
        "        Args:\n",
        "            timeout_seconds: Maximum time allowed for test execution\n",
        "        \"\"\"\n",
        "        self.timeout_seconds = timeout_seconds\n",
        "    \n",
        "    def evaluate_code(\n",
        "        self,\n",
        "        generated_code: str,\n",
        "        test_code: str,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate generated code using unit tests.\n",
        "        \n",
        "        Args:\n",
        "            generated_code: The code to evaluate\n",
        "            test_code: Unit test code to verify correctness\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with:\n",
        "            - passed: bool indicating if all tests passed\n",
        "            - error: str with error message if failed\n",
        "            - output: str with test output\n",
        "        \"\"\"\n",
        "        # Combine generated code with tests\n",
        "        full_code = f\"{generated_code}\\n\\n{test_code}\\n\\n\"\n",
        "        full_code += \"if __name__ == '__main__':\\n\"\n",
        "        full_code += \"    import sys\\n\"\n",
        "        full_code += \"    # Run all test functions\\n\"\n",
        "        full_code += \"    test_functions = [name for name in dir() if name.startswith('test_')]\\n\"\n",
        "        full_code += \"    all_passed = True\\n\"\n",
        "        full_code += \"    for test_name in test_functions:\\n\"\n",
        "        full_code += \"        try:\\n\"\n",
        "        full_code += \"            globals()[test_name]()\\n\"\n",
        "        full_code += \"            print(f'‚úì {test_name} passed')\\n\"\n",
        "        full_code += \"        except AssertionError as e:\\n\"\n",
        "        full_code += \"            print(f'‚úó {test_name} failed: {e}')\\n\"\n",
        "        full_code += \"            all_passed = False\\n\"\n",
        "        full_code += \"        except Exception as e:\\n\"\n",
        "        full_code += \"            print(f'‚úó {test_name} error: {e}')\\n\"\n",
        "        full_code += \"            all_passed = False\\n\"\n",
        "        full_code += \"    if all_passed:\\n\"\n",
        "        full_code += \"        print('All tests passed!')\\n\"\n",
        "        full_code += \"    else:\\n\"\n",
        "        full_code += \"        sys.exit(1)\\n\"\n",
        "        \n",
        "        temp_path = None\n",
        "        try:\n",
        "            # Write code to temporary file\n",
        "            with tempfile.NamedTemporaryFile(\n",
        "                mode='w',\n",
        "                suffix='.py',\n",
        "                delete=False\n",
        "            ) as f:\n",
        "                f.write(full_code)\n",
        "                temp_path = f.name\n",
        "            \n",
        "            # Execute the code\n",
        "            result = subprocess.run(\n",
        "                ['python', temp_path],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=self.timeout_seconds\n",
        "            )\n",
        "            \n",
        "            # Check result\n",
        "            if result.returncode == 0:\n",
        "                return {\n",
        "                    \"passed\": True,\n",
        "                    \"error\": None,\n",
        "                    \"output\": result.stdout,\n",
        "                }\n",
        "            else:\n",
        "                return {\n",
        "                    \"passed\": False,\n",
        "                    \"error\": result.stderr or result.stdout,\n",
        "                    \"output\": result.stdout,\n",
        "                }\n",
        "                \n",
        "        except subprocess.TimeoutExpired:\n",
        "            return {\n",
        "                \"passed\": False,\n",
        "                \"error\": f\"Execution timed out after {self.timeout_seconds} seconds\",\n",
        "                \"output\": \"\",\n",
        "            }\n",
        "        except SyntaxError as e:\n",
        "            return {\n",
        "                \"passed\": False,\n",
        "                \"error\": f\"Syntax error: {str(e)}\",\n",
        "                \"output\": \"\",\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"passed\": False,\n",
        "                \"error\": f\"Execution error: {str(e)}\",\n",
        "                \"output\": \"\",\n",
        "            }\n",
        "        finally:\n",
        "            # Clean up temporary file\n",
        "            if temp_path and os.path.exists(temp_path):\n",
        "                os.unlink(temp_path)\n",
        "    \n",
        "    def compute_pass_rate(self, results: List[Dict[str, Any]]) -> float:\n",
        "        \"\"\"\n",
        "        Compute the pass rate across multiple evaluations.\n",
        "        \n",
        "        Args:\n",
        "            results: List of evaluation results\n",
        "            \n",
        "        Returns:\n",
        "            Pass rate as a float between 0.0 and 1.0\n",
        "        \"\"\"\n",
        "        if not results:\n",
        "            return 0.0\n",
        "        passed = sum(1 for r in results if r[\"passed\"])\n",
        "        return passed / len(results)\n",
        "\n",
        "\n",
        "print(\"‚úÖ CodeEvaluator class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèÉ Step 4: Initialize the Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the CodeEvaluator\n",
        "evaluator = CodeEvaluator(timeout_seconds=5)\n",
        "\n",
        "print(\"‚úÖ CodeEvaluator initialized!\")\n",
        "print(f\"   Timeout: {evaluator.timeout_seconds} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úì Step 5: Verify Expected Fixes Pass All Tests\n",
        "\n",
        "First, let's verify that our expected fixes actually pass the unit tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç Verifying Expected Fixes...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "verification_results = []\n",
        "for tc in BUG_FIX_DATASET:\n",
        "    result = evaluator.evaluate_code(\n",
        "        tc[\"expected_fix\"],\n",
        "        tc[\"test_code\"]\n",
        "    )\n",
        "    \n",
        "    verification_results.append({\n",
        "        \"name\": tc[\"name\"],\n",
        "        \"bug_type\": tc[\"bug_type\"],\n",
        "        \"passed\": result[\"passed\"],\n",
        "        \"error\": result[\"error\"],\n",
        "    })\n",
        "    \n",
        "    status = \"‚úÖ PASS\" if result[\"passed\"] else \"‚ùå FAIL\"\n",
        "    print(f\"\\n{status} {tc['name']}\")\n",
        "    if result[\"output\"]:\n",
        "        for line in result[\"output\"].strip().split('\\n'):\n",
        "            print(f\"   {line}\")\n",
        "\n",
        "# Summary\n",
        "pass_rate = evaluator.compute_pass_rate(verification_results)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"üìä Verification Pass Rate: {pass_rate:.0%}\")\n",
        "if pass_rate == 1.0:\n",
        "    print(\"‚úÖ All expected fixes pass their tests!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úó Step 6: Verify Buggy Code Fails Tests\n",
        "\n",
        "Now let's verify that the buggy code actually fails the tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üêõ Verifying Buggy Code Fails...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "buggy_results = []\n",
        "for tc in BUG_FIX_DATASET:\n",
        "    result = evaluator.evaluate_code(\n",
        "        tc[\"buggy_code\"],\n",
        "        tc[\"test_code\"]\n",
        "    )\n",
        "    \n",
        "    buggy_results.append({\n",
        "        \"name\": tc[\"name\"],\n",
        "        \"bug_type\": tc[\"bug_type\"],\n",
        "        \"passed\": result[\"passed\"],\n",
        "    })\n",
        "    \n",
        "    # For buggy code, we WANT it to fail\n",
        "    status = \"‚úÖ Correctly fails\" if not result[\"passed\"] else \"‚ö†Ô∏è Unexpectedly passes\"\n",
        "    print(f\"\\n{status}: {tc['name']}\")\n",
        "    print(f\"   Bug type: {tc['bug_type']}\")\n",
        "\n",
        "# Summary - for buggy code, pass rate should be 0%\n",
        "fail_rate = 1.0 - evaluator.compute_pass_rate(buggy_results)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"üìä Buggy Code Fail Rate: {fail_rate:.0%}\")\n",
        "if fail_rate == 1.0:\n",
        "    print(\"‚úÖ All buggy code correctly fails tests!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§ñ Step 7: Define Mock Model for Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockCodeFixModel:\n",
        "    \"\"\"\n",
        "    Mock model that simulates LLM code fix responses.\n",
        "    \n",
        "    For demonstration, it returns correct fixes for some bugs\n",
        "    and incorrect fixes for others.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, success_rate: float = 0.6):\n",
        "        \"\"\"\n",
        "        Initialize the mock model.\n",
        "        \n",
        "        Args:\n",
        "            success_rate: Probability of returning correct fix\n",
        "        \"\"\"\n",
        "        self.success_rate = success_rate\n",
        "        self.call_count = 0\n",
        "        \n",
        "        # Predefined responses - some correct, some incorrect\n",
        "        self.responses = {\n",
        "            \"sum_to_n\": {\n",
        "                \"correct\": '''def sum_to_n(n):\n",
        "    total = 0\n",
        "    for i in range(1, n + 1):\n",
        "        total += i\n",
        "    return total''',\n",
        "                \"incorrect\": '''def sum_to_n(n):\n",
        "    total = 0\n",
        "    for i in range(n + 1):\n",
        "        total += i\n",
        "    return total''',  # Still wrong - starts at 0\n",
        "            },\n",
        "            \"is_even\": {\n",
        "                \"correct\": '''def is_even(n):\n",
        "    return n % 2 == 0''',\n",
        "                \"incorrect\": '''def is_even(n):\n",
        "    return n / 2 == 0''',  # Wrong - uses division\n",
        "            },\n",
        "            \"find_max\": {\n",
        "                \"correct\": '''def find_max(lst):\n",
        "    if not lst:\n",
        "        return None\n",
        "    max_val = lst[0]\n",
        "    for item in lst:\n",
        "        if item > max_val:\n",
        "            max_val = item\n",
        "    return max_val''',\n",
        "                \"incorrect\": '''def find_max(lst):\n",
        "    if not lst:\n",
        "        return None\n",
        "    max_val = lst[0]\n",
        "    for item in lst:\n",
        "        if item > max_val:\n",
        "            max_val = item''',  # Still missing return\n",
        "            },\n",
        "            \"filter_greater_than\": {\n",
        "                \"correct\": '''def filter_greater_than(numbers, threshold):\n",
        "    result = []\n",
        "    for n in numbers:\n",
        "        if n > threshold:\n",
        "            result.append(n)\n",
        "    return result''',\n",
        "                \"incorrect\": '''def filter_greater_than(numbers, threshold):\n",
        "    result = []\n",
        "    for n in numbers:\n",
        "        if n >= threshold:\n",
        "            result.append(n)\n",
        "    return result''',  # Wrong - uses >= instead of >\n",
        "            },\n",
        "            \"reverse_string\": {\n",
        "                \"correct\": '''def reverse_string(s):\n",
        "    result = \"\"\n",
        "    for char in s:\n",
        "        result = char + result\n",
        "    return result''',\n",
        "                \"incorrect\": '''def reverse_string(s):\n",
        "    return s[::-1]''',  # Correct but different approach - we'll count as correct\n",
        "            },\n",
        "        }\n",
        "    \n",
        "    def generate_fix(self, prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a code fix based on the prompt.\n",
        "        \n",
        "        Args:\n",
        "            prompt: Bug fix prompt containing description and buggy code\n",
        "            \n",
        "        Returns:\n",
        "            Generated code fix\n",
        "        \"\"\"\n",
        "        self.call_count += 1\n",
        "        \n",
        "        # Determine which function we're fixing\n",
        "        for func_name in self.responses:\n",
        "            if func_name in prompt:\n",
        "                # Alternate between correct and incorrect based on call count\n",
        "                # to simulate varying model performance\n",
        "                if self.call_count % 2 == 0:\n",
        "                    return self.responses[func_name][\"correct\"]\n",
        "                else:\n",
        "                    return self.responses[func_name][\"incorrect\"]\n",
        "        \n",
        "        # Default: return a syntax error\n",
        "        return \"def broken_code(\\n    # This won't work\"\n",
        "\n",
        "\n",
        "# Create mock model\n",
        "mock_model = MockCodeFixModel()\n",
        "print(\"‚úÖ Mock code fix model created!\")\n",
        "print(\"   (Simulates varying model performance for demonstration)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìù Step 8: Define Bug-Fix Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BUG_FIX_PROMPT_TEMPLATE = \"\"\"You are a skilled Python programmer. Your task is to fix the bug in the following code.\n",
        "\n",
        "## Function Description\n",
        "{description}\n",
        "\n",
        "## Buggy Code\n",
        "```python\n",
        "{buggy_code}\n",
        "```\n",
        "\n",
        "## Instructions\n",
        "1. Identify the bug in the code above\n",
        "2. Fix the bug while maintaining the same function signature\n",
        "3. Return ONLY the fixed Python code, no explanations\n",
        "\n",
        "## Fixed Code\n",
        "```python\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def create_bug_fix_prompt(description: str, buggy_code: str) -> str:\n",
        "    \"\"\"Create a prompt for bug-fix tasks.\"\"\"\n",
        "    return BUG_FIX_PROMPT_TEMPLATE.format(\n",
        "        description=description,\n",
        "        buggy_code=buggy_code\n",
        "    )\n",
        "\n",
        "\n",
        "# Show an example prompt\n",
        "example_prompt = create_bug_fix_prompt(\n",
        "    BUG_FIX_DATASET[0][\"description\"],\n",
        "    BUG_FIX_DATASET[0][\"buggy_code\"]\n",
        ")\n",
        "print(\"üìù Example Bug-Fix Prompt:\")\n",
        "print(\"=\" * 60)\n",
        "print(example_prompt)\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 9: Run Bug-Fix Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üß™ Running Bug-Fix Evaluation...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "for tc in BUG_FIX_DATASET:\n",
        "    # Create prompt\n",
        "    prompt = create_bug_fix_prompt(\n",
        "        tc[\"description\"],\n",
        "        tc[\"buggy_code\"]\n",
        "    )\n",
        "    \n",
        "    # Get model's fix attempt\n",
        "    generated_fix = mock_model.generate_fix(prompt)\n",
        "    \n",
        "    # Evaluate the fix\n",
        "    result = evaluator.evaluate_code(\n",
        "        generated_fix,\n",
        "        tc[\"test_code\"]\n",
        "    )\n",
        "    \n",
        "    evaluation_results.append({\n",
        "        \"name\": tc[\"name\"],\n",
        "        \"bug_type\": tc[\"bug_type\"],\n",
        "        \"passed\": result[\"passed\"],\n",
        "        \"error\": result[\"error\"],\n",
        "        \"generated_fix\": generated_fix,\n",
        "    })\n",
        "    \n",
        "    status = \"‚úÖ PASS\" if result[\"passed\"] else \"‚ùå FAIL\"\n",
        "    print(f\"\\n{status} {tc['name']}\")\n",
        "    print(f\"   Bug Type: {tc['bug_type']}\")\n",
        "    if not result[\"passed\"] and result[\"error\"]:\n",
        "        # Show first line of error only\n",
        "        error_line = result[\"error\"].strip().split('\\n')[0][:60]\n",
        "        print(f\"   Error: {error_line}...\")\n",
        "\n",
        "# Summary\n",
        "pass_rate = evaluator.compute_pass_rate(evaluation_results)\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(f\"üìä Model Pass Rate: {pass_rate:.0%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 10: Display Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Bug-Fix Evaluation Results Summary\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Test Case':<35} {'Bug Type':<20} {'Result':<10}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for r in evaluation_results:\n",
        "    status = \"‚úÖ Pass\" if r[\"passed\"] else \"‚ùå Fail\"\n",
        "    print(f\"{r['name']:<35} {r['bug_type']:<20} {status:<10}\")\n",
        "\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'TOTAL':<35} {'':<20} {pass_rate:.0%} pass rate\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìà Step 11: Analyze Results by Bug Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìà Results by Bug Type\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Group results by bug type\n",
        "bug_type_results = {}\n",
        "for r in evaluation_results:\n",
        "    bug_type = r[\"bug_type\"]\n",
        "    if bug_type not in bug_type_results:\n",
        "        bug_type_results[bug_type] = {\"passed\": 0, \"total\": 0}\n",
        "    bug_type_results[bug_type][\"total\"] += 1\n",
        "    if r[\"passed\"]:\n",
        "        bug_type_results[bug_type][\"passed\"] += 1\n",
        "\n",
        "print(f\"\\n{'Bug Type':<25} {'Passed':<10} {'Total':<10} {'Rate':<10}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for bug_type, stats in sorted(bug_type_results.items()):\n",
        "    rate = stats[\"passed\"] / stats[\"total\"] if stats[\"total\"] > 0 else 0\n",
        "    status = \"‚úÖ\" if rate == 1.0 else \"‚ùå\" if rate == 0.0 else \"‚ö†Ô∏è\"\n",
        "    print(f\"{status} {bug_type:<23} {stats['passed']:<10} {stats['total']:<10} {rate:.0%}\")\n",
        "\n",
        "print(\"\\nüìã Analysis:\")\n",
        "# Find hardest bug types\n",
        "sorted_types = sorted(bug_type_results.items(), key=lambda x: x[1][\"passed\"]/x[1][\"total\"])\n",
        "print(f\"   Hardest bug type: {sorted_types[0][0]}\")\n",
        "print(f\"   Easiest bug type: {sorted_types[-1][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 12: View Failed Fix Attempts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç Failed Fix Attempts\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "failed_attempts = [r for r in evaluation_results if not r[\"passed\"]]\n",
        "\n",
        "if not failed_attempts:\n",
        "    print(\"\\n‚úÖ No failures - all fixes passed!\")\n",
        "else:\n",
        "    for r in failed_attempts:\n",
        "        print(f\"\\n‚ùå {r['name']} ({r['bug_type']})\")\n",
        "        print(\"-\" * 60)\n",
        "        print(\"Generated Fix (incorrect):\")\n",
        "        print(r[\"generated_fix\"])\n",
        "        if r[\"error\"]:\n",
        "            print(f\"\\nError: {r['error'][:200]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîß Step 13: Integration with BenchRight Engine\n",
        "\n",
        "Here's how to integrate the code evaluator with BenchRight's benchmark engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def code_pass_fail_metric(\n",
        "    generated_code: str,\n",
        "    test_code: str,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Metric function that returns 1.0 if code passes all tests, 0.0 otherwise.\n",
        "    \n",
        "    This can be used with BenchRight's run_benchmark function.\n",
        "    \n",
        "    Args:\n",
        "        generated_code: The generated code to evaluate\n",
        "        test_code: Unit tests for verification\n",
        "        \n",
        "    Returns:\n",
        "        1.0 if all tests pass, 0.0 otherwise\n",
        "    \"\"\"\n",
        "    eval_instance = CodeEvaluator(timeout_seconds=5)\n",
        "    result = eval_instance.evaluate_code(generated_code, test_code)\n",
        "    return 1.0 if result[\"passed\"] else 0.0\n",
        "\n",
        "\n",
        "# Demonstrate the metric\n",
        "print(\"üìä Demonstrating pass/fail metric:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test with correct code\n",
        "correct_code = BUG_FIX_DATASET[0][\"expected_fix\"]\n",
        "test_code = BUG_FIX_DATASET[0][\"test_code\"]\n",
        "score = code_pass_fail_metric(correct_code, test_code)\n",
        "print(f\"Correct code score: {score} {'‚úÖ' if score == 1.0 else '‚ùå'}\")\n",
        "\n",
        "# Test with buggy code\n",
        "buggy_code = BUG_FIX_DATASET[0][\"buggy_code\"]\n",
        "score = code_pass_fail_metric(buggy_code, test_code)\n",
        "print(f\"Buggy code score: {score} {'‚úÖ' if score == 1.0 else '‚ùå'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Mini-Project: Your Bug-Fix Evaluation\n",
        "\n",
        "### Task\n",
        "\n",
        "Create your own bug-fix test case and evaluate it.\n",
        "\n",
        "### Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define your own bug-fix test case\n",
        "my_test_case = {\n",
        "    \"name\": \"Your Bug Name\",\n",
        "    \"description\": \"# What should the function do?\",\n",
        "    \"buggy_code\": '''# Your buggy code here\n",
        "def my_function(x):\n",
        "    # Contains a bug\n",
        "    pass''',\n",
        "    \"expected_fix\": '''# Your fixed code here\n",
        "def my_function(x):\n",
        "    # Bug is fixed\n",
        "    pass''',\n",
        "    \"test_code\": '''# Your unit tests here\n",
        "def test_my_function():\n",
        "    # assert my_function(input) == expected_output\n",
        "    pass''',\n",
        "    \"bug_type\": \"your-bug-type\",\n",
        "}\n",
        "\n",
        "# Evaluate your fix (uncomment to run)\n",
        "# result = evaluator.evaluate_code(\n",
        "#     my_test_case[\"expected_fix\"],\n",
        "#     my_test_case[\"test_code\"]\n",
        "# )\n",
        "# print(f\"Your fix passed: {result['passed']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§î Paul-Elder Critical Thinking Questions\n",
        "\n",
        "Reflect on these questions as you complete the exercises:\n",
        "\n",
        "### Question 1: TEST COVERAGE\n",
        "**What are the risks if an LLM-generated code fix passes all provided unit tests but contains a subtle bug that wasn't covered by the tests?**\n",
        "\n",
        "*Consider: Test coverage vs. correctness, edge cases not tested, the difference between \"passes tests\" and \"is correct,\" and how comprehensive test suites should be.*\n",
        "\n",
        "### Question 2: PROMPT ENGINEERING\n",
        "**How does the quality of the bug description in the prompt affect the LLM's ability to fix the bug correctly?**\n",
        "\n",
        "*Consider: The importance of function specifications, the role of examples, whether showing the expected output helps, and the tradeoff between detailed prompts and model generalization.*\n",
        "\n",
        "### Question 3: SECURITY IMPLICATIONS\n",
        "**Should LLM-generated code be trusted in production systems? What safeguards should be in place?**\n",
        "\n",
        "*Consider: Code review requirements, automated security scanning, testing requirements, the potential for introducing vulnerabilities, and the difference between \"works\" and \"safe.\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitations and Risks\n",
        "\n",
        "### What This Evaluation DOESN'T Cover\n",
        "\n",
        "1. **Security Vulnerabilities:** Code may pass tests but have security flaws\n",
        "2. **Performance Issues:** Correct but inefficient code\n",
        "3. **Edge Cases:** Tests may not cover all scenarios\n",
        "4. **Readability:** Code quality beyond correctness\n",
        "5. **Maintainability:** Long-term code health\n",
        "\n",
        "### Required Safeguards for Production\n",
        "\n",
        "- **Human Code Review:** Always review generated code\n",
        "- **Security Scanning:** Use automated vulnerability detection\n",
        "- **Comprehensive Testing:** Include edge cases, error handling\n",
        "- **Performance Testing:** Benchmark critical code paths\n",
        "- **Documentation:** Require clear comments and docstrings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 14, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand why code generation evaluation requires unit tests rather than semantic comparison\n",
        "- [ ] I can design a synthetic bug-fix dataset with descriptions, buggy code, expected fixes, and tests\n",
        "- [ ] I can use the CodeEvaluator to automatically run unit tests on generated code\n",
        "- [ ] I understand the pass/fail metric and how to compute pass rates\n",
        "- [ ] I know how BenchRight's engine can integrate with code evaluation\n",
        "- [ ] I can identify different bug types and analyze which are hardest to fix\n",
        "- [ ] I understand the security implications of LLM-generated code\n",
        "- [ ] I can articulate the limitations of unit test-based evaluation\n",
        "\n",
        "---\n",
        "\n",
        "**Week 13 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 14 ‚Äî Data Analytics Use Cases*"
      ]
    }
  ]
}
