# BenchRight
LLM benchmark evaluation framework on Google Colab, this repository contains the full 18 week learning journey to master the below:

1. LLM Evaluation
2. Benchmarking
3. Safety & Hallucination Detection
4. LLM as Judge scoring
5. Enterprise Use Cases
6. ONNX runtime
7. Google Colab Hands on Labs
8. Performance Profiling
9. Robustness, bias, adversarial tests
10. End to end system design

## Evaluation Feastures
### Standard Benchmarks
a. LAMBADA
b. MMLU
c. HellaSwag
d. BBH
e. TruthfulQA
f. Toxicity Tests
g. Adversarial robustness

### Performance Benchmarks
a. Latency
b. Throughput
c. Memory
d. ONNX operator profiling

### Quality Benchmarks
a. Coherence
b. Accuracy
c. Factuality
d. LLM-as-Judge (GPT-4/Claude-style scoring)

### Safety & Bias
a. Truthfulness
b. ToxiGen
c. Red-Teaming prompts

### Reports
a. PDF
b. Markdown model Cards
c. Dashboard Plots
