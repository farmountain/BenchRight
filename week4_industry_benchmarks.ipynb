{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 4 â€” Industry Benchmark Suites\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸŽ¯ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Load benchmark data from Hugging Face datasets\n",
        "2. Format questions for multiple-choice evaluation\n",
        "3. Run tinyGPT on benchmark subsets\n",
        "4. Compute and interpret accuracy metrics\n",
        "5. Understand why results are limited but instructive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ§  Industry Benchmarks Overview\n",
        "\n",
        "| Benchmark | Purpose | Size |\n",
        "|-----------|---------|------|\n",
        "| **MMLU** | World knowledge across 57 subjects | 15,908 questions |\n",
        "| **HellaSwag** | Commonsense reasoning / story completion | ~40k examples |\n",
        "| **TruthfulQA** | Truthfulness, avoiding misconceptions | 817 questions |\n",
        "| **BBH** | Multi-step reasoning (23 hard tasks) | ~6.5k examples |\n",
        "| **ToxiGen** | Implicit toxicity detection | ~274k statements |\n",
        "\n",
        "### Why we still evaluate small models:\n",
        "- Learn the evaluation process and mechanics\n",
        "- Build reusable evaluation infrastructure\n",
        "- Understand what capabilities are missing\n",
        "- Establish baselines for comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ› ï¸ Step 1: Setup & Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install onnxruntime transformers datasets pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import onnxruntime as ort\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(f\"âœ… ONNX Runtime version: {ort.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“¦ Step 2: Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Path to the ONNX model\n",
        "model_path = \"/tmp/tinygpt.onnx\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Create inference session\n",
        "session = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
        "\n",
        "print(\"âœ… Model and tokenizer loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š Step 3: Load Benchmark Datasets\n",
        "\n",
        "We'll load small slices from 2-3 benchmarks for efficiency.\n",
        "Full evaluations take hoursâ€”we'll use 50 examples per benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load HellaSwag - Commonsense reasoning benchmark\n",
        "print(\"ðŸ“¥ Loading HellaSwag...\")\n",
        "hellaswag = load_dataset(\"Rowan/hellaswag\", split=\"validation\")\n",
        "hellaswag_sample = hellaswag.select(range(50))\n",
        "print(f\"   Loaded {len(hellaswag_sample)} examples\")\n",
        "\n",
        "# Load TruthfulQA - Truthfulness benchmark\n",
        "print(\"\\nðŸ“¥ Loading TruthfulQA...\")\n",
        "truthfulqa = load_dataset(\"truthful_qa\", \"multiple_choice\", split=\"validation\")\n",
        "truthfulqa_sample = truthfulqa.select(range(50))\n",
        "print(f\"   Loaded {len(truthfulqa_sample)} examples\")\n",
        "\n",
        "# Load a simple QA dataset for additional testing\n",
        "print(\"\\nðŸ“¥ Loading simple QA dataset (sciq)...\")\n",
        "sciq = load_dataset(\"sciq\", split=\"test\")\n",
        "sciq_sample = sciq.select(range(50))\n",
        "print(f\"   Loaded {len(sciq_sample)} examples\")\n",
        "\n",
        "print(\"\\nâœ… All datasets loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ”¤ Step 4: Explore Dataset Formats\n",
        "\n",
        "Each benchmark has a different structure. Let's examine them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine HellaSwag format\n",
        "print(\"ðŸ“‹ HellaSwag Example:\")\n",
        "print(\"=\" * 50)\n",
        "example = hellaswag_sample[0]\n",
        "print(f\"Activity: {example['activity_label']}\")\n",
        "print(f\"Context: {example['ctx']}\")\n",
        "print(f\"Endings:\")\n",
        "for i, ending in enumerate(example['endings']):\n",
        "    marker = \"âœ“\" if i == int(example['label']) else \" \"\n",
        "    print(f\"  [{marker}] {chr(65+i)}. {ending[:80]}...\" if len(ending) > 80 else f\"  [{marker}] {chr(65+i)}. {ending}\")\n",
        "print(f\"Correct Answer: {chr(65 + int(example['label']))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine TruthfulQA format\n",
        "print(\"\\nðŸ“‹ TruthfulQA Example:\")\n",
        "print(\"=\" * 50)\n",
        "example = truthfulqa_sample[0]\n",
        "print(f\"Question: {example['question']}\")\n",
        "print(f\"Category: {example['category']}\")\n",
        "print(f\"Choices:\")\n",
        "choices = example['mc1_targets']['choices']\n",
        "labels = example['mc1_targets']['labels']\n",
        "for i, (choice, label) in enumerate(zip(choices, labels)):\n",
        "    marker = \"âœ“\" if label == 1 else \" \"\n",
        "    print(f\"  [{marker}] {chr(65+i)}. {choice[:80]}...\" if len(choice) > 80 else f\"  [{marker}] {chr(65+i)}. {choice}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Examine SciQ format\n",
        "print(\"\\nðŸ“‹ SciQ Example:\")\n",
        "print(\"=\" * 50)\n",
        "example = sciq_sample[0]\n",
        "print(f\"Question: {example['question']}\")\n",
        "print(f\"Correct Answer: {example['correct_answer']}\")\n",
        "print(f\"Distractors: {example['distractor1']}, {example['distractor2']}, {example['distractor3']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Step 5: Define Question Formatters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_hellaswag(example: dict) -> tuple:\n",
        "    \"\"\"\n",
        "    Format a HellaSwag example as a multiple-choice question.\n",
        "    \n",
        "    Args:\n",
        "        example: Dataset example with ctx, endings, and label\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (prompt, correct_answer_letter)\n",
        "    \"\"\"\n",
        "    context = example[\"ctx\"]\n",
        "    endings = example[\"endings\"]\n",
        "    \n",
        "    prompt = f\"Context: {context}\\n\\n\"\n",
        "    prompt += \"Which ending makes the most sense?\\n\"\n",
        "    for i, ending in enumerate(endings):\n",
        "        prompt += f\"{chr(65+i)}. {ending}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    \n",
        "    correct_answer = chr(65 + int(example[\"label\"]))\n",
        "    return prompt, correct_answer\n",
        "\n",
        "\n",
        "def format_truthfulqa(example: dict) -> tuple:\n",
        "    \"\"\"\n",
        "    Format a TruthfulQA example as a multiple-choice question.\n",
        "    \n",
        "    Args:\n",
        "        example: Dataset example with question and mc1_targets\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (prompt, correct_answer_letter)\n",
        "    \"\"\"\n",
        "    question = example[\"question\"]\n",
        "    choices = example[\"mc1_targets\"][\"choices\"]\n",
        "    labels = example[\"mc1_targets\"][\"labels\"]\n",
        "    \n",
        "    prompt = f\"Question: {question}\\n\\n\"\n",
        "    for i, choice in enumerate(choices):\n",
        "        prompt += f\"{chr(65+i)}. {choice}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    \n",
        "    # Find the correct answer (label == 1)\n",
        "    correct_idx = labels.index(1) if 1 in labels else 0\n",
        "    correct_answer = chr(65 + correct_idx)\n",
        "    return prompt, correct_answer\n",
        "\n",
        "\n",
        "def format_sciq(example: dict) -> tuple:\n",
        "    \"\"\"\n",
        "    Format a SciQ example as a multiple-choice question.\n",
        "    \n",
        "    Args:\n",
        "        example: Dataset example with question, correct_answer, and distractors\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (prompt, correct_answer_letter)\n",
        "    \"\"\"\n",
        "    question = example[\"question\"]\n",
        "    correct = example[\"correct_answer\"]\n",
        "    \n",
        "    # Combine correct answer with distractors and shuffle\n",
        "    options = [\n",
        "        correct,\n",
        "        example[\"distractor1\"],\n",
        "        example[\"distractor2\"],\n",
        "        example[\"distractor3\"]\n",
        "    ]\n",
        "    \n",
        "    # Use consistent shuffling based on question hash for reproducibility\n",
        "    np.random.seed(hash(question) % 2**32)\n",
        "    indices = np.random.permutation(4)\n",
        "    shuffled_options = [options[i] for i in indices]\n",
        "    correct_idx = list(indices).index(0)  # Original correct was at index 0\n",
        "    \n",
        "    prompt = f\"Question: {question}\\n\\n\"\n",
        "    for i, option in enumerate(shuffled_options):\n",
        "        prompt += f\"{chr(65+i)}. {option}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    \n",
        "    correct_answer = chr(65 + correct_idx)\n",
        "    return prompt, correct_answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the formatters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test HellaSwag formatter\n",
        "prompt, answer = format_hellaswag(hellaswag_sample[0])\n",
        "print(\"ðŸ“‹ Formatted HellaSwag Question:\")\n",
        "print(\"=\" * 50)\n",
        "print(prompt)\n",
        "print(f\"\\n(Correct: {answer})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸŽ¯ Step 6: Define MCQ Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_tinygpt_on_mcq(\n",
        "    dataset,\n",
        "    format_fn,\n",
        "    session: ort.InferenceSession,\n",
        "    tokenizer,\n",
        "    benchmark_name: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Evaluate tinyGPT on a multiple-choice benchmark.\n",
        "    \n",
        "    Args:\n",
        "        dataset: Hugging Face dataset with examples\n",
        "        format_fn: Function to format examples as (prompt, correct_answer)\n",
        "        session: ONNX Runtime inference session\n",
        "        tokenizer: Hugging Face tokenizer\n",
        "        benchmark_name: Name of the benchmark for logging\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with evaluation results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    correct_count = 0\n",
        "    \n",
        "    print(f\"\\nðŸ”„ Evaluating on {benchmark_name}...\")\n",
        "    \n",
        "    for i, example in enumerate(dataset):\n",
        "        # Format the question\n",
        "        prompt, correct_answer = format_fn(example)\n",
        "        \n",
        "        # Tokenize and run inference\n",
        "        inputs = tokenizer(prompt, return_tensors=\"np\")\n",
        "        outputs = session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "        \n",
        "        # Get logits for the next token\n",
        "        next_token_logits = outputs[0][0, -1, :]\n",
        "        \n",
        "        # Get token IDs for A, B, C, D\n",
        "        option_tokens = {\n",
        "            'A': tokenizer.encode(' A', add_special_tokens=False)[-1],\n",
        "            'B': tokenizer.encode(' B', add_special_tokens=False)[-1],\n",
        "            'C': tokenizer.encode(' C', add_special_tokens=False)[-1],\n",
        "            'D': tokenizer.encode(' D', add_special_tokens=False)[-1],\n",
        "        }\n",
        "        \n",
        "        # Find which option has highest probability\n",
        "        option_logits = {opt: next_token_logits[tid] for opt, tid in option_tokens.items()}\n",
        "        model_answer = max(option_logits, key=option_logits.get)\n",
        "        \n",
        "        # Check if correct\n",
        "        is_correct = model_answer == correct_answer\n",
        "        if is_correct:\n",
        "            correct_count += 1\n",
        "        \n",
        "        results.append({\n",
        "            \"benchmark\": benchmark_name,\n",
        "            \"question_id\": i,\n",
        "            \"correct_answer\": correct_answer,\n",
        "            \"model_answer\": model_answer,\n",
        "            \"is_correct\": is_correct,\n",
        "            \"confidence\": float(option_logits[model_answer])\n",
        "        })\n",
        "        \n",
        "        # Progress update\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"   Processed {i + 1}/{len(dataset)}...\")\n",
        "    \n",
        "    accuracy = correct_count / len(dataset) * 100\n",
        "    print(f\"\\nâœ… {benchmark_name}: {accuracy:.1f}% accuracy ({correct_count}/{len(dataset)})\")\n",
        "    \n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸš€ Step 7: Run Evaluation on All Benchmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on HellaSwag\n",
        "hellaswag_results = evaluate_tinygpt_on_mcq(\n",
        "    hellaswag_sample,\n",
        "    format_hellaswag,\n",
        "    session,\n",
        "    tokenizer,\n",
        "    \"HellaSwag\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on TruthfulQA\n",
        "truthfulqa_results = evaluate_tinygpt_on_mcq(\n",
        "    truthfulqa_sample,\n",
        "    format_truthfulqa,\n",
        "    session,\n",
        "    tokenizer,\n",
        "    \"TruthfulQA\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run evaluation on SciQ\n",
        "sciq_results = evaluate_tinygpt_on_mcq(\n",
        "    sciq_sample,\n",
        "    format_sciq,\n",
        "    session,\n",
        "    tokenizer,\n",
        "    \"SciQ\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“Š Step 8: Combine and Analyze Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all results\n",
        "all_results = pd.concat([\n",
        "    hellaswag_results,\n",
        "    truthfulqa_results,\n",
        "    sciq_results\n",
        "], ignore_index=True)\n",
        "\n",
        "# Summary by benchmark\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“Š BENCHMARK RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "summary = all_results.groupby(\"benchmark\").agg({\n",
        "    \"is_correct\": [\"sum\", \"count\", \"mean\"]\n",
        "}).round(3)\n",
        "\n",
        "summary.columns = [\"Correct\", \"Total\", \"Accuracy\"]\n",
        "summary[\"Accuracy\"] = (summary[\"Accuracy\"] * 100).round(1).astype(str) + \"%\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Random baseline comparison\n",
        "print(\"\\nðŸ“ˆ Comparison to Random Baseline (25% for 4-choice):\")\n",
        "for benchmark in summary.index:\n",
        "    acc = float(summary.loc[benchmark, \"Accuracy\"].strip(\"%\"))\n",
        "    diff = acc - 25\n",
        "    status = \"â†‘\" if diff > 0 else \"â†“\" if diff < 0 else \"=\"\n",
        "    print(f\"   {benchmark}: {acc}% ({status} {abs(diff):.1f}% from random)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show some example predictions\n",
        "print(\"\\nðŸ“‹ Sample Predictions (first 5 per benchmark):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for benchmark in [\"HellaSwag\", \"TruthfulQA\", \"SciQ\"]:\n",
        "    print(f\"\\n{benchmark}:\")\n",
        "    subset = all_results[all_results[\"benchmark\"] == benchmark].head(5)\n",
        "    for _, row in subset.iterrows():\n",
        "        status = \"âœ“\" if row[\"is_correct\"] else \"âœ—\"\n",
        "        print(f\"  [{status}] Q{row['question_id']}: Model={row['model_answer']}, Correct={row['correct_answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ¤” Why These Results Are Limited But Instructive\n",
        "\n",
        "### What we learned:\n",
        "\n",
        "1. **Evaluation mechanics work** â€” Our pipeline successfully loads data, formats questions, and computes accuracy\n",
        "2. **Small models struggle** â€” Results near random (25%) indicate tinyGPT lacks the capabilities these benchmarks test\n",
        "3. **Infrastructure is reusable** â€” The same code works with any ONNX model\n",
        "\n",
        "### Why results are limited:\n",
        "\n",
        "1. **Model size** â€” tinyGPT doesn't have enough parameters to store world knowledge\n",
        "2. **Training data** â€” May not have seen similar content during training\n",
        "3. **Sample size** â€” 50 examples has high variance; full benchmarks use thousands\n",
        "4. **Prompt format** â€” Different prompts can significantly change results\n",
        "\n",
        "### The key insight:\n",
        "\n",
        "> **Understanding how to run benchmarks is separate from getting good results.**\n",
        "> \n",
        "> You now have the skills to evaluate any model on industry benchmarks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸ“ Mini-Project: Design Your Own 10-Question Benchmark\n",
        "\n",
        "Create a small benchmark to test a specific capability.\n",
        "\n",
        "### Example: Basic Arithmetic Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example custom benchmark\n",
        "my_benchmark = [\n",
        "    {\"question\": \"What is 2 + 2?\", \"options\": [\"3\", \"4\", \"5\", \"6\"], \"correct\": 1},\n",
        "    {\"question\": \"What is 5 - 3?\", \"options\": [\"1\", \"2\", \"3\", \"4\"], \"correct\": 1},\n",
        "    {\"question\": \"What is 3 Ã— 4?\", \"options\": [\"7\", \"10\", \"12\", \"15\"], \"correct\": 2},\n",
        "    {\"question\": \"What is 10 Ã· 2?\", \"options\": [\"3\", \"4\", \"5\", \"6\"], \"correct\": 2},\n",
        "    {\"question\": \"What is 7 + 8?\", \"options\": [\"13\", \"14\", \"15\", \"16\"], \"correct\": 2},\n",
        "    {\"question\": \"What is 20 - 7?\", \"options\": [\"11\", \"12\", \"13\", \"14\"], \"correct\": 2},\n",
        "    {\"question\": \"What is 6 Ã— 3?\", \"options\": [\"15\", \"16\", \"18\", \"21\"], \"correct\": 2},\n",
        "    {\"question\": \"What is 15 Ã· 3?\", \"options\": [\"3\", \"4\", \"5\", \"6\"], \"correct\": 2},\n",
        "    {\"question\": \"What is 9 + 6?\", \"options\": [\"13\", \"14\", \"15\", \"16\"], \"correct\": 2},\n",
        "    {\"question\": \"What is 100 - 50?\", \"options\": [\"25\", \"40\", \"50\", \"75\"], \"correct\": 2},\n",
        "]\n",
        "\n",
        "def format_custom_benchmark(example: dict) -> tuple:\n",
        "    \"\"\"Format a custom benchmark question.\"\"\"\n",
        "    prompt = f\"Question: {example['question']}\\n\\n\"\n",
        "    for i, opt in enumerate(example['options']):\n",
        "        prompt += f\"{chr(65+i)}. {opt}\\n\"\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    return prompt, chr(65 + example['correct'])\n",
        "\n",
        "# Run evaluation on custom benchmark\n",
        "print(\"ðŸ§® Running Custom Arithmetic Benchmark...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "correct = 0\n",
        "for i, example in enumerate(my_benchmark):\n",
        "    prompt, correct_answer = format_custom_benchmark(example)\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"np\")\n",
        "    outputs = session.run(None, {\"input_ids\": inputs[\"input_ids\"]})\n",
        "    \n",
        "    next_token_logits = outputs[0][0, -1, :]\n",
        "    option_tokens = {\n",
        "        'A': tokenizer.encode(' A', add_special_tokens=False)[-1],\n",
        "        'B': tokenizer.encode(' B', add_special_tokens=False)[-1],\n",
        "        'C': tokenizer.encode(' C', add_special_tokens=False)[-1],\n",
        "        'D': tokenizer.encode(' D', add_special_tokens=False)[-1],\n",
        "    }\n",
        "    option_logits = {opt: next_token_logits[tid] for opt, tid in option_tokens.items()}\n",
        "    model_answer = max(option_logits, key=option_logits.get)\n",
        "    \n",
        "    is_correct = model_answer == correct_answer\n",
        "    if is_correct:\n",
        "        correct += 1\n",
        "    \n",
        "    status = \"âœ“\" if is_correct else \"âœ—\"\n",
        "    print(f\"[{status}] Q{i+1}: {example['question']} | Model={model_answer}, Correct={correct_answer}\")\n",
        "\n",
        "accuracy = correct / len(my_benchmark) * 100\n",
        "print(f\"\\nðŸ“Š Custom Benchmark Accuracy: {accuracy:.1f}% ({correct}/{len(my_benchmark)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœ… Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 5, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I can load benchmark datasets from Hugging Face\n",
        "- [ ] I understand the structure of HellaSwag, TruthfulQA, and SciQ\n",
        "- [ ] I can format MCQ questions for model evaluation\n",
        "- [ ] I can parse model outputs to extract answer predictions\n",
        "- [ ] I understand why small models perform near random baseline\n",
        "- [ ] I created and ran my own custom benchmark\n",
        "\n",
        "---\n",
        "\n",
        "**Week 4 Complete!** ðŸŽ‰\n",
        "\n",
        "**Next:** *Week 5 â€” Building a Generic Benchmark Engine*"
      ]
    }
  ]
}
