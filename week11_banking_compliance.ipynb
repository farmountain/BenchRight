{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 11 ‚Äî Banking & Compliance Evaluation\n",
        "### BenchRight LLM Evaluation Master Program (18 Weeks)\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will:\n",
        "\n",
        "1. Understand the unique challenges of evaluating LLMs for compliance summarization\n",
        "2. Use a ComplianceJudge to evaluate regulatory summaries\n",
        "3. Score summaries on both correctness and completeness\n",
        "4. Detect and track omitted critical details\n",
        "5. Analyze patterns in summarization failures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß† Why Compliance Summarization is Different\n",
        "\n",
        "### The Stakes\n",
        "\n",
        "Unlike general summarization, compliance summarization has **high-stakes failure modes**:\n",
        "\n",
        "| Failure Type | General Summary | Compliance Summary |\n",
        "|--------------|-----------------|--------------------|\n",
        "| Missing detail | Reader misses context | Potential violation |\n",
        "| Inaccurate number | Minor confusion | Wrong threshold applied |\n",
        "| Softened language | Tone shift | Mandatory becomes optional |\n",
        "\n",
        "### What We Evaluate\n",
        "\n",
        "1. **Correctness:** Is the summary accurate?\n",
        "2. **Completeness:** Are all critical details included?\n",
        "3. **Omissions:** What specific details were left out?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üõ†Ô∏è Step 1: Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import sys\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# Add src to path if running in Colab\n",
        "sys.path.insert(0, '.')\n",
        "\n",
        "# For data display\n",
        "try:\n",
        "    from IPython.display import display, HTML\n",
        "except ImportError:\n",
        "    display = print\n",
        "\n",
        "print(\"‚úÖ Setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 2: Define the Compliance Judge System Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "COMPLIANCE_JUDGE_PROMPT = \"\"\"You are an expert compliance officer evaluating regulatory summaries.\n",
        "\n",
        "Your task is to evaluate a summary of a regulatory text based on two criteria:\n",
        "\n",
        "1. **Correctness (0.0-1.0):** Does the summary accurately represent the regulatory requirements?\n",
        "   - 1.0: Fully accurate, no misrepresentations\n",
        "   - 0.7-0.9: Minor inaccuracies that don't affect compliance\n",
        "   - 0.4-0.6: Significant inaccuracies that could mislead compliance decisions\n",
        "   - 0.0-0.3: Major errors or misrepresentations\n",
        "\n",
        "2. **Completeness (0.0-1.0):** Are all critical details included?\n",
        "   - 1.0: All critical details present (amounts, deadlines, penalties, requirements)\n",
        "   - 0.7-0.9: Most critical details present, minor omissions\n",
        "   - 0.4-0.6: Some critical details missing\n",
        "   - 0.0-0.3: Major critical details missing\n",
        "\n",
        "CRITICAL DETAILS to check for:\n",
        "- Monetary thresholds and amounts\n",
        "- Time limits and deadlines\n",
        "- Penalty amounts and consequences\n",
        "- Specific requirements (documents, actions, retention periods)\n",
        "- Key terms (\"must\", \"shall\", prohibited actions)\n",
        "\n",
        "Respond ONLY with a valid JSON object:\n",
        "{\n",
        "    \"correctness_score\": <float 0.0-1.0>,\n",
        "    \"completeness_score\": <float 0.0-1.0>,\n",
        "    \"omitted_details\": [\"<list of critical details missing from summary>\"],\n",
        "    \"rationale\": \"<brief explanation of scores>\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(\"üìã Compliance Judge System Prompt defined!\")\n",
        "print(f\"\\nüìù Prompt length: {len(COMPLIANCE_JUDGE_PROMPT)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèõÔ∏è Step 3: Implement the ComplianceJudge Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ComplianceJudge:\n",
        "    \"\"\"\n",
        "    LLM-as-Judge for evaluating regulatory and compliance summaries.\n",
        "    \n",
        "    This specialized judge evaluates:\n",
        "    - Correctness: Accuracy of the summary\n",
        "    - Completeness: Whether critical details are included\n",
        "    - Omissions: Specific details that were left out\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, client, model: str = \"gpt-4o-mini\"):\n",
        "        \"\"\"\n",
        "        Initialize the ComplianceJudge.\n",
        "        \n",
        "        Args:\n",
        "            client: An LLM client with chat.completions.create() method\n",
        "            model: Model to use for judging (default: gpt-4o-mini)\n",
        "        \"\"\"\n",
        "        self.client = client\n",
        "        self.model = model\n",
        "        self.system_prompt = COMPLIANCE_JUDGE_PROMPT\n",
        "    \n",
        "    def evaluate_summary(\n",
        "        self,\n",
        "        regulatory_text: str,\n",
        "        summary: str,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate a regulatory summary.\n",
        "        \n",
        "        Args:\n",
        "            regulatory_text: The original regulatory text\n",
        "            summary: The model-generated summary\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with correctness_score, completeness_score, \n",
        "            omitted_details, and rationale\n",
        "        \"\"\"\n",
        "        user_prompt = f\"\"\"Regulatory Text:\n",
        "{regulatory_text}\n",
        "\n",
        "Summary to Evaluate:\n",
        "{summary}\n",
        "\n",
        "Evaluate the summary for correctness and completeness.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=self.model,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt},\n",
        "                ],\n",
        "                temperature=0.0,\n",
        "            )\n",
        "            \n",
        "            result_text = response.choices[0].message.content\n",
        "            result = json.loads(result_text)\n",
        "            \n",
        "            return {\n",
        "                \"correctness_score\": float(result.get(\"correctness_score\", 0.0)),\n",
        "                \"completeness_score\": float(result.get(\"completeness_score\", 0.0)),\n",
        "                \"omitted_details\": result.get(\"omitted_details\", []),\n",
        "                \"rationale\": result.get(\"rationale\", \"\"),\n",
        "            }\n",
        "            \n",
        "        except (json.JSONDecodeError, KeyError, TypeError) as e:\n",
        "            return {\n",
        "                \"correctness_score\": 0.0,\n",
        "                \"completeness_score\": 0.0,\n",
        "                \"omitted_details\": [\"Error parsing judge response\"],\n",
        "                \"rationale\": f\"Error: {str(e)}\",\n",
        "            }\n",
        "    \n",
        "    def compute_combined_score(\n",
        "        self,\n",
        "        correctness_score: float,\n",
        "        completeness_score: float,\n",
        "        correctness_weight: float = 0.5,\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Compute a weighted combined score.\n",
        "        \n",
        "        Args:\n",
        "            correctness_score: Score for accuracy (0-1)\n",
        "            completeness_score: Score for completeness (0-1)\n",
        "            correctness_weight: Weight for correctness (completeness = 1 - this)\n",
        "            \n",
        "        Returns:\n",
        "            Weighted combined score\n",
        "        \"\"\"\n",
        "        completeness_weight = 1.0 - correctness_weight\n",
        "        return (correctness_score * correctness_weight + \n",
        "                completeness_score * completeness_weight)\n",
        "\n",
        "\n",
        "print(\"‚úÖ ComplianceJudge class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§ñ Step 4: Create a Mock LLM Client for Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MockComplianceJudgeClient:\n",
        "    \"\"\"\n",
        "    Mock client that simulates LLM responses for compliance evaluation.\n",
        "    \n",
        "    This allows demonstration without an API key.\n",
        "    \"\"\"\n",
        "    \n",
        "    class MockChat:\n",
        "        class MockCompletions:\n",
        "            def create(self, model: str, messages: List[Dict], temperature: float = 0.0):\n",
        "                \"\"\"Simulate a compliance evaluation response.\"\"\"\n",
        "                user_msg = messages[-1][\"content\"].lower()\n",
        "                \n",
        "                # Determine response based on summary quality indicators\n",
        "                if \"$500,000\" in user_msg and \"five years\" in user_msg and \"penalty\" in user_msg.split(\"summary\")[1] if \"summary\" in user_msg else \"\":\n",
        "                    # Good summary with critical details\n",
        "                    response = {\n",
        "                        \"correctness_score\": 0.95,\n",
        "                        \"completeness_score\": 0.90,\n",
        "                        \"omitted_details\": [],\n",
        "                        \"rationale\": \"Summary accurately captures all key requirements including penalties and retention period.\"\n",
        "                    }\n",
        "                elif \"should\" in user_msg.split(\"summary\")[1] if \"summary\" in user_msg else \"\" or \"around\" in user_msg:\n",
        "                    # Summary with softened language or approximations\n",
        "                    response = {\n",
        "                        \"correctness_score\": 0.50,\n",
        "                        \"completeness_score\": 0.40,\n",
        "                        \"omitted_details\": [\n",
        "                            \"Specific monetary threshold\",\n",
        "                            \"Exact time deadline\",\n",
        "                            \"Penalty amount\",\n",
        "                            \"Mandatory vs optional distinction\"\n",
        "                        ],\n",
        "                        \"rationale\": \"Summary softens mandatory requirements and omits critical numerical details.\"\n",
        "                    }\n",
        "                elif \"60 days\" in user_msg or \"$10,000\" in user_msg or \"inform\" in user_msg.split(\"summary\")[1] if \"summary\" in user_msg else \"\":\n",
        "                    # Inaccurate summary\n",
        "                    response = {\n",
        "                        \"correctness_score\": 0.20,\n",
        "                        \"completeness_score\": 0.30,\n",
        "                        \"omitted_details\": [\n",
        "                            \"Correct deadline (30 days, not 60)\",\n",
        "                            \"Correct threshold ($5,000, not $10,000)\",\n",
        "                            \"Customer notification prohibition\"\n",
        "                        ],\n",
        "                        \"rationale\": \"Summary contains multiple factual errors that would lead to compliance violations.\"\n",
        "                    }\n",
        "                elif \"documentation\" in user_msg.split(\"summary\")[1] if \"summary\" in user_msg else \"\" and len(user_msg.split(\"summary\")[1] if \"summary\" in user_msg else \"\") < 200:\n",
        "                    # Very brief summary missing details\n",
        "                    response = {\n",
        "                        \"correctness_score\": 0.70,\n",
        "                        \"completeness_score\": 0.25,\n",
        "                        \"omitted_details\": [\n",
        "                            \"Specific time deadline\",\n",
        "                            \"Monetary threshold\",\n",
        "                            \"Penalty for non-compliance\",\n",
        "                            \"Retention period\",\n",
        "                            \"Required verification documents\"\n",
        "                        ],\n",
        "                        \"rationale\": \"Summary is technically accurate but critically incomplete, missing most actionable details.\"\n",
        "                    }\n",
        "                else:\n",
        "                    # Default moderate response\n",
        "                    response = {\n",
        "                        \"correctness_score\": 0.75,\n",
        "                        \"completeness_score\": 0.65,\n",
        "                        \"omitted_details\": [\n",
        "                            \"Some specific thresholds or deadlines\"\n",
        "                        ],\n",
        "                        \"rationale\": \"Summary captures main requirements but may be missing some specific details.\"\n",
        "                    }\n",
        "                \n",
        "                class MockMessage:\n",
        "                    content = json.dumps(response)\n",
        "                \n",
        "                class MockChoice:\n",
        "                    message = MockMessage()\n",
        "                \n",
        "                class MockResponse:\n",
        "                    choices = [MockChoice()]\n",
        "                \n",
        "                return MockResponse()\n",
        "        \n",
        "        completions = MockCompletions()\n",
        "    \n",
        "    chat = MockChat()\n",
        "\n",
        "\n",
        "# Create the mock client\n",
        "mock_client = MockComplianceJudgeClient()\n",
        "print(\"‚úÖ Mock compliance evaluation client created!\")\n",
        "print(\"   (Replace with real OpenAI client for production use)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üèõÔ∏è Step 5: Initialize the ComplianceJudge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the ComplianceJudge with our mock client\n",
        "judge = ComplianceJudge(\n",
        "    client=mock_client,\n",
        "    model=\"gpt-4o-mini\",\n",
        ")\n",
        "\n",
        "print(\"‚úÖ ComplianceJudge initialized!\")\n",
        "print(f\"   Model: {judge.model}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 6: Define Regulatory Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define regulatory texts and summaries of varying quality\n",
        "test_cases = [\n",
        "    {\n",
        "        \"name\": \"KYC - Complete Summary\",\n",
        "        \"regulatory_text\": \"\"\"\n",
        "Financial institutions must verify the identity of each customer opening an account.\n",
        "Verification requires obtaining the customer's name, date of birth, address, and \n",
        "identification number. For individuals, acceptable identification includes a \n",
        "government-issued photo ID (passport, driver's license) or two forms of non-photo \n",
        "identification. The institution must maintain records of the verification process \n",
        "for at least five years after the account is closed. Failure to comply may result \n",
        "in penalties up to $500,000 per violation.\n",
        "        \"\"\".strip(),\n",
        "        \"summary\": \"\"\"\n",
        "Banks must verify customer identity when opening accounts using name, DOB, address, \n",
        "and ID number. Acceptable ID: government photo ID or two non-photo IDs. Records \n",
        "must be kept five years after account closure. Penalty up to $500,000 per violation.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"high\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"KYC - Missing Critical Details\",\n",
        "        \"regulatory_text\": \"\"\"\n",
        "Financial institutions must verify the identity of each customer opening an account.\n",
        "Verification requires obtaining the customer's name, date of birth, address, and \n",
        "identification number. For individuals, acceptable identification includes a \n",
        "government-issued photo ID (passport, driver's license) or two forms of non-photo \n",
        "identification. The institution must maintain records of the verification process \n",
        "for at least five years after the account is closed. Failure to comply may result \n",
        "in penalties up to $500,000 per violation.\n",
        "        \"\"\".strip(),\n",
        "        \"summary\": \"\"\"\n",
        "Banks should verify customer identity when opening accounts. Records should be \n",
        "kept for documentation purposes.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"low\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"SAR - Inaccurate Summary\",\n",
        "        \"regulatory_text\": \"\"\"\n",
        "Banks must file a Suspicious Activity Report (SAR) within 30 calendar days of \n",
        "detecting suspicious activity. Suspicious activity includes transactions over \n",
        "$5,000 that appear to involve money laundering. The bank must not notify the \n",
        "customer that a SAR has been filed. SAR records must be retained for five years.\n",
        "        \"\"\".strip(),\n",
        "        \"summary\": \"\"\"\n",
        "Banks should file SARs within 60 days for transactions over $10,000. Customers \n",
        "should be informed when a SAR is filed about their account.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"inaccurate\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Data Retention - Moderate Summary\",\n",
        "        \"regulatory_text\": \"\"\"\n",
        "Under the Bank Secrecy Act, financial institutions must retain records of \n",
        "transactions over $3,000 for a minimum of five years. Records must include \n",
        "the customer's name, account number, transaction amount, and date. Electronic \n",
        "records are acceptable if they can be produced within 24 hours upon regulatory \n",
        "request. Institutions must designate a compliance officer responsible for \n",
        "ensuring retention requirements are met.\n",
        "        \"\"\".strip(),\n",
        "        \"summary\": \"\"\"\n",
        "Banks must keep transaction records over $3,000 for five years. Records need \n",
        "customer name, account number, amount, and date. Electronic records must be \n",
        "producible within 24 hours. A compliance officer must be designated.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"high\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"AML - Partial Summary\",\n",
        "        \"regulatory_text\": \"\"\"\n",
        "Financial institutions must implement an Anti-Money Laundering (AML) program \n",
        "that includes: (1) internal policies and procedures, (2) designation of a \n",
        "compliance officer, (3) ongoing employee training, and (4) independent testing. \n",
        "The program must be approved by the board of directors. Failure to maintain \n",
        "an adequate program may result in penalties up to $1,000,000 per day.\n",
        "        \"\"\".strip(),\n",
        "        \"summary\": \"\"\"\n",
        "Banks need an AML program with policies, a compliance officer, training, and \n",
        "testing. The board must approve it.\n",
        "        \"\"\".strip(),\n",
        "        \"expected_quality\": \"moderate\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìä Defined {len(test_cases)} regulatory test cases:\")\n",
        "for i, tc in enumerate(test_cases, 1):\n",
        "    print(f\"   {i}. {tc['name']} (expected: {tc['expected_quality']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üß™ Step 7: Evaluate All Test Cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate all test cases\n",
        "print(\"üîç Evaluating Regulatory Summaries...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results = []\n",
        "for tc in test_cases:\n",
        "    result = judge.evaluate_summary(\n",
        "        regulatory_text=tc[\"regulatory_text\"],\n",
        "        summary=tc[\"summary\"]\n",
        "    )\n",
        "    \n",
        "    combined = judge.compute_combined_score(\n",
        "        result[\"correctness_score\"],\n",
        "        result[\"completeness_score\"]\n",
        "    )\n",
        "    \n",
        "    results.append({\n",
        "        \"name\": tc[\"name\"],\n",
        "        \"expected_quality\": tc[\"expected_quality\"],\n",
        "        \"correctness_score\": result[\"correctness_score\"],\n",
        "        \"completeness_score\": result[\"completeness_score\"],\n",
        "        \"combined_score\": combined,\n",
        "        \"omitted_details\": result[\"omitted_details\"],\n",
        "        \"rationale\": result[\"rationale\"],\n",
        "    })\n",
        "    \n",
        "    print(f\"\\nüìã {tc['name']}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"   Correctness:  {result['correctness_score']:.2f}\")\n",
        "    print(f\"   Completeness: {result['completeness_score']:.2f}\")\n",
        "    print(f\"   Combined:     {combined:.2f}\")\n",
        "    if result[\"omitted_details\"]:\n",
        "        print(f\"   Omissions:    {len(result['omitted_details'])} detail(s)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Evaluation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìä Step 8: Display Results Summary Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Results Summary Table\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"{'Test Case':<35} {'Expected':<12} {'Correct':<10} {'Complete':<10} {'Combined':<10}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for r in results:\n",
        "    # Determine status symbol\n",
        "    if r[\"combined_score\"] >= 0.8:\n",
        "        status = \"‚úÖ\"\n",
        "    elif r[\"combined_score\"] >= 0.5:\n",
        "        status = \"‚ö†Ô∏è\"\n",
        "    else:\n",
        "        status = \"‚ùå\"\n",
        "    \n",
        "    print(f\"{status} {r['name']:<33} {r['expected_quality']:<12} {r['correctness_score']:<10.2f} {r['completeness_score']:<10.2f} {r['combined_score']:<10.2f}\")\n",
        "\n",
        "print(\"-\" * 90)\n",
        "\n",
        "# Calculate averages\n",
        "avg_correctness = sum(r[\"correctness_score\"] for r in results) / len(results)\n",
        "avg_completeness = sum(r[\"completeness_score\"] for r in results) / len(results)\n",
        "avg_combined = sum(r[\"combined_score\"] for r in results) / len(results)\n",
        "\n",
        "print(f\"{'AVERAGE':<35} {'':<12} {avg_correctness:<10.2f} {avg_completeness:<10.2f} {avg_combined:<10.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üîç Step 9: Analyze Omitted Details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üîç Omitted Details Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Collect all omitted details\n",
        "all_omissions = []\n",
        "for r in results:\n",
        "    if r[\"omitted_details\"]:\n",
        "        print(f\"\\nüìã {r['name']}:\")\n",
        "        for detail in r[\"omitted_details\"]:\n",
        "            print(f\"   ‚ùå {detail}\")\n",
        "            all_omissions.append(detail)\n",
        "\n",
        "# Count omission types (simplified categorization)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìà Omission Categories\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "categories = {\n",
        "    \"monetary\": 0,\n",
        "    \"deadline\": 0,\n",
        "    \"penalty\": 0,\n",
        "    \"requirement\": 0,\n",
        "    \"other\": 0,\n",
        "}\n",
        "\n",
        "for omission in all_omissions:\n",
        "    omission_lower = omission.lower()\n",
        "    if any(word in omission_lower for word in [\"threshold\", \"amount\", \"$\", \"monetary\"]):\n",
        "        categories[\"monetary\"] += 1\n",
        "    elif any(word in omission_lower for word in [\"deadline\", \"time\", \"days\", \"period\"]):\n",
        "        categories[\"deadline\"] += 1\n",
        "    elif any(word in omission_lower for word in [\"penalty\", \"fine\", \"violation\"]):\n",
        "        categories[\"penalty\"] += 1\n",
        "    elif any(word in omission_lower for word in [\"requirement\", \"must\", \"mandatory\", \"document\"]):\n",
        "        categories[\"requirement\"] += 1\n",
        "    else:\n",
        "        categories[\"other\"] += 1\n",
        "\n",
        "for category, count in sorted(categories.items(), key=lambda x: -x[1]):\n",
        "    if count > 0:\n",
        "        print(f\"   {category.capitalize()}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìã Step 10: View Detailed Rationales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìã Detailed Evaluation Rationales\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for r in results:\n",
        "    print(f\"\\nüìå {r['name']}\")\n",
        "    print(f\"   Scores: Correctness={r['correctness_score']:.2f}, Completeness={r['completeness_score']:.2f}\")\n",
        "    print(f\"   Rationale: {r['rationale']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Step 11: Define Score Thresholds for Compliance Use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define thresholds for compliance use\n",
        "THRESHOLDS = {\n",
        "    \"auto_approve\": 0.90,      # Summary can be used without review\n",
        "    \"review_required\": 0.70,   # Summary needs human review\n",
        "    \"reject\": 0.50,            # Summary should not be used\n",
        "}\n",
        "\n",
        "print(\"üìã Compliance Use Recommendations\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"{'Test Case':<35} {'Combined':<10} {'Recommendation':<20}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for r in results:\n",
        "    score = r[\"combined_score\"]\n",
        "    \n",
        "    if score >= THRESHOLDS[\"auto_approve\"]:\n",
        "        recommendation = \"‚úÖ Auto-approve\"\n",
        "    elif score >= THRESHOLDS[\"review_required\"]:\n",
        "        recommendation = \"‚ö†Ô∏è Review required\"\n",
        "    elif score >= THRESHOLDS[\"reject\"]:\n",
        "        recommendation = \"‚ùå Major review needed\"\n",
        "    else:\n",
        "        recommendation = \"üö´ Reject\"\n",
        "    \n",
        "    print(f\"{r['name']:<35} {score:<10.2f} {recommendation:<20}\")\n",
        "\n",
        "print(\"\\nüìå Thresholds:\")\n",
        "print(f\"   Auto-approve: ‚â• {THRESHOLDS['auto_approve']}\")\n",
        "print(f\"   Review required: ‚â• {THRESHOLDS['review_required']}\")\n",
        "print(f\"   Major review: ‚â• {THRESHOLDS['reject']}\")\n",
        "print(f\"   Reject: < {THRESHOLDS['reject']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üéì Mini-Project: Your Compliance Evaluation\n",
        "\n",
        "### Task\n",
        "\n",
        "Create your own regulatory text and evaluate different summary qualities.\n",
        "\n",
        "### Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your regulatory text\n",
        "my_regulatory_text = \"\"\"\n",
        "# Paste a regulatory text here or write your own\n",
        "# Include: monetary thresholds, deadlines, penalties, requirements\n",
        "\"\"\"\n",
        "\n",
        "# Your good summary\n",
        "my_good_summary = \"\"\"\n",
        "# Write a summary that includes all critical details\n",
        "\"\"\"\n",
        "\n",
        "# Your poor summary\n",
        "my_poor_summary = \"\"\"\n",
        "# Write a summary that omits critical details\n",
        "\"\"\"\n",
        "\n",
        "# Evaluate both\n",
        "# good_result = judge.evaluate_summary(my_regulatory_text, my_good_summary)\n",
        "# poor_result = judge.evaluate_summary(my_regulatory_text, my_poor_summary)\n",
        "\n",
        "# Compare results\n",
        "# print(f\"Good summary: {good_result['combined_score']:.2f}\")\n",
        "# print(f\"Poor summary: {poor_result['combined_score']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ü§î Paul-Elder Critical Thinking Questions\n",
        "\n",
        "Reflect on these questions as you complete the exercises:\n",
        "\n",
        "### Question 1: RISK ASSESSMENT\n",
        "**What are the potential consequences if a compliance summary incorrectly states a reporting deadline as 60 days instead of 30 days?**\n",
        "\n",
        "*Consider: Regulatory violations, audit findings, potential fines, reputational damage, and the chain of decisions that might rely on this summary.*\n",
        "\n",
        "### Question 2: TRUST CALIBRATION\n",
        "**Should a compliance officer trust an LLM-generated summary of a regulation they haven't read themselves? Under what conditions?**\n",
        "\n",
        "*Consider: The role of human oversight, the complexity of the regulation, the stakes involved, available verification methods, and organizational liability.*\n",
        "\n",
        "### Question 3: OMISSION DETECTION\n",
        "**How can we systematically detect what an LLM summary has omitted from a regulatory text, and why is this particularly challenging?**\n",
        "\n",
        "*Consider: The difficulty of proving a negative, the need for domain expertise to identify critical vs. non-critical details, and how omission detection differs from error detection.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Limitations and Risks\n",
        "\n",
        "### What This Evaluation DOESN'T Cover\n",
        "\n",
        "1. **Regulatory Updates:** Regulations change; summaries may become outdated\n",
        "2. **Jurisdiction:** Rules vary by location; summaries may not be universally applicable\n",
        "3. **Interconnections:** Regulations interact; isolated summaries miss context\n",
        "4. **Legal Nuance:** Subtle legal distinctions may be lost in summarization\n",
        "5. **Liability:** Using LLM summaries doesn't transfer legal responsibility\n",
        "\n",
        "### Required Safeguards for Production Use\n",
        "\n",
        "- **Human Review:** All summaries should be reviewed by qualified compliance staff\n",
        "- **Source Linking:** Always provide links to original regulatory text\n",
        "- **Version Control:** Track which version of regulations were summarized\n",
        "- **Audit Trail:** Log all summary generations and reviews\n",
        "- **Periodic Validation:** Re-evaluate summaries when regulations change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Knowledge Mastery Checklist\n",
        "\n",
        "Before moving to Week 12, ensure you can check all boxes:\n",
        "\n",
        "- [ ] I understand why compliance summarization is different from general summarization\n",
        "- [ ] I can explain the difference between correctness and completeness scores\n",
        "- [ ] I can use the ComplianceJudge to evaluate regulatory summaries\n",
        "- [ ] I understand what types of critical details are most important in compliance\n",
        "- [ ] I can identify when human review is essential vs. when automation is acceptable\n",
        "- [ ] I know the risks of omission in regulatory summarization\n",
        "- [ ] I can set appropriate score thresholds for compliance use cases\n",
        "\n",
        "---\n",
        "\n",
        "**Week 11 Complete!** üéâ\n",
        "\n",
        "**Next:** *Week 12 ‚Äî Healthcare Use Cases*"
      ]
    }
  ]
}
